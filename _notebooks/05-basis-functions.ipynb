{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "===============\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In the last session we explored least squares for\n",
    "univariate and multivariate *regression*. We introduced *matrices*,\n",
    "*linear algebra* and *derivatives*.\n",
    "\n",
    "In this session we will introduce *basis functions* which allow us to\n",
    "implement *non-linear regression models*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "-----\n",
    "\n",
    "First we download some libraries and files to support the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pods\n",
    "----\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/sods/ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on github:\n",
    "<a href=\"https://github.com/sods/ods\" class=\"uri\">https://github.com/sods/ods</a>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinear Regression\n",
    "--------------------\n",
    "\n",
    "We’ve now seen how we may perform linear regression. Now, we are going\n",
    "to consider how we can perform *non-linear* regression. However, before\n",
    "we get into the details of how to do that we first need to consider in\n",
    "what ways the regression can be non-linear. Multivariate linear\n",
    "regression allows us to build models that take many features into\n",
    "account when making our prediction. In this session we are going to\n",
    "introduce *basis functions*. The term seems complicted, but they are\n",
    "actually based on rather a simple idea. If we are doing a multivariate\n",
    "linear regression, we get extra features that *might* help us predict\n",
    "our required response varible (or target value), $y$. But what if we\n",
    "only have one input value? We can actually artificially generate more\n",
    "input values with basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear in the Inputs\n",
    "------------------------\n",
    "\n",
    "When we refer to non-linear regression, we are normally referring to\n",
    "whether the regression is non-linear in the input space, or non-linear\n",
    "in the *covariates*. The covariates are the observations that move with\n",
    "the target (or *response*) variable. In our notation we have been using\n",
    "$\\mathbf{ x}_i$ to represent a vector of the covariates associated with\n",
    "the $i$th observation. The coresponding response variable is $y_i$. If a\n",
    "model is non-linear in the inputs, it means that there is a non-linear\n",
    "function between the inputs and the response variable. Linear functions\n",
    "are functions that only involve multiplication and addition, in other\n",
    "words they can be represented through *linear algebra*. Linear\n",
    "regression involves assuming that a function takes the form $$\n",
    "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\mathbf{ x}\n",
    "$$ where $\\mathbf{ w}$ are our regression weights. A very easy way to\n",
    "make the linear regression non-linear is to introduce non-linear\n",
    "functions. When we are introducing non-linear regression these functions\n",
    "are known as *basis functions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "Here’s the idea, instead of working directly on the original input\n",
    "space, $\\mathbf{ x}$, we build models in a new space,\n",
    "$\\boldsymbol{ \\phi}(\\mathbf{ x})$ where $\\boldsymbol{ \\phi}(\\cdot)$ is a\n",
    "*vector-valued* function that is defined on the space $\\mathbf{ x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Basis\n",
    "---------------\n",
    "\n",
    "Remember, that a *vector-valued function* is just a vector that contains\n",
    "functions instead of values. Here’s an example for a one dimensional\n",
    "input space, $x$, being projected to a *quadratic* basis. First we\n",
    "consider each basis function in turn, we can think of the elements of\n",
    "our vector as being indexed so that we have $$\n",
    "\\begin{align*}\n",
    "\\phi_1(x) & = 1, \\\\\n",
    "\\phi_2(x) & = x, \\\\\n",
    "\\phi_3(x) & = x^2.\n",
    "\\end{align*}\n",
    "$$ Now we can consider them together by placing them in a vector, $$\n",
    "\\boldsymbol{ \\phi}(x) = \\begin{bmatrix} 1\\\\ x \\\\ x^2\\end{bmatrix}.\n",
    "$$ For the vector-valued function, we have simply collected the\n",
    "different functions together in the same vector making them notationally\n",
    "easier to deal with in our mathematics.\n",
    "\n",
    "When we consider the vector-valued function for each data point, then we\n",
    "place all the data into a matrix. The result is a matrix valued\n",
    "function, $$\n",
    "\\boldsymbol{ \\Phi}(\\mathbf{ x}) = \n",
    "\\begin{bmatrix} 1 & x_1 &\n",
    "x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_n & x_n^2\n",
    "\\end{bmatrix}\n",
    "$$ where we are still in the one dimensional input setting so\n",
    "$\\mathbf{ x}$ here represents a vector of our inputs with $n$ elements.\n",
    "\n",
    "Let’s try constructing such a matrix for a set of inputs. First of all,\n",
    "we create a function that returns the matrix valued function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x, **kwargs):\n",
    "    \"\"\"Take in a vector of input values and return the design matrix associated \n",
    "    with the basis functions.\"\"\"\n",
    "    return np.hstack([np.ones((x.shape[0], 1)), x, x**2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Quadratic Basis\n",
    "--------------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = {\\color{red}{w_0}}   + {\\color{magenta}{w_1 x}} + {\\color{blue}{w_2 x^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [0, -0.7],\n",
    "      [0.75, -0.2]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = x$',\n",
    "       '$\\phi(x) = x^2$']\n",
    "\n",
    "plot.basis(quadratic, x_min=-1.3, x_max=1.3, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/quadratic_basis002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The set of functions which are combined to form a *quadratic*\n",
    "basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('quadratic_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in an $n\\times 1$ dimensional vector and returns an\n",
    "$n\\times 3$ dimensional *design matrix* containing the basis functions.\n",
    "We can plot those basis functions against there input as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's generate some inputs\n",
    "n = 100\n",
    "x = np.zeros((n, 1))  # create a data set of zeros\n",
    "x[:, 0] = np.linspace(-1, 1, n) # fill it with values between -1 and 1\n",
    "\n",
    "Phi = quadratic(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.set_ylim([-1.2, 1.2]) # set y limits to ensure basis functions show.\n",
    "ax.plot(x[:,0], Phi[:, 0], 'r-', label = '$\\phi=1$', linewidth=3)\n",
    "ax.plot(x[:,0], Phi[:, 1], 'g-', label = '$\\phi=x$', linewidth=3)\n",
    "ax.plot(x[:,0], Phi[:, 2], 'b-', label = '$\\phi=x^2$', linewidth=3)\n",
    "ax.legend(loc='lower right')\n",
    "_ = ax.set_title('Quadratic Basis Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function we observe is then made up of a sum of these\n",
    "functions. This is the reason for the name basis. The term *basis* means\n",
    "‘the underlying support or foundation for an idea, argument, or\n",
    "process’, and in this context they form the underlying support for our\n",
    "prediction function. Our prediction function can only be composed of a\n",
    "weighted linear sum of our basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Functions\n",
    "-------------------\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/quadratic_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Functions constructed by weighted sum of the components of a\n",
    "quadratic basis.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('quadratic_function{num_function:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Bases\n",
    "---------------\n",
    "\n",
    "Our choice of basis can be made based on what our beliefs about what is\n",
    "appropriate for the data. For example, the polynomial basis extends the\n",
    "quadratic basis to aribrary degree, so we might define the $j$th basis\n",
    "function associated with the model as $$\n",
    "\\phi_j(x_i) = x_i^j\n",
    "$$ which is known as the *polynomial basis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Basis\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s polynomial mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [0, -0.7],\n",
    "      [0.75, -0.2]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = x$',\n",
    "       '$\\phi(x) = x^2$'\n",
    "       '$\\phi(x) = x^3$',\n",
    "       '$\\phi(x) = x^4$']\n",
    "\n",
    "plot.basis(mlai.polynomial, x_min=-1.3, x_max=1.3, \n",
    "           fig=f, ax=ax, loc=loc, text=text, num_basis=5,\n",
    "           diagrams='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Polynomial Basis\n",
    "---------------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = {\\color{red}w_0} + {\\color{magenta}w_1 x} + {\\color{blue}w_2 x^2} + {\\color{green}w_3 x^3} + {\\color{cyan}w_4 x^4}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/polynomial_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A polynomial basis is made up of different degrees of\n",
    "polynomial.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('polynomial_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                num_basis=IntSlider(1,1,5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in understanding how a basis works, we’ve provided you with a\n",
    "small interactive tool for exploring this polynomial basis. The tool can\n",
    "be summoned with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.polynomial, num_basis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try moving the sliders around to change the weight of each basis\n",
    "function. Click the control box `display_basis` to show the underlying\n",
    "basis functions (in red). The prediction function is shown in a thick\n",
    "blue line. *Warning* the sliders aren’t presented quite in the correct\n",
    "order. `w_0` is associated with the bias, `w_1` is the linear term,\n",
    "`w_2` the quadratic and here (because we have four basis functions) we\n",
    "have `w_3` for the *cubic* term. So the subscript of the weight\n",
    "parameter is always associated with the corresponding polynomial’s\n",
    "degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Basis\n",
    "---------------\n",
    "\n",
    "The polynomial basis is widely used in Engineering and graphics, but it\n",
    "has some drawbacks in machine learning: outside the input region between\n",
    "-1 and 1, the values of the polynomial basis rise very quickly.\n",
    "\n",
    "Now we look at basis functions that have been used as the *activation*\n",
    "functions in neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Basis Functions\n",
    "----------------------\n",
    "\n",
    "Another type of basis is sometimes known as a ‘radial basis’ because the\n",
    "effect basis functions are constructed on ‘centres’ and the effect of\n",
    "each basis function decreases as the radial distance from each centre\n",
    "increases.\n",
    "\n",
    "$$\\phi_j(x) = \\exp\\left(-\\frac{(x-\\mu_j)^2}{\\ell^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s radial mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "loc = [[-1.25, -0.4],\n",
    "       [0., 1.25],\n",
    "       [1.25, -0.4]]\n",
    "text = ['$\\phi_1(x) = e^{-(x + 1)^2}$',\n",
    "        '$\\phi_2(x) = e^{-2x^2}$', \n",
    "        '$\\phi_3(x) = e^{-2(x-1)^2}$']\n",
    "plot.basis(mlai.radial, x_min=-2, x_max=2, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.radial, num_basis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('radial_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Radial Basis\n",
    "-----------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = \\color{red}{w_1 e^{-2(x+1)^2}}  + \\color{magenta}{w_2e^{-2x^2}} + \\color{blue}{w_3 e^{-2(x-1)^2}}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/radial_basis002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A radial basis is made up of different locally effective\n",
    "functions centered at different points.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('radial_function{func_num:0>3}.svg', directory='./ml', func_num=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified Linear Units\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s relu mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.relu, num_basis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified linear units are popular in the current generation of\n",
    "multilayer perceptron models, or deep networks. These basis functions\n",
    "start flat, and then become linear functions at a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [-1, -0.5],\n",
    "      [-0.33, 0.2],\n",
    "      [0.33, -0.5],\n",
    "      [1, 0.2]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = xH(x+1.0)$',\n",
    "       '$\\phi(x) = xH(x+0.33)$',\n",
    "       '$\\phi(x) = xH(x-0.33)$',\n",
    "       '$\\phi(x) = xH(x-1.0)$']\n",
    "plot.basis(mlai.relu, x_min=-2.0, x_max=2.0, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml',\n",
    "           num_basis=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Relu Basis\n",
    "---------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = \\color{red}{w_0}   + \\color{magenta}{w_1 xH(x+1.0) } + \\color{blue}{w_2 xH(x+0.33) } + \\color{green}{w_3 xH(x-0.33)} +  \\color{cyan}{w_4 xH(x-1.0)}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/relu_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A rectified linear unit basis is made up of different\n",
    "rectified linear unit functions centered at different points.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('relu_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                num_basis=IntSlider(0,0,4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperbolic Tangent Basis\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s tanh mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.tanh, num_basis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid or hyperbolic tangent basis was popular in the original\n",
    "generation of multilayer perceptron models, or deep networks. These\n",
    "basis functions start flat, rise and then saturate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0, 1.4,],\n",
    "      [-1, -0.7],\n",
    "      [-0.33, 0],\n",
    "      [0.33, -0.7],\n",
    "      [1, 0]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = \\\\tanh(x+1.0)$',\n",
    "       '$\\phi(x) = \\\\tanh(x+0.33)$',\n",
    "       '$\\phi(x) = \\\\tanh(x-0.33)$',\n",
    "       '$\\phi(x) = \\\\tanh(x-1.0)$']\n",
    "plot.basis(mlai.tanh, x_min=-2.0, x_max=2.0,\n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml',\n",
    "           num_basis=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Tanh Basis\n",
    "---------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = {\\color{red}w_0}   + {\\color{magenta}w_1 \\text{tanh}\\left(x+1\\right)}  + {\\color{blue}w_2 \\text{tanh}\\left(x+0.33\\right)}  + {\\color{green}w_3 \\text{tanh}\\left(x-0.33\\right)} + {\\color{cyan}w_4 \\text{tanh}\\left(x-1\\right)}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/tanh_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A hyperbolic tangent basis is made up of s-shaped basis\n",
    "functions centered at different points.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('tanh_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier Basis\n",
    "-------------\n",
    "\n",
    "[Joseph Fourier](https://en.wikipedia.org/wiki/Joseph_Fourier) suggested\n",
    "that functions could be converted to a sum of sines and cosines. A\n",
    "Fourier basis is a linear weighted sum of these functions.\n",
    "$$\\phi_j(x) = w_0  + w_1 \\sin(x) + w_2 \\cos(x) + w_3 \\sin(2x) + w_4 \\cos(2x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s fourier mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "loc =[[0., 0.4,],\n",
    "      [0.5, 0.4],\n",
    "      [1, -0.4],\n",
    "      [1.25, 0.4],\n",
    "      [1.5, -0.4]]\n",
    "text =['$\\phi(x) = 1$',\n",
    "       '$\\phi(x) = \\sin(x)$',\n",
    "       '$\\phi(x) = \\cos(x)$',\n",
    "       '$\\phi(x) = \\sin(2x)$',\n",
    "       '$\\phi(x) = \\cos(2x)$']\n",
    "plot.basis(mlai.fourier, x_min=0, x_max=2, \n",
    "           fig=f, ax=ax, loc=loc, text=text,\n",
    "           diagrams='./ml',\n",
    "           num_basis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('fourier_basis{num_basis:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(0,0,4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, basis functions with an *odd* index are sine and basis\n",
    "functions with an *even* index are cosine. The first basis function\n",
    "(index 0, so cosine) has a frequency of 0 and then frequencies increase\n",
    "every time a sine and cosine are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=mlai.fourier, num_basis=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Derived from Fourier Basis\n",
    "------------------------------------\n",
    "\n",
    "$$\n",
    "f(x) = {\\color{red}w_0}  + {\\color{magenta}w_1 \\sin(x)} + {\\color{blue}w_2 \\cos(x)} + {\\color{green}w_3 \\sin(2x)} + {\\color{cyan}w_4 \\cos(2x)}\n",
    "$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/fourier_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Fourier basis is made up of sine and cosine functions with\n",
    "different frequencies.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('fourier_function{func_num:0>3}.svg', directory='./ml', func_num=IntSlider(0,0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Try increasing the number of basis functions (thereby increasing the\n",
    "*degree* of the resulting polynomial). Describe what you see as you\n",
    "increase number of basis up to 10. Is it easy to change the function in\n",
    "intiutive ways?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting to Data\n",
    "---------------\n",
    "\n",
    "Now we are going to consider how these basis functions can be adjusted\n",
    "to fit to a particular data set. We will return to the olympic marathon\n",
    "data from last time. First we will scale the output of the data to be\n",
    "zero mean and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "y = data['Y']\n",
    "x = data['X']\n",
    "y -= y.mean()\n",
    "y /= y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "\n",
    "Now we are going to redefine our polynomial basis. Have a careful look\n",
    "at the operations we perform on `x` to create `z`. We use `z` in the\n",
    "polynomial computation. What are we doing to the inputs? Why do you\n",
    "think we are changing `x` in this manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "### Exercise 0 Answer\n",
    "\n",
    "Write your answer to Exercise 0 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s polynomial mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[:, 0] = np.linspace(1888, 2020, 1000)\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "ax.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_prediction(basis=dict(radial=mlai.radial, \n",
    "                                            polynomial=mlai.polynomial, \n",
    "                                            fourier=mlai.fourier, \n",
    "                                            relu=mlai.relu), \n",
    "                                 data_limits=(1888, 2020),\n",
    "                                 fig=fig, ax=ax,\n",
    "                                 offset=0.,\n",
    "                                 wlim = (-4., 4.),\n",
    "                                 num_basis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use the tool provided above to try and find the best fit you can to the\n",
    "data. Explore the parameter space and give the weight values you used\n",
    "for the\n",
    "\n",
    "1.  polynomial basis\n",
    "2.  Radial basis\n",
    "3.  Fourier basis\n",
    "\n",
    "Write your answers in the code box below creating a new vector of\n",
    "parameters (in the correct order!) for each basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 2 here\n",
    "\n",
    "\n",
    "\n",
    "# (a) polynomial\n",
    "###### Edit these lines #####\n",
    "# w_0 =\n",
    "# w_1 = \n",
    "# w_2 = \n",
    "# w_3 =\n",
    "##############################\n",
    "# w_polynomial = np.asarray([[w_0], [w_1], [w_2], [w_3]]) \n",
    "\n",
    "# (b) radial\n",
    "###### Edit these lines #####\n",
    "# w_0 =\n",
    "# w_1 = \n",
    "# w_2 = \n",
    "# w_3 =\n",
    "##############################\n",
    "# w_rbf = np.asarray([[w_0], [w_1], [w_2], [w_3]]) \n",
    "\n",
    "# (c) fourier\n",
    "###### Edit these lines #####\n",
    "# w_0 =\n",
    "# w_1 = \n",
    "# w_2 = \n",
    "# w_3 =\n",
    "##############################\n",
    "# w_fourier = np.asarray([[w_0], [w_1], [w_2], [w_3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([[1, 2, 3, 4]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Function Models\n",
    "---------------------\n",
    "\n",
    "$$\n",
    "  f(\\mathbf{ x}_i) = \\sum_{j=1}^mw_j \\phi_{i, j}\n",
    "  $$\n",
    "\n",
    "$$\n",
    "  f(\\mathbf{ x}_i) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}_i\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Likelihood for Basis Function Model\n",
    "---------------------------------------\n",
    "\n",
    "$$\n",
    "  p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(y_i-\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{2\\sigma^2}\\right).\n",
    "  $$\n",
    "\n",
    "$$\n",
    "  L(\\mathbf{ w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2-\\frac{n}{2}\\log 2\\pi -\\frac{\\sum_{i=1}^{n}\\left(y_i-\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{2\\sigma^2}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function\n",
    "------------------\n",
    "\n",
    "$$\n",
    "  E(\\mathbf{ w},\\sigma^2)= \\frac{n}{2}\\log\\sigma^2 + \\frac{\\sum_{i=1}^{n}\\left(y_i-\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{2\\sigma^2}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the Brackets\n",
    "-------------------\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  E(\\mathbf{ w},\\sigma^2) = &\\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}y_i\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\\\ &+\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^{\\top}\\mathbf{ w}+\\text{const}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the Brackets\n",
    "-------------------\n",
    "\n",
    "$$\\begin{align} E(\\mathbf{ w}, \\sigma^2) = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2} \\mathbf{ w}^\\top\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i y_i\\\\ & +\\frac{1}{2\\sigma^2}\\mathbf{ w}^{\\top}\\left[\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^{\\top}\\right]\\mathbf{ w}+\\text{const}.\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Matrices\n",
    "---------------\n",
    "\n",
    "We like to make use of *design* matrices for our data. Design matrices,\n",
    "as you will recall, involve placing the data points into rows of the\n",
    "matrix and data features into the columns of the matrix. By convention,\n",
    "we are referincing a vector with a bold lower case letter, and a matrix\n",
    "with a bold upper case letter. The design matrix is therefore given by\n",
    "$$\n",
    "  \\boldsymbol{ \\Phi}= \\begin{bmatrix} \\mathbf{1} & \\mathbf{ x}& \\mathbf{ x}^2\\end{bmatrix}\n",
    "  $$ so that $$\n",
    "  \\boldsymbol{ \\Phi}\\in \\Re^{n\\times p}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Derivatives Reminder\n",
    "---------------------------------\n",
    "\n",
    "$$\\frac{\\text{d}\\mathbf{a}^{\\top}\\mathbf{ w}}{\\text{d}\\mathbf{ w}}=\\mathbf{a}$$\n",
    "and\n",
    "$$\\frac{\\text{d}\\mathbf{ w}^{\\top}\\mathbf{A}\\mathbf{ w}}{\\text{d}\\mathbf{ w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{ w}$$\n",
    "or if $\\mathbf{A}$ is symmetric (*i.e.* $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
    "$$\\frac{\\text{d}\\mathbf{ w}^{\\top}\\mathbf{A}\\mathbf{ w}}{\\text{d}\\mathbf{ w}}=2\\mathbf{A}\\mathbf{ w}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiate\n",
    "-------------\n",
    "\n",
    "Differentiating with respect to the vector $\\mathbf{ w}$ we obtain\n",
    "$$\\frac{\\text{d} E\\left(\\mathbf{ w},\\sigma^2 \\right)}{\\text{d}\\mathbf{ w}}=-\\frac{1}{\\sigma^2} \\sum_{i=1}^{n}\\boldsymbol{ \\phi}_iy_i+\\frac{1}{\\sigma^2} \\left[\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^{\\top}\\right]\\mathbf{ w}$$\n",
    "Leading to\n",
    "$$\\mathbf{ w}^{*}=\\left[\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^{\\top}\\right]^{-1}\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_iy_i,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Notation\n",
    "---------------\n",
    "\n",
    "Rewrite in matrix notation: $$\n",
    "\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^\\top = \\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}$$\n",
    "$$\\sum _{i=1}^{n}\\boldsymbol{ \\phi}_iy_i = \\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Equations\n",
    "----------------\n",
    "\n",
    "$$\n",
    "  \\mathbf{ w}^{*} = \\left(\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}\\right)^{-1} \\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
    "  $$\n",
    "\n",
    "$$\n",
    "  \\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum_{i=1}^{n}\\left(y_i-\\left.\\mathbf{ w}^{*}\\right.^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{n}.\n",
    "  $$\n",
    "\n",
    "$$\n",
    "  \\left(\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}\\right)\\mathbf{ w}= \\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olympic Marathon Data\n",
    "---------------------\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons\n",
    "<a href=\"http://bit.ly/16kMKHQ\" class=\"uri\">http://bit.ly/16kMKHQ</a></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. First we load in the data\n",
    "and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, \n",
    "                  filename='olympic-marathon.svg', \n",
    "                  diagrams='./datasets',\n",
    "                  transparent=True, \n",
    "                  facecolor=(1, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/datasets/olympic-marathon.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1892.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in this\n",
    "year, the olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed.\n",
    "\n",
    "More recent years see more consistently quick marathons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Fits to Olympic Data\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import mlai\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = mlai.polynomial\n",
    "\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "xlim = [1892, 2020]\n",
    "\n",
    "basis=mlai.Basis(mlai.polynomial, number=1, data_limits=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rmse_fit(x, y, param_name='number', param_range=(1, 27), \n",
    "              model=mlai.LM, \n",
    "              basis=basis,\n",
    "              xlim=xlim, objective_ylim=[0, 0.8],\n",
    "              diagrams='./ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_LM_polynomial_number{num_basis:0>3}.svg',\n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(1,1,27,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = mlai.polynomial\n",
    "\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "xlim = [1892, 2020]\n",
    "max_basis = 27\n",
    "\n",
    "ll = np.array([np.nan]*(max_basis))\n",
    "sum_squares = np.array([np.nan]*(max_basis))\n",
    "basis=mlai.Basis(mlai.polynomial, number=1, data_limits=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rmse_fit(x, y, param_name='number', param_range=(1, 28), \n",
    "              model=mlai.LM, basis=basis, \n",
    "              xlim=xlim, objective_ylim=[0, 0.8],\n",
    "              diagrams='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_LM_polynomial_number{num_basis:0>3}.svg',\n",
    "                            directory='./ml', \n",
    "                            num_basis=IntSlider(1,1,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/olympic_LM_polynomial_number002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 1 degree polynomial to the olympic marathon\n",
    "data.</i>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/olympic_LM_polynomial_number003.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of a 2 degree polynomial to the olympic marathon\n",
    "data.</i>\n",
    "\n",
    "::: {.cell .markdown}\n",
    "\n",
    "Non-linear but Linear in the Parameters\n",
    "---------------------------------------\n",
    "\n",
    "One rather nice aspect of our model is that whilst it is non-linear in\n",
    "the inputs, it is still linear in the parameters $\\mathbf{ w}$. This\n",
    "means that our derivations from before continue to operate to allow us\n",
    "to work with this model. In fact, although this is a non-linear\n",
    "regression it is still known as a *linear model* because it is linear in\n",
    "the parameters,\n",
    "\n",
    "$$\n",
    "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})\n",
    "$$ where the vector $\\mathbf{ x}$ appears inside the basis functions,\n",
    "making our result, $f(\\mathbf{ x})$ non-linear in the inputs, but\n",
    "$\\mathbf{ w}$ appears outside our basis function, making our result\n",
    "*linear* in the parameters. In practice, our basis function itself may\n",
    "contain its own set of parameters, $$\n",
    "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x};\n",
    "\\boldsymbol{\\theta}),\n",
    "$$ that we’ve denoted here as $\\boldsymbol{\\theta}$. If these parameters\n",
    "appear inside the basis function then our model is *non-linear* in these\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "\n",
    "For the following prediction functions state whether the model is linear\n",
    "in the inputs, the parameters or both.\n",
    "\n",
    "1.  $f(x) = w_1x_1 + w_2$\n",
    "\n",
    "2.  $f(x) = w_1\\exp(x_1) + w_2x_2 + w_3$\n",
    "\n",
    "3.  $f(x) = \\log(x_1^{w_1}) + w_2x_2^2 + w_3$\n",
    "\n",
    "4.  $f(x) = \\exp(-\\sum_i(x_i - w_i)^2)$\n",
    "\n",
    "5.  $f(x) = \\exp(-\\mathbf{ w}^\\top \\mathbf{ x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0 Answer\n",
    "\n",
    "Write your answer to Exercise 0 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Model Yourself\n",
    "--------------------------\n",
    "\n",
    "You now have everything you need to fit a non- linear (in the inputs)\n",
    "basis function model to the marathon data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Choose one of the basis functions you have explored above. Compute the\n",
    "design matrix on the covariates (or input data), `x`. Use the design\n",
    "matrix and the response variable `y` to solve the following linear\n",
    "system for the model parameters `w`. $$\n",
    "\\boldsymbol{ \\phi}^\\top\\boldsymbol{ \\phi}\\mathbf{ w}= \\boldsymbol{ \\phi}^\\top \\mathbf{ y}\n",
    "$$ Compute the corresponding error on the training data. How does it\n",
    "compare to the error you were able to achieve fitting the basis above?\n",
    "Plot the form of your prediction function from the least squares\n",
    "estimate alongside the form of you prediction function you fitted by\n",
    "hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 3 here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "---------------\n",
    "\n",
    "-   Section 1.4 of Rogers and Girolami (2011)\n",
    "\n",
    "-   Chapter 1, pg 1-6 of Bishop (2006)\n",
    "\n",
    "-   Chapter 3, Section 3.1 up to pg 143 of Bishop (2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture on Basis Functions from GPRS Uganda\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('PoNbOnUnOao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of QR Decomposition for Numerical Stability\n",
    "-----------------------------------------------\n",
    "\n",
    "In the last session we showed how rather than computing\n",
    "$\\mathbf{X}^\\top\\mathbf{X}$ as an intermediate step to our solution, we\n",
    "could compute the solution to the regressiond directly through\n",
    "[QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition). Now\n",
    "we will consider an example with non linear basis functions where such\n",
    "computation is critical for forming the right answer.\n",
    "\n",
    "*TODO* example with polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = fourier(x, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.dot(Phi.T,Phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi*Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bishop, C.M., 2006. Pattern recognition and machine learning. springer.\n",
    "\n",
    "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
    "Press."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
