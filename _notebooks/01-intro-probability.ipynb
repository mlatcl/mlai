{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and an Introduction to Jupyter, Python and Pandas\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of Sheffield\n",
    "### 2015-09-29\n",
    "\n",
    "**Abstract**: In this first session we will introduce *machine learning*, review\n",
    "*probability* and begin familiarization with the Jupyter notebook,\n",
    "python and pandas.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "%\\newcommand{\\tk}[1]{\\textbf{TK}: #1}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "## Course Text \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_mlai/includes/welcome.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/mlai/a-first-course-in-machine-learning.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>The main course text is \"A First Course in Machine Learning\"\n",
    "by @Rogers:book11.</i>\n",
    "\n",
    "}\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/mlai/978-0-387-31073-2.png\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>For additional reading we will regularly refer to \"Pattern\n",
    "Recognition and Machine Learning\" by @Bishop:book06</i>\n",
    "\n",
    "Welcome to the Machine Learning and Adaptive Intelligence course. In\n",
    "this course we will introduce the basic concepts of machine learning and\n",
    "data science. In particular we will look at tools and techniques that\n",
    "describe how to model. An integrated part of that is how we approach\n",
    "data with the computer. We are choosing to do that with the tool you see\n",
    "in front of you: the Jupyter Notebook.\n",
    "\n",
    "The notebook provides us with a way of interacting with the data that\n",
    "allows us to give the computer instructions and explore the nature of a\n",
    "data set. It is *different* to normal coding, but it is related. In this\n",
    "course you will, through intensive practical sessions and labs, develop\n",
    "your understanding of the interaction between data and computers. The\n",
    "first thing we are going to do is ask you to forget a bit about what you\n",
    "think about normal programming, or 'classical software engineering'.\n",
    "Classical software engineering demands a large amount of design and\n",
    "testing. In data analysis, testing remains very important, but the\n",
    "design is often evolving. The design evolves through a process known as\n",
    "*exploratory data analysis*. You will learn some of the techniques of\n",
    "exploratory data analysis in this course.\n",
    "\n",
    "A particular difference between classical software engineering and data\n",
    "analysis is the way in which programs are run. Classically we spend a\n",
    "deal of time working with a text editor, writing code. Compilations are\n",
    "done on a regular basis and aspects of the code are tested (perhaps with\n",
    "unit tests).\n",
    "\n",
    "Data analysis is more like coding in a debugger. In a debugger\n",
    "(particularly a visual debugger) you interact with the data stored in\n",
    "the memory of the computer to try and understand what is happening in\n",
    "the computer, you need to understand exactly what your bug is: you often\n",
    "have a fixed idea of what the program is trying to do, you are just\n",
    "struggling to find out why it isn't doing it.\n",
    "\n",
    "Naturally, debugging is an important part of data analysis also, but in\n",
    "some sense it can be seen as its entire premise. You load in a data set\n",
    "into a computer that you don't understand, your entire objective is to\n",
    "understand the data. This is best done by interogating the data to\n",
    "visualise it or summarize it, just like in a power visual debugger.\n",
    "However, for data science the requirements for visualization and\n",
    "summarization are far greater than in a regular program. When the data\n",
    "is well understood, the actual number of lines of your program may well\n",
    "be very few (particularly if you disregard commands that load in the\n",
    "data and commands which plot your results). If a powerful data science\n",
    "library is available, you may be able to summarize your code with just\n",
    "two or three lines, but the amount of intellectual energy that is\n",
    "expended on writing those three lines is far greater than in standard\n",
    "code.\n",
    "\n",
    "# Assumed Knowledge \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_mlai/includes/assumed-knowledge.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "## Linear Algebra, Probability and Differential Calculus\n",
    "\n",
    "We will be assuming that you have good background in maths. In\n",
    "particular we will be making use of linear algrebra (matrix operations\n",
    "including inverse, inner products, determinant etc), probability (sum\n",
    "rule of probability, product rule of probability), and the calculus of\n",
    "variation, e.g. differentiation and integration. A new concept for the\n",
    "course is multivariate differentiation and integration. This combines\n",
    "linear algebra and differential calculus. These techniques are vital in\n",
    "understanding probability distributions over high dimensional\n",
    "distributions.\n",
    "\n",
    "## Choice of Language \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_mlai/includes/environment-python-jupyter.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "In this course we will be using Python for our programming language. A\n",
    "prerequisite of attending this course is that you have learnt at least\n",
    "one programming language in the past. It is not our objective to teach\n",
    "you python. At Level 4 and Masters we expect our students to be able\n",
    "pick up a language as they go. If you have not experienced python before\n",
    "it may be worth your while spending some time understanding the\n",
    "language. There are resources available for you to do this\n",
    "[here](https://docs.python.org/2/tutorial/) that are based on the\n",
    "standard console. An introduction to the Jupyter notebook (formerly\n",
    "known as the IPython notebook) is available\n",
    "[here](http://ipython.org/ipython-%20doc/2/notebook/index.html).\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Who invented python and why? What was the language designed to do? What\n",
    "is the origin of the name \"python\"? Is the language a compiled language?\n",
    "Is it an object orientated language?\n",
    "\n",
    "*10 marks*\n",
    "\n",
    "### Write your answer to Question 1 here\n",
    "\n",
    "# Choice of Environment\n",
    "\n",
    "We are working in the Jupyter notebook (formerly known as the IPython\n",
    "notebook). It provides an environment for interacting with data in a\n",
    "natural way which is reproducible. We will be learning how to make use\n",
    "of the notebook throughout the course. The notebook allows us to combine\n",
    "code with descriptions, interactive visualizations, plots etc. In fact\n",
    "it allows us to do many of the things we need for data science.\n",
    "Notebooks can also be easily shared through the internet for ease of\n",
    "communication of ideas. The box this text is written in is a *markdown*\n",
    "box. Below we have a *code* box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the Jupyter notebook\")\n",
    "print(\"It provides a platform for:\")\n",
    "words = ['Open', 'Data', 'Science']\n",
    "from random import shuffle\n",
    "for i in range(3):\n",
    "    shuffle(words)\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a play with the code in the above box. Think about the following\n",
    "questions: what is the difference between `CTRL-enter` and `SHIFT-enter`\n",
    "in running the code? What does the command `shuffle` do? Can you find\n",
    "out by typing `shuffle?` in a code box? Once you've had a play with the\n",
    "code we can load in some data using the `pandas` library for data\n",
    "analysis.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "What is jupyter and why was it invented? Give some examples of\n",
    "functionality it gives over standard python. What is the jupyter\n",
    "project? Name two languages involved in the Jupyter project other than\n",
    "python.\n",
    "\n",
    "*10 marks*\n",
    "\n",
    "### Write your answer to Question 3 here\n",
    "\n",
    "# What is Machine Learning? \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "What is machine learning? At its most basic level machine learning is a\n",
    "combination of\n",
    "\n",
    "$$\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}$$\n",
    "\n",
    "where *data* is our observations. They can be actively or passively\n",
    "acquired (meta-data). The *model* contains our assumptions, based on\n",
    "previous experience. That experience can be other data, it can come from\n",
    "transfer learning, or it can merely be our beliefs about the\n",
    "regularities of the universe. In humans our models include our inductive\n",
    "biases. The *prediction* is an action to be taken or a categorization or\n",
    "a quality score. The reason that machine learning has become a mainstay\n",
    "of artificial intelligence is the importance of predictions in\n",
    "artificial intelligence. The data and the model are combined through\n",
    "computation.\n",
    "\n",
    "In practice we normally perform machine learning using two functions. To\n",
    "combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** a function which is used to make the\n",
    "predictions. It includes our beliefs about the regularities of the\n",
    "universe, our assumptions about how the world works, e.g. smoothness,\n",
    "spatial similarities, temporal similarities.\n",
    "\n",
    "**an objective function** a function which defines the cost of\n",
    "misprediction. Typically it includes knowledge about the world's\n",
    "generating processes (probabilistic objectives) or the costs we pay for\n",
    "mispredictions (empiricial risk minimization).\n",
    "\n",
    "The combination of data and model through the prediction function and\n",
    "the objectie function leads to a *learning algorithm*. The class of\n",
    "prediction functions and objective functions we can make use of is\n",
    "restricted by the algorithms they lead to. If the prediction function or\n",
    "the objective function are too complex, then it can be difficult to find\n",
    "an appropriate learning algorithm. Much of the acdemic field of machine\n",
    "learning is the quest for new learning algorithms that allow us to bring\n",
    "different types of models and data together.\n",
    "\n",
    "A useful reference for state of the art in machine learning is the UK\n",
    "Royal Society Report, [Machine Learning: Power and Promise of Computers\n",
    "that Learn by\n",
    "Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf).\n",
    "\n",
    "You can also check my post blog post on [What is Machine\n",
    "Learning?](http://inverseprobability.com/2017/07/17/what-is-machine-learning)..\n",
    "\n",
    "\\defined{overdeterminedInaugural}\n",
    "## Overdetermined System \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/overdetermined-inaugural.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "The challenge with a linear model is that it has two unknowns, $m$, and\n",
    "$c$. Observing data allows us to write down a system of simultaneous\n",
    "linear equations. So, for example if we observe two data points, the\n",
    "first with the input value, $\\inputScalar_1 = 1$ and the output value,\n",
    "$\\dataScalar_1 =3$ and a second data point, $\\inputScalar = 3$,\n",
    "$\\dataScalar=1$, then we can write two simultaneous linear equations of\n",
    "the form.\n",
    "\n",
    "point 1: $\\inputScalar = 1$, $\\dataScalar=3$ $$3 = m + c$$ point 2:\n",
    "$\\inputScalar = 3$, $\\dataScalar=1$ $$1 = 3m + c$$\n",
    "\n",
    "The solution to these two simultaneous equations can be represented\n",
    "graphically as\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/over_determined_system003.svg\" class=\"\" align=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The solution of two linear equations represented as the fit\n",
    "of a straight line through two data</i>\n",
    "\n",
    "The challenge comes when a third data point is observed and it doesn't\n",
    "naturally fit on the straight line.\n",
    "\n",
    "point 3: $\\inputScalar = 2$, $\\dataScalar=2.5$ $$2.5 = 2m + c$$\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/over_determined_system004.svg\" class=\"\" align=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A third observation of data is inconsistent with the solution\n",
    "dictated by the first two observations</i>\n",
    "\n",
    "Now there are three candidate lines, each consistent with our data.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/over_determined_system007.svg\" class=\"\" align=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Three solutions to the problem, each consistent with two\n",
    "points of the three observations</i>\n",
    "\n",
    "This is known as an *overdetermined* system because there are more data\n",
    "than we need to determine our parameters. The problem arises because the\n",
    "model is a simplification of the real world, and the data we observe is\n",
    "therefore inconsistent with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('over_determined_system{samp:0>3}.svg',\n",
    "                            directory='../slides/diagrams/ml', \n",
    "                            samp=IntSlider(1,1,7,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution was proposed by Pierre-Simon Laplace. His idea was to\n",
    "accept that the model was an incomplete representation of the real\n",
    "world, and the manner in which it was incomplete is *unknown*. His idea\n",
    "was that such unknowns could be dealt with through probability.\n",
    "\n",
    "### Pierre-Simon Laplace \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_physics/includes/laplace-portrait.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/Pierre-Simon_Laplace.png\" style=\"width:30%\">\n",
    "\n",
    "Figure: <i>Pierre-Simon Laplace 1749-1827.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famously, Laplace considered the idea of a deterministic Universe, one\n",
    "in which the model is *known*, or as the below translation refers to it,\n",
    "\"an intelligence which could comprehend all the forces by which nature\n",
    "is animated\". He speculates on an \"intelligence\" that can submit this\n",
    "vast data to analysis and propsoses that such an entity would be able to\n",
    "predict the future.\n",
    "\n",
    "> Given for one instant an intelligence which could comprehend all the\n",
    "> forces by which nature is animated and the respective situation of the\n",
    "> beings who compose it---an intelligence sufficiently vast to submit\n",
    "> these data to analysis---it would embrace in the same formulate the\n",
    "> movements of the greatest bodies of the universe and those of the\n",
    "> lightest atom; for it, nothing would be uncertain and the future, as\n",
    "> the past, would be present in its eyes.\n",
    "\n",
    "This notion is known as *Laplace's demon* or *Laplace's superman*.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/laplacesDeterminismEnglish.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Laplace's determinsim in English translation.</i>\n",
    "\n",
    "Unfortunately, most analyses of his ideas stop at that point, whereas\n",
    "his real point is that such a notion is unreachable. Not so much\n",
    "*superman* as *strawman*. Just three pages later in the \"Philosophical\n",
    "Essay on Probabilities\" [@Laplace:essai14], Laplace goes on to observe:\n",
    "\n",
    "> The curve described by a simple molecule of air or vapor is regulated\n",
    "> in a manner just as certain as the planetary orbits; the only\n",
    "> difference between them is that which comes from our ignorance.\n",
    ">\n",
    "> Probability is relative, in part to this ignorance, in part to our\n",
    "> knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/physics/philosophicaless00lapliala.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>To Laplace, determinism is a strawman. Ignorance of mechanism\n",
    "and data leads to uncertainty which should be dealt with through\n",
    "probability.</i>\n",
    "\n",
    "In other words, we can never make use of the idealistic deterministic\n",
    "Universe due to our ignorance about the world, Laplace's suggestion, and\n",
    "focus in this essay is that we turn to probability to deal with this\n",
    "uncertainty. This is also our inspiration for using probability in\n",
    "machine learning.\n",
    "\n",
    "The \"forces by which nature is animated\" is our *model*, the \"situation\n",
    "of beings that compose it\" is our *data* and the \"intelligence\n",
    "sufficiently vast enough to submit these data to analysis\" is our\n",
    "compute. The fly in the ointment is our *ignorance* about these aspects.\n",
    "And *probability* is the tool we use to incorporate this ignorance\n",
    "leading to uncertainty or *doubt* in our predictions.\n",
    "\n",
    "Laplace's concept was that the reason that the data doesn't match up to\n",
    "the model is because of unconsidered factors, and that these might be\n",
    "well represented through probability densities. He tackles the challenge\n",
    "of the unknown factors by adding a variable, $\\noiseScalar$, that\n",
    "represents the unknown. In modern parlance we would call this a *latent*\n",
    "variable. But in the context Laplace uses it, the variable is so common\n",
    "that it has other names such as a \"slack\" variable or the *noise* in the\n",
    "system.\n",
    "\n",
    "point 1: $\\inputScalar = 1$, $\\dataScalar=3$ $$\n",
    "3 = m + c + \\noiseScalar_1\n",
    "$$ point 2: $\\inputScalar = 3$, $\\dataScalar=1$ $$\n",
    "1 = 3m + c + \\noiseScalar_2\n",
    "$$ point 3: $\\inputScalar = 2$, $\\dataScalar=2.5$ $$\n",
    "2.5 = 2m + c + \\noiseScalar_3\n",
    "$$\n",
    "\n",
    "Laplace's trick has converted the *overdetermined* system into an\n",
    "*underdetermined* system. He has now added three variables,\n",
    "$\\{\\noiseScalar_i\\}_{i=1}^3$, which represent the unknown corruptions of\n",
    "the real world. Laplace's idea is that we should represent that unknown\n",
    "corruption with a *probability distribution*.\n",
    "\n",
    "## A Probabilistic Process \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/overdetermined-inaugural.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "However, it was left to an admirer of Gauss to develop a practical\n",
    "probability density for that purpose. It was Carl Friederich Gauss who\n",
    "suggested that the *Gaussian* density (which at the time was unnamed!)\n",
    "should be used to represent this error.\n",
    "\n",
    "The result is a *noisy* function, a function which has a deterministic\n",
    "part, and a stochastic part. This type of function is sometimes known as\n",
    "a probabilistic or stochastic process, to distinguish it from a\n",
    "deterministic process.\n",
    "\n",
    "# Nigerian NMIS Data \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigerian-nmis-data.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "As an example data set we will use Nigerian NMIS Health Facility data\n",
    "from openAFRICA. It can be found here\n",
    "<https://africaopendata.org/dataset/nigeria-nmis-health-facility-data-2014>\n",
    "\n",
    "Taking from the information on the site,\n",
    "\n",
    "> The Nigeria MDG (Millennium Development Goals) Information System --\n",
    "> NMIS health facility data is collected by the Office of the Senior\n",
    "> Special Assistant to the President on the Millennium Development Goals\n",
    "> (OSSAP-MDGs) in partner with the Sustainable Engineering Lab at\n",
    "> Columbia University. A rigorous, geo-referenced baseline facility\n",
    "> inventory across Nigeria is created spanning from 2009 to 2011 with an\n",
    "> additional survey effort to increase coverage in 2014, to build\n",
    "> Nigeria's first nation-wide inventory of health facility. The database\n",
    "> includes 34,139 health facilities info in Nigeria.\n",
    ">\n",
    "> The goal of this database is to make the data collected available to\n",
    "> planners, government officials, and the public, to be used to make\n",
    "> strategic decisions for planning relevant interventions.\n",
    ">\n",
    "> For data inquiry, please contact Ms. Funlola Osinupebi, Performance\n",
    "> Monitoring & Communications, Advisory Power Team, Office of the Vice\n",
    "> President at funlola.osinupebi\\@aptovp.org\n",
    ">\n",
    "> To learn more, please visit\n",
    "> <http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/>\n",
    ">\n",
    "> Suggested citation: Nigeria NMIS facility database (2014), the Office\n",
    "> of the Senior Special Assistant to the President on the Millennium\n",
    "> Development Goals (OSSAP-MDGs) & Columbia University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://energydata.info/dataset/f85d1796-e7f2-4630-be84-79420174e3bd/resource/6e640a13-cab4-457b-b9e6-0336051bac27/download/healthmopupandbaselinenmisfacility.csv', 'healthmopupandbaselinenmisfacility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('healthmopupandbaselinenmisfacility.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is loaded in the data can be summarized using the `describe`\n",
    "method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python and jupyter notebook it is possible to see a list of all\n",
    "possible functions and attributes by typing the name of the object\n",
    "followed by `.<Tab>` for example in the above case if we type\n",
    "`data.<Tab>` it show the columns available (these are attributes in\n",
    "pandas dataframes) such as `num_nurses_fulltime`, and also functions,\n",
    "such as `.describe()`.\n",
    "\n",
    "For functions we can also see the documentation about the function by\n",
    "following the name with a question mark. This will open a box with\n",
    "documentation at the bottom which can be closed with the x button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NMIS facility data is stored in an object known as a 'data frame'.\n",
    "Data frames come from the statistical family of programming languages\n",
    "based on `S`, the most widely used of which is\n",
    "[`R`](http://en.wikipedia.org/wiki/R_(programming_language)). The data\n",
    "frame gives us a convenient object for manipulating data. The describe\n",
    "method summarizes which columns there are in the data frame and gives us\n",
    "counts, means, standard deviations and percentiles for the values in\n",
    "those columns. To access a column directly we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['num_doctors_fulltime'])\n",
    "#print(data['num_nurses_fulltime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the number of doctors per facility, number of nurses and\n",
    "number of community health workers (CHEWS). We can plot the number of\n",
    "doctors against the number of nurses as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ensures the plot appears in the web browser\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt # this imports the plotting library in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(data['num_doctors_fulltime'], data['num_nurses_fulltime'], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be curious what the arguments we give to `plt.plot` are for, now\n",
    "is the perfect time to look at the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately note that some facilities have a lot of nurses, which\n",
    "prevent's us seeing the detail of the main number of facilities. First\n",
    "lets identify the facilities with the most nurses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['num_nurses_fulltime']>100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the command `data['num_nurses_fulltime']>100` to index\n",
    "the facilities in the pandas data frame which have over 100 nurses. To\n",
    "sort them in order we can also use the `sort` command. The result of\n",
    "this command on its own is a data `Series` of `True` and `False` values.\n",
    "However, when it is passed to the `data` data frame it returns a new\n",
    "data frame which contains only those values for which the data series is\n",
    "`True`. We can also sort the result. To sort the result by the values in\n",
    "the `num_nurses_fulltime` column in *descending* order we use the\n",
    "following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['num_nurses_fulltime']>100].sort_values(by='num_nurses_fulltime', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the 'University of Calabar Teaching Hospital' is a large\n",
    "outlier with 513 nurses. We can try and determine how much of an outlier\n",
    "by histograming the data.\n",
    "\n",
    "## Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_nurses_fulltime'].hist(bins=20) # histogram the data with 20 bins.\n",
    "plt.title('Histogram of Number of Nurses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see very much here. Two things are happening. There are so many\n",
    "facilities with zero or one nurse that we don't see the histogram for\n",
    "hospitals with many nurses. We can try more bins and using a *log* scale\n",
    "on the $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_nurses_fulltime'].hist(bins=100) # histogram the data with 20 bins.\n",
    "plt.title('Histogram of Number of Nurses')\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Read on the internet about the following python libraries: `numpy`,\n",
    "`matplotlib`, `scipy` and `pandas`. What functionality does each provide\n",
    "python?\n",
    "\n",
    "*10 marks*\n",
    "\n",
    "### Write your answer to Question 1 here\n",
    "\n",
    "Let's try and see how the number of nurses relates to the number of\n",
    "doctors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7)) \n",
    "ax.plot(data['num_doctors_fulltime'], data['num_nurses_fulltime'], 'rx')\n",
    "ax.set_xscale('log') # use a logarithmic x scale\n",
    "ax.set_yscale('log') # use a logarithmic Y scale\n",
    "# give the plot some titles and labels\n",
    "plt.title('Number of Nurses against Number of Doctors')\n",
    "plt.ylabel('number of nurses')\n",
    "plt.xlabel('number of doctors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a few things. We are interacting with our data. In particular, we\n",
    "are replotting the data according to what we have learned so far. We are\n",
    "using the progamming language as a *scripting* language to give the\n",
    "computer one command or another, and then the next command we enter is\n",
    "dependent on the result of the previous. This is a very different\n",
    "paradigm to classical software engineering. In classical software\n",
    "engineering we normally write many lines of code (entire object classes\n",
    "or functions) before compiling the code and running it. Our approach is\n",
    "more similar to the approach we take whilst debugging. Historically,\n",
    "researchers interacted with data using a *console*. A command line\n",
    "window which allowed command entry. The notebook format we are using is\n",
    "slightly different. Each of the code entry boxes acts like a separate\n",
    "console window. We can move up and down the notebook and run each part\n",
    "in a different order. The *state* of the program is always as we left it\n",
    "after running the previous part.\n",
    "\n",
    "# Probabilities \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/probability-intro.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "We are now going to do some simple review of probabilities and use this\n",
    "review to explore some aspects of our data.\n",
    "\n",
    "A probability distribution expresses uncertainty about the outcome of an\n",
    "event. We often encode this uncertainty in a variable. So if we are\n",
    "considering the outcome of an event, $Y$, to be a coin toss, then we\n",
    "might consider $Y=1$ to be heads and $Y=0$ to be tails. We represent the\n",
    "probability of a given outcome with the notation: $$\n",
    "P(Y=1) = 0.5\n",
    "$$ The first rule of probability is that the probability must normalize.\n",
    "The sum of the probability of all events must equal 1. So if the\n",
    "probability of heads ($Y=1$) is 0.5, then the probability of tails (the\n",
    "only other possible outcome) is given by $$\n",
    "P(Y=0) = 1-P(Y=1) = 0.5\n",
    "$$\n",
    "\n",
    "Probabilities are often defined as the limit of the ratio between the\n",
    "number of positive outcomes (e.g. *heads*) given the number of trials.\n",
    "If the number of positive outcomes for event $y$ is denoted by $n$ and\n",
    "the number of trials is denoted by $N$ then this gives the ratio $$\n",
    "P(Y=y) = \\lim_{N\\rightarrow\n",
    "\\infty}\\frac{n_y}{N}.\n",
    "$$ In practice we never get to observe an event infinite times, so\n",
    "rather than considering this we often use the following estimate $$\n",
    "P(Y=y) \\approx \\frac{n_y}{N}.\n",
    "$$\n",
    "\n",
    "## Probability and the NMIS Data \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/probability-nigerian-nmis.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "Let's use the sum rule to compute the estimate the probability that a\n",
    "facility has more than two nurses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large = (data.num_nurses_fulltime>2).sum()  # number of positive outcomes (in sum True counts as 1, False counts as 0)\n",
    "total_facilities = data.num_nurses_fulltime.count()\n",
    "\n",
    "prob_large = float(large)/float(total_facilities)\n",
    "print(\"Probability of number of nurses being greather than 2 is:\", prob_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning\n",
    "\n",
    "When predicting whether a coin turns up head or tails, we might think\n",
    "that this event is *independent* of the year or time of day. If we\n",
    "include an observation such as time, then in a probability this is known\n",
    "as *condtioning*. We use this notation, $P(Y=y|X=x)$, to condition the\n",
    "outcome on a second variable (in this case the number of doctors). Or,\n",
    "often, for a shorthand we use $P(y|x)$ to represent this distribution\n",
    "(the $Y=$ and $X=$ being implicit). If two variables are independent\n",
    "then we find that $$\n",
    "P(y|x) = p(y).\n",
    "$$ However, we might believe that the number of nurses is dependent on\n",
    "the number of doctors. For this we can try estimating $P(Y>2 | X>1)$ and\n",
    "compare the result, for example to $P(Y>2|X\\leq 1)$ using our empirical\n",
    "estimate of the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large = ((data.num_nurses_fulltime>2) & (data.num_doctors_fulltime>1)).sum()\n",
    "total_large_doctors = (data.num_doctors_fulltime>1).sum()\n",
    "prob_both_large = large/total_large_doctors\n",
    "print(\"Probability of number of nurses being greater than 2 given number of doctors is greater than 1 is:\", prob_both_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Write code that prints out the probability of nurses being greater than\n",
    "2 for different numbers of doctors.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for your answer to Question 2 in this box\n",
    "# provide the answers so that the code runs correctly otherwise you will loose marks!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes for Question\n",
    "\n",
    "Make sure the plot is included in *this* notebook file (the `IPython`\n",
    "magic command `%matplotlib inline` we ran above will do that for you, it\n",
    "only needs to be run once per file).\n",
    "\n",
    "  Terminology   Mathematical notation   Description\n",
    "  ------------- ----------------------- ----------------------------------\n",
    "  joint         $P(X=x, Y=y)$           prob. that X=x *and* Y=y\n",
    "  marginal      $P(X=x)$                prob. that X=x *regardless of* Y\n",
    "  conditional   $P(X=x\\vert Y=y)$       prob. that X=x *given that* Y=y\n",
    "\n",
    "<center>\n",
    "The different basic probability distributions.\n",
    "</center>\n",
    "## A Pictorial Definition of Probability \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/probability-review.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/mlai/prob_diagram.svg\" class=\"\" align=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Diagram representing the different probabilities, joint,\n",
    "marginal and conditional. This diagram was inspired by lectures given by\n",
    "Christopher Bishop.</i>\n",
    "\n",
    "[Inspired by lectures from Christopher Bishop]{style=\"text-align:right\"}\n",
    "\n",
    "## Definition of probability distributions\n",
    "\n",
    "  ------------------------------------------------------------------------------------------------------\n",
    "  Terminology    Definition                                               Probability Notation\n",
    "  -------------- -------------------------------------------------------- ------------------------------\n",
    "  Joint          $\\lim_{N\\rightarrow\\infty}\\frac{n_{X=3,Y=4}}{N}$         $P\\left(X=3,Y=4\\right)$\n",
    "  Probability                                                             \n",
    "\n",
    "  Marginal       $\\lim_{N\\rightarrow\\infty}\\frac{n_{X=5}}{N}$             $P\\left(X=5\\right)$\n",
    "  Probability                                                             \n",
    "\n",
    "  Conditional    $\\lim_{N\\rightarrow\\infty}\\frac{n_{X=3,Y=4}}{n_{Y=4}}$   $P\\left(X=3\\vert Y=4\\right)$\n",
    "  Probability                                                             \n",
    "  ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Notational Details\n",
    "\n",
    "Typically we should write out $P\\left(X=x,Y=y\\right)$, but in practice\n",
    "we often shorten this to $P\\left(x,y\\right)$. This looks very much like\n",
    "we might write a multivariate function, *e.g.* $$\n",
    "  f\\left(x,y\\right)=\\frac{x}{y},\n",
    "  $$ but for a multivariate function $$\n",
    "f\\left(x,y\\right)\\neq f\\left(y,x\\right).\n",
    "$$ However, $$\n",
    "P\\left(x,y\\right)=P\\left(y,x\\right)\n",
    "$$ because $$\n",
    "P\\left(X=x,Y=y\\right)=P\\left(Y=y,X=x\\right).\n",
    "$$ Sometimes I think of this as akin to the way in Python we can write\n",
    "'keyword arguments' in functions. If we use keyword arguments, the\n",
    "ordering of arguments doesn't matter.\n",
    "\n",
    "We've now introduced conditioning and independence to the notion of\n",
    "probability and computed some conditional probabilities on a practical\n",
    "example The scatter plot of deaths vs year that we created above can be\n",
    "seen as a *joint* probability distribution. We represent a joint\n",
    "probability using the notation $P(Y=y, X=x)$ or $P(y, x)$ for short.\n",
    "Computing a joint probability is equivalent to answering the\n",
    "simultaneous questions, what's the probability that the number of nurses\n",
    "was over 2 and the number of doctors was 1? Or any other question that\n",
    "may occur to us. Again we can easily use pandas to ask such questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_doctors = 1\n",
    "large = (data.num_nurses_fulltime[data.num_doctors_fulltime==num_doctors]>2).sum()\n",
    "total_facilities = data.num_nurses_fulltime.count() # this is total number of films\n",
    "prob_large = float(large)/float(total_facilities)\n",
    "print(\"Probability of nurses being greater than 2 and number of doctors being\", num_doctors, \"is:\", prob_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Product Rule\n",
    "\n",
    "This number is the joint probability, $P(Y, X)$ which is much *smaller*\n",
    "than the conditional probability. The number can never be bigger than\n",
    "the conditional probabililty because it is computed using the *product\n",
    "rule*. $$\n",
    "p(Y=y, X=x) = p(Y=y|X=x)p(X=x)\n",
    "$$ and $$p(X=x)$$ is a probability distribution, which is equal or less\n",
    "than 1, ensuring the joint distribution is typically smaller than the\n",
    "conditional distribution.\n",
    "\n",
    "The product rule is a *fundamental* rule of probability, and you must\n",
    "remember it! It gives the relationship between the two questions: 1)\n",
    "What's the probability that a facility has over two nurses *and* one\n",
    "doctor? and 2) What's the probability that a facility has over two\n",
    "nurses *given that* it has one doctor?\n",
    "\n",
    "In our shorter notation we can write the product rule as $$\n",
    "p(y, x) = p(y|x)p(x)\n",
    "$$ We can see the relation working in practice for our data above by\n",
    "computing the different values for $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_doctors=1\n",
    "num_nurses=2\n",
    "p_x = float((data.num_doctors_fulltime==num_doctors).sum())/float(data.num_nurses_fulltime.count())\n",
    "p_y_given_x = float((data.num_nurses_fulltime[data.num_doctors_fulltime==num_doctors]>num_nurses).sum())/float((data.num_doctors_fulltime==num_doctors).sum())\n",
    "p_y_and_x = float((data.num_nurses_fulltime[data.num_doctors_fulltime==num_doctors]>num_nurses).sum())/float(data.num_nurses_fulltime.count())\n",
    "\n",
    "print(\"P(x) is\", p_x)\n",
    "print(\"P(y|x) is\", p_y_given_x)\n",
    "print(\"P(y,x) is\", p_y_and_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sum Rule\n",
    "\n",
    "The other *fundamental rule* of probability is the *sum rule* this tells\n",
    "us how to get a *marginal* distribution from the joint distribution.\n",
    "Simply put it says that we need to sum across the value we'd like to\n",
    "remove. $$\n",
    "P(Y=y) = \\sum_{x} P(Y=y, X=x)\n",
    "$$ Or in our shortened notation $$\n",
    "P(y) = \\sum_{x} P(y, x)\n",
    "$$\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Write code that computes $P(y)$ by adding $P(y, x)$ for all values of\n",
    "$x$.\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for your answer to Question 3 in this box\n",
    "# provide the answers so that the code runs correctly otherwise you will loose marks!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "Bayes rule is a very simple rule, it's hardly worth the name of a rule\n",
    "at all. It follows directly from the product rule of probability.\n",
    "Because $P(y, x) = P(y|x)P(x)$ and by symmetry\n",
    "$P(y,x)=P(x,y)=P(x|y)P(y)$ then by equating these two equations and\n",
    "dividing through by $P(y)$ we have $$\n",
    "P(x|y) =\n",
    "\\frac{P(y|x)P(x)}{P(y)}\n",
    "$$ which is known as Bayes' rule (or Bayes's rule, it depends how you\n",
    "choose to pronounce it). It's not difficult to derive, and its\n",
    "importance is more to do with the semantic operation that it enables.\n",
    "Each of these probability distributions represents the answer to a\n",
    "question we have about the world. Bayes rule (via the product rule)\n",
    "tells us how to *invert* the probability.\n",
    "\n",
    "\\addreading{@Bishop:book06}{Probability distributions: page 12–17 (Section 1.2)}\n",
    "\\addexercise{@Bishop:book06}{Exercise 1.3}\n",
    "\\reading\n",
    "\\exercises\n",
    "## Probabilities for Extracting Information from Data \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/probability-review.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "What use is all this probability in data science? Let's think about how\n",
    "we might use the probabilities to do some decision making. Let's look at\n",
    "the information data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Now we see we have several additional features. Let's assume we want to\n",
    "predict `maternal_health_delivery_services`. How would we go about doing\n",
    "it?\n",
    "\n",
    "Using what you've learnt about joint, conditional and marginal\n",
    "probabilities, as well as the sum and product rule, how would you\n",
    "formulate the question you want to answer in terms of probabilities?\n",
    "Should you be using a joint or a conditional distribution? If it's\n",
    "conditional, what should the distribution be over, and what should it be\n",
    "conditioned on?\n",
    "\n",
    "*20 marks*\n",
    "\n",
    "### Write your answer to Question 4 here\n",
    "\n",
    "## Assignment Questions\n",
    "\n",
    "The questions in the above lab sheet need to be answered and handed in\n",
    "before 09:00 on 7th October 2014 (i.e. before next lecture). The hand\n",
    "should be done via file upload through [MOLE](http://vle.shef.ac.uk).\n",
    "\n",
    "<!--## More Fun on the Python Data Farm \n",
    "\n",
    "If you want to explore more of the things\n",
    "you can do with movies and python you might be interested in the `imdbpy` python\n",
    "library.\n",
    "\n",
    "You can try installing it using `pip` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install IMDbPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this doesn't work on your machine, try following instructions on\n",
    "(http://imdbpy.sourceforge.net/)\n",
    "\n",
    "Once you've installed `imdbpy` you can test it\n",
    "works with the following script, which should list movies with the word 'python'\n",
    "in their title. To run the code in the following box, simply click the box and\n",
    "press `SHIFT-enter` or `CTRL-enter`. Then you can try running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imdb import IMDb\n",
    "ia = IMDb()\n",
    "\n",
    "for movie in ia.search_movie('python'):\n",
    "    print(movie)\n",
    "```-->\n",
    "``` {.python}\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('GX8VLYUYScM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   See probability review at end of slides for reminders.\n",
    "\n",
    "\\addreading{@Rogers:book11}{Section 2.2 (pg 41–53)}\n",
    "\\addreading{@Rogers:book11}{Section 2.4 (pg 55–58)}\n",
    "\\addreading{@Rogers:book11}{Section 2.5.1 (pg 58–60)}\n",
    "\\addreading{@Rogers:book11}{Section 2.5.3 (pg 61–62)}\n",
    "-   For other material in read:\n",
    "\n",
    "\\addreading{@Bishop:book06}{Probability densities: Section 1.2.1 (Pages 17–19)}\n",
    "\\addreading{@Bishop:book06}{Expectations and Covariances: Section 1.2.2 (Pages 19–20)}\n",
    "\\addreading{@Bishop:book06}{The Gaussian density: Section 1.2.4 (Pages 24–28) (don’t worry about material on bias)}\n",
    "\\addreading{@Bishop:book06}{For material on information theory and KL divergence try Section 1.6 & 1.6.1 (pg 48 onwards)}\n",
    "-   If you are unfamiliar with probabilities you should complete the\n",
    "    following exercises:\n",
    "\n",
    "\\addexercise{@Bishop:book06}{Exercise 1.7}\n",
    "\\addexercise{@Bishop:book06}{Exercise 1.8}\n",
    "\\addexercise{@Bishop:book06}{Exercise 1.9}\n",
    "\\reading\n",
    "\\exercises\n",
    "# References {#references .unnumbered}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
