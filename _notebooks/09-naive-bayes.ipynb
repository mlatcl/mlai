{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Classification: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In the last lecture we looked at unsupervised learning. We\n",
    "introduced latent variables, dimensionality reduction and clustering. In\n",
    "this lecture we’re going to look at clustering, specifically the\n",
    "probabilistic approach to clustering. We’ll focus on a simple but often\n",
    "effective algorithm known as *naive Bayes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_notebooks/includes/notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_notebooks/includes/notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notutils\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/notutils-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/notutils-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "This small package is a helper package for various notebook utilities\n",
    "used\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install notutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub:\n",
    "<https://github.com/lawrennd/notutils>\n",
    "\n",
    "Once `notutils` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pods\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/pods-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/pods-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science.’ Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/ods>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlai\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/mlai-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/mlai-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The `mlai` software is a suite of helper functions for teaching and\n",
    "demonstrating machine learning algorithms. It was first used in the\n",
    "Machine Learning and Adaptive Intelligence course in Sheffield in 2013.\n",
    "\n",
    "The software can be installed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/mlai>\n",
    "\n",
    "Once `mlai` is installed, it can be imported in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Classification\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/classification-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/classification-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Classification is perhaps the technique most closely assocated with\n",
    "machine learning. In the speech based agents, on-device classifiers are\n",
    "used to determine when the wake word is used. A wake word is a word that\n",
    "wakes up the device. For the Amazon Echo it is “Alexa,” for Siri it is\n",
    "“Hey Siri.” Once the wake word detected with a classifier, the speech\n",
    "can be uploaded to the cloud for full processing, the speech recognition\n",
    "stages.\n",
    "\n",
    "This isn’t just useful for intelligent agents, the UN global pulse\n",
    "project on public discussion on radio also uses [wake word detection for\n",
    "recording radio conversations](https://radio.unglobalpulse.net/uganda/).\n",
    "\n",
    "A major breakthrough in image classification came in 2012 with the\n",
    "ImageNet result of [Alex Krizhevsky, Ilya Sutskever and Geoff\n",
    "Hinton](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-)\n",
    "from the University of Toronto. ImageNet is a large data base of 14\n",
    "million images with many thousands of classes. The data is used in a\n",
    "community-wide challenge for object categorization. Krizhevsky et al\n",
    "used convolutional neural networks to outperform all previous approaches\n",
    "on the challenge. They formed a company which was purchased shortly\n",
    "after by Google. This challenge, known as object categorisation, was a\n",
    "major obstacle for practical computer vision systems. Modern object\n",
    "categorization systems are close to human performance.\n",
    "\n",
    "Machine learning problems normally involve a prediction function and an\n",
    "objective function. Regression is the case where the prediction function\n",
    "iss over the real numbers, so the codomain of the functions,\n",
    "$f(\\mathbf{X})$ was the real numbers or sometimes real vectors. The\n",
    "classification problem consists of predicting whether or not a\n",
    "particular example is a member of a particular class. So we may want to\n",
    "know if a particular image represents a digit 6 or if a particular user\n",
    "will click on a given advert. These are classification problems, and\n",
    "they require us to map to *yes* or *no* answers. That makes them\n",
    "naturally discrete mappings.\n",
    "\n",
    "In classification we are given an input vector, $\\mathbf{ x}$, and an\n",
    "associated label, $y$ which either takes the value $-1$ to represent\n",
    "*no* or $1$ to represent *yes*.\n",
    "\n",
    "In supervised learning the inputs, $\\mathbf{ x}$, are mapped to a label,\n",
    "$y$, through a function $f(\\cdot)$ that is dependent on a set of\n",
    "parameters, $\\mathbf{ w}$, $$\n",
    "y= f(\\mathbf{ x}; \\mathbf{ w}).\n",
    "$$ The function $f(\\cdot)$ is known as the *prediction function*. The\n",
    "key challenges are (1) choosing which features, $\\mathbf{ x}$, are\n",
    "relevant in the prediction, (2) defining the appropriate *class of\n",
    "function*, $f(\\cdot)$, to use and (3) selecting the right parameters,\n",
    "$\\mathbf{ w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Examples\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/classification-examples.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/classification-examples.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "-   Classifiying hand written digits from binary images (automatic zip\n",
    "    code reading)\n",
    "-   Detecting faces in images (e.g. digital cameras).\n",
    "-   Who a detected face belongs to (e.g. Facebook, DeepFace)\n",
    "-   Classifying type of cancer given gene expression data.\n",
    "-   Categorization of document types (different types of news article on\n",
    "    the internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Distribution\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bernoulli-distribution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bernoulli-distribution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Our focus has been on models where the objective function is inspired by\n",
    "a probabilistic analysis of the problem. In particular we’ve argued that\n",
    "we answer questions about the data set by placing probability\n",
    "distributions over the various quantities of interest. For the case of\n",
    "binary classification this will normally involve introducing probability\n",
    "distributions for discrete variables. Such probability distributions,\n",
    "are in some senses easier than those for continuous variables, in\n",
    "particular we can represent a probability distribution over $y$, where\n",
    "$y$ is binary, with one value. If we specify the probability that $y=1$\n",
    "with a number that is between 0 and 1, i.e. let’s say that\n",
    "$P(y=1) = \\pi$ (here we don’t mean $\\pi$ the number, we are setting\n",
    "$\\pi$ to be a variable) then we can specify the probability distribution\n",
    "through a table.\n",
    "\n",
    "|  $y$   |     0     |   1   |\n",
    "|:------:|:---------:|:-----:|\n",
    "| $P(y)$ | $(1-\\pi)$ | $\\pi$ |\n",
    "\n",
    "Mathematically we can use a trick to implement this same table. We can\n",
    "use the value $y$ as a mathematical switch and write that $$\n",
    "  P(y) = \\pi^y(1-\\pi)^{(1-y)}\n",
    "  $$ where our probability distribution is now written as a function of\n",
    "$y$. This probability distribution is known as the [Bernoulli\n",
    "distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution). The\n",
    "Bernoulli distribution is a clever trick for mathematically switching\n",
    "between two probabilities if we were to write it as code it would be\n",
    "better described as\n",
    "\n",
    "``` python\n",
    "def bernoulli(y_i, pi):\n",
    "    if y_i == 1:\n",
    "        return pi\n",
    "    else:\n",
    "        return 1-pi\n",
    "```\n",
    "\n",
    "If we insert $y=1$ then the function is equal to $\\pi$, and if we insert\n",
    "$y=0$ then the function is equal to $1-\\pi$. So the function recreates\n",
    "the table for the distribution given above.\n",
    "\n",
    "The probability distribution is named for [Jacob\n",
    "Bernoulli](http://en.wikipedia.org/wiki/Jacob_Bernoulli), the swiss\n",
    "mathematician. In his book Ars Conjectandi he considered the\n",
    "distribution and the result of a number of ‘trials’ under the Bernoulli\n",
    "distribution to form the *binomial* distribution. Below is the page\n",
    "where he considers Pascal’s triangle in forming combinations of the\n",
    "Bernoulli distribution to realise the binomial distribution for the\n",
    "outcome of positive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='CF4UAAAAQAAJ', page='PA87')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.one_figsize)\n",
    "plot.bernoulli_urn(ax, diagrams='./ml/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/mlai/./slides/diagrams//ml/bernoulli-urn.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Jacob Bernoulli described the Bernoulli distribution through\n",
    "an urn in which there are black and red balls.</i>\n",
    "\n",
    "Thomas Bayes also described the Bernoulli distribution, only he didn’t\n",
    "refer to Jacob Bernoulli’s work, so he didn’t call it by that name. He\n",
    "described the distribution in terms of a table (think of a *billiard\n",
    "table*) and two balls. Bayes suggests that each ball can be rolled\n",
    "across the table such that it comes to rest at a position that is\n",
    "*uniformly distributed* between the sides of the table.\n",
    "\n",
    "Let’s assume that the first ball is rolled, and that it comes to reset\n",
    "at a position that is $\\pi$ times the width of the table from the left\n",
    "hand side.\n",
    "\n",
    "Now, we roll the second ball. We are interested if the second ball ends\n",
    "up on the left side (+ve result) or the right side (-ve result) of the\n",
    "first ball. We use the Bernoulli distribution to determine this.\n",
    "\n",
    "For this reason in Bayes’s distribution there is considered to be\n",
    "*aleatoric* uncertainty about the distribution parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.one_figsize)\n",
    "plot.bayes_billiard(ax, diagrams='./ml/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard009.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Thomas Bayes described the Bernoulli distribution\n",
    "independently of Jacob Bernoulli. He used the analogy of a billiard\n",
    "table. Any ball on the table is given a uniformly random position\n",
    "between the left and right side of the table. The first ball (in the\n",
    "figure) gives the parameter of the Bernoulli distribution. The second\n",
    "ball (in the figure) gives the outcome as either left or right (relative\n",
    "to the first ball). This is the origin of the term Bayesian because the\n",
    "parameter of the distribution is drawn from a probsbility.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('bayes-billiard{counter:0>3}.svg', \n",
    "                            directory='./ml', \n",
    "                            counter=IntSlider(0,0,9,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood in the Bernoulli\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bernoulli-maximum-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bernoulli-maximum-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Maximum likelihood in the Bernoulli distribution is straightforward.\n",
    "Let’s assume we have data, $\\mathbf{ y}$ which consists of a vector of\n",
    "binary values of length $n$. If we assume each value was sampled\n",
    "independently from the Bernoulli distribution, conditioned on the\n",
    "parameter $\\pi$ then our joint probability density has the form $$\n",
    "p(\\mathbf{ y}|\\pi) = \\prod_{i=1}^{n} \\pi^{y_i} (1-\\pi)^{1-y_i}.\n",
    "$$ As normal in maximum likelihood we consider the negative log\n",
    "likelihood as our objective, $$\\begin{align*}\n",
    "  E(\\pi)& = -\\log p(\\mathbf{ y}|\\pi)\\\\ \n",
    "                     & = -\\sum_{i=1}^{n} y_i \\log \\pi - \\sum_{i=1}^{n} (1-y_i) \\log(1-\\pi),\n",
    "  \\end{align*}$$\n",
    "\n",
    "and we can derive the gradient with respect to the parameter $\\pi$.\n",
    "$$\\frac{\\text{d}E(\\pi)}{\\text{d}\\pi} = -\\frac{\\sum_{i=1}^{n} y_i}{\\pi}  + \\frac{\\sum_{i=1}^{n} (1-y_i)}{1-\\pi},$$\n",
    "\n",
    "and as normal we look for a stationary point for the log likelihood by\n",
    "setting this derivative to zero,\n",
    "$$0 = -\\frac{\\sum_{i=1}^{n} y_i}{\\pi}  + \\frac{\\sum_{i=1}^{n} (1-y_i)}{1-\\pi},$$\n",
    "rearranging we form\n",
    "$$(1-\\pi)\\sum_{i=1}^{n} y_i =   \\pi\\sum_{i=1}^{n} (1-y_i),$$ which\n",
    "implies\n",
    "$$\\sum_{i=1}^{n} y_i =   \\pi\\left(\\sum_{i=1}^{n} (1-y_i) + \\sum_{i=1}^{n} y_i\\right),$$\n",
    "\n",
    "and now we recognise that\n",
    "$\\sum_{i=1}^{n} (1-y_i) + \\sum_{i=1}^{n} y_i = n$ so we have\n",
    "$$\\pi = \\frac{\\sum_{i=1}^{n} y_i}{n}$$\n",
    "\n",
    "so in other words we estimate the probability associated with the\n",
    "Bernoulli by setting it to the number of observed positives, divided by\n",
    "the total length of $y$. This makes intiutive sense. If I asked you to\n",
    "estimate the probability of a coin being heads, and you tossed the coin\n",
    "100 times, and recovered 47 heads, then the estimate of the probability\n",
    "of heads should be $\\frac{47}{100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Show that the maximum likelihood solution we have found is a *minimum*\n",
    "for our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{posterior} =\n",
    "\\frac{\\text{likelihood}\\times\\text{prior}}{\\text{marginal likelihood}}\n",
    "$$\n",
    "\n",
    "Four components:\n",
    "\n",
    "1.  Prior distribution\n",
    "2.  Likelihood\n",
    "3.  Posterior distribution\n",
    "4.  Marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/naive-bayes.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/naive-bayes.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "*Note*: Everything we do below is possible using standard packages like\n",
    "`scikit-learn`, our purpose in this session is to help you understand\n",
    "how those engines are constructed. In practice for an application you\n",
    "should use a library like `scikit-learn`.\n",
    "\n",
    "In probabilistic machine learning we place probability distributions (or\n",
    "densities) over all the variables of interest, our first classification\n",
    "algorithm will do just that. We will consider how to form a\n",
    "classification by making assumptions about the *joint* density of our\n",
    "observations. We need to make assumptions to reduce the number of\n",
    "parameters we need to optimise.\n",
    "\n",
    "In the ideal world, given label data $\\mathbf{ y}$ and the inputs\n",
    "$\\mathbf{X}$ we should be able to specify the joint density of all\n",
    "potential values of $\\mathbf{ y}$ and $\\mathbf{X}$,\n",
    "$p(\\mathbf{ y}, \\mathbf{X})$. If $\\mathbf{X}$ and $\\mathbf{ y}$ are our\n",
    "training data, and we can somehow extend our density to incorporate\n",
    "future test data (by augmenting $\\mathbf{ y}$ with a new observation\n",
    "$y^*$ and $\\mathbf{X}$ with the corresponding inputs, $\\mathbf{ x}^*$),\n",
    "then we can answer any given question about a future test point $y^*$\n",
    "given its covariates $\\mathbf{ x}^*$ by conditioning on the training\n",
    "variables to recover, $$\n",
    "p(y^*|\\mathbf{X}, \\mathbf{ y}, \\mathbf{ x}^*),\n",
    "$$\n",
    "\n",
    "We can compute this distribution using the product and sum rules.\n",
    "However, to specify this density we must give the probability associated\n",
    "with all possible combinations of $\\mathbf{ y}$ and $\\mathbf{X}$. There\n",
    "are $2^{n}$ possible combinations for the vector $\\mathbf{ y}$ and the\n",
    "probability for each of these combinations must be jointly specified\n",
    "along with the joint density of the matrix $\\mathbf{X}$, as well as\n",
    "being able to *extend* the density for any chosen test location\n",
    "$\\mathbf{ x}^*$.\n",
    "\n",
    "In naive Bayes we make certain simplifying assumptions that allow us to\n",
    "perform all of the above in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conditional Independence\n",
    "\n",
    "If we are given model parameters $\\boldsymbol{ \\theta}$ we assume that\n",
    "conditioned on all these parameters that all data points in the model\n",
    "are independent. In other words we have, $$\n",
    "  p(y^*, \\mathbf{ x}^*, \\mathbf{ y}, \\mathbf{X}|\\boldsymbol{ \\theta}) = p(y^*, \\mathbf{ x}^*|\\boldsymbol{ \\theta})\\prod_{i=1}^{n} p(y_i, \\mathbf{ x}_i | \\boldsymbol{ \\theta}).\n",
    "  $$ This is a conditional independence assumption because we are not\n",
    "assuming our data are purely independent. If we were to assume that,\n",
    "then there would be nothing to learn about our test data given our\n",
    "training data. We are assuming that they are independent *given* our\n",
    "parameters, $\\boldsymbol{ \\theta}$. We made similar assumptions for\n",
    "regression, where our parameter set included $\\mathbf{ w}$ and\n",
    "$\\sigma^2$. Given those parameters we assumed that the density over\n",
    "$\\mathbf{ y}, y^*$ was *independent*. Here we are going a little further\n",
    "with that assumption because we are assuming the *joint* density of\n",
    "$\\mathbf{ y}$ and $\\mathbf{X}$ is independent across the data given the\n",
    "parameters.\n",
    "\n",
    "Computing posterior distribution in this case becomes easier, this is\n",
    "known as the ‘Bayes classifier.’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Conditional Independence\n",
    "\n",
    "$$\n",
    "p(\\mathbf{ x}_i | y_i, \\boldsymbol{ \\theta}) = \\prod_{j=1}^{p} p(x_{i,j}|y_i, \\boldsymbol{ \\theta})\n",
    "$$ where $p$ is the dimensionality of our inputs.\n",
    "\n",
    "The assumption that is particular to naive Bayes is to now consider that\n",
    "the *features* are also conditionally independent, but not only given\n",
    "the parameters. We assume that the features are independent given the\n",
    "parameters *and* the label. So for each data point we have\n",
    "$$p(\\mathbf{ x}_i | y_i, \\boldsymbol{ \\theta}) = \\prod_{j=1}^{p} p(x_{i,j}|y_i,\\boldsymbol{ \\theta})$$\n",
    "where $p$ is the dimensionality of our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Density for $y_i$\n",
    "\n",
    "$$\n",
    "p(x_{i,j},y_i| \\boldsymbol{ \\theta}) = p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i).\n",
    "$$\n",
    "\n",
    "We now have nearly all of the components we need to specify the full\n",
    "joint density. However, the feature conditional independence doesn’t yet\n",
    "give us the joint density over $p(y_i, \\mathbf{ x}_i)$ which is required\n",
    "to subsitute in to our data conditional independence to give us the full\n",
    "density. To recover the joint density given the conditional distribution\n",
    "of each feature, $p(x_{i,j}|y_i, \\boldsymbol{ \\theta})$, we need to make\n",
    "use of the product rule and combine it with a marginal density for\n",
    "$y_i$,\n",
    "\n",
    "$$p(x_{i,j},y_i| \\boldsymbol{ \\theta}) = p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i).$$\n",
    "Because $y_i$ is binary the *Bernoulli* density makes a suitable choice\n",
    "for our prior over $y_i$, $$p(y_i|\\pi) = \\pi^{y_i} (1-\\pi)^{1-y_i}$$\n",
    "where $\\pi$ now has the interpretation as being the *prior* probability\n",
    "that the classification should be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Density for Naive Bayes\n",
    "\n",
    "This allows us to write down the full joint density of the training\n",
    "data, $$\n",
    "  p(\\mathbf{ y}, \\mathbf{X}|\\boldsymbol{ \\theta}, \\pi) = \\prod_{i=1}^{n} \\prod_{j=1}^{p} p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i|\\pi)\n",
    "  $$\n",
    "\n",
    "which can now be fit by maximum likelihood. As normal we form our\n",
    "objective as the negative log likelihood,\n",
    "\n",
    "$$\\begin{align*}\n",
    "E(\\boldsymbol{ \\theta}, \\pi)& =  -\\log p(\\mathbf{ y}, \\mathbf{X}|\\boldsymbol{ \\theta}, \\pi) \\\\ &= -\\sum_{i=1}^{n} \\sum_{j=1}^{p} \\log p(x_{i, j}|y_i, \\boldsymbol{ \\theta}) -  \\sum_{i=1}^{n} \\log p(y_i|\\pi),\n",
    "\\end{align*}$$ which we note *decomposes* into two objective functions,\n",
    "one which is dependent on $\\pi$ alone and one which is dependent on\n",
    "$\\boldsymbol{ \\theta}$ alone so we have, $$\n",
    "E(\\pi, \\boldsymbol{ \\theta}) = E(\\boldsymbol{ \\theta}) + E(\\pi).\n",
    "$$ Since the two objective functions are separately dependent on the\n",
    "parameters $\\pi$ and $\\boldsymbol{ \\theta}$ we can minimize them\n",
    "independently. Firstly, minimizing the Bernoulli likelihood over the\n",
    "labels we have, $$\n",
    "E(\\pi) = -\\sum_{i=1}^{n}\\log p(y_i|\\pi) = -\\sum_{i=1}^{n} y_i \\log \\pi - \\sum_{i=1}^{n} (1-y_i) \\log (1-\\pi)\n",
    "$$ which we already minimized above recovering $$\n",
    "\\pi = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
    "$$\n",
    "\n",
    "We now need to minimize the objective associated with the conditional\n",
    "distributions for the features, $$\n",
    "E(\\boldsymbol{ \\theta}) = -\\sum_{i=1}^{n} \\sum_{j=1}^{p} \\log p(x_{i, j} |y_i, \\boldsymbol{ \\theta}),\n",
    "$$ which necessarily implies making some assumptions about the form of\n",
    "the conditional distributions. The right assumption will depend on the\n",
    "nature of our input data. For example, if we have an input which is real\n",
    "valued, we could use a Gaussian density and we could allow the mean and\n",
    "variance of the Gaussian to be different according to whether the class\n",
    "was positive or negative and according to which feature we were\n",
    "measuring. That would give us the form, $$\n",
    "p(x_{i, j} | y_i,\\boldsymbol{ \\theta}) = \\frac{1}{\\sqrt{2\\pi \\sigma_{y_i,j}^2}} \\exp \\left(-\\frac{(x_{i,j} - \\mu_{y_i, j})^2}{\\sigma_{y_i,j}^2}\\right),\n",
    "$$ where $\\sigma_{1, j}^2$ is the variance of the density for the $j$th\n",
    "output and the class $y_i=1$ and $\\sigma_{0, j}^2$ is the variance if\n",
    "the class is 0. The means can vary similarly. Our parameters,\n",
    "$\\boldsymbol{ \\theta}$ would consist of all the means and all the\n",
    "variances for the different dimensions.\n",
    "\n",
    "As normal we form our objective as the negative log likelihood, $$\n",
    "E(\\boldsymbol{ \\theta}, \\pi) = -\\log p(\\mathbf{ y}, \\mathbf{X}|\\boldsymbol{ \\theta}, \\pi) = -\\sum_{i=1}^{n} \\sum_{j=1}^{p} \\log p(x_{i, j}|y_i, \\boldsymbol{ \\theta}) - \\sum_{i=1}^{n} \\log p(y_i|\\pi),\n",
    "$$ which we note *decomposes* into two objective functions, one which is\n",
    "dependent on $\\pi$ alone and one which is dependent on\n",
    "$\\boldsymbol{ \\theta}$ alone so we have, $$\n",
    "E(\\pi, \\boldsymbol{ \\theta}) = E(\\boldsymbol{ \\theta}) + E(\\pi).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nigeria NMIS Data\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/nigeria-nmis-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/nigeria-nmis-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "As an example data set we will use Nigerian Millennium Development Goals\n",
    "Information System Health Facility (The Office of the Senior Special\n",
    "Assistant to the President on the Millennium Development Goals\n",
    "(OSSAP-MDGs) and Columbia University, 2014). It can be found here\n",
    "<https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014>.\n",
    "\n",
    "Taking from the information on the site,\n",
    "\n",
    "> The Nigeria MDG (Millennium Development Goals) Information System –\n",
    "> NMIS health facility data is collected by the Office of the Senior\n",
    "> Special Assistant to the President on the Millennium Development Goals\n",
    "> (OSSAP-MDGs) in partner with the Sustainable Engineering Lab at\n",
    "> Columbia University. A rigorous, geo-referenced baseline facility\n",
    "> inventory across Nigeria is created spanning from 2009 to 2011 with an\n",
    "> additional survey effort to increase coverage in 2014, to build\n",
    "> Nigeria’s first nation-wide inventory of health facility. The database\n",
    "> includes 34,139 health facilities info in Nigeria.\n",
    ">\n",
    "> The goal of this database is to make the data collected available to\n",
    "> planners, government officials, and the public, to be used to make\n",
    "> strategic decisions for planning relevant interventions.\n",
    ">\n",
    "> For data inquiry, please contact Ms. Funlola Osinupebi, Performance\n",
    "> Monitoring & Communications, Advisory Power Team, Office of the Vice\n",
    "> President at funlola.osinupebi@aptovp.org\n",
    ">\n",
    "> To learn more, please visit\n",
    "> <http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/>\n",
    ">\n",
    "> Suggested citation: Nigeria NMIS facility database (2014), the Office\n",
    "> of the Senior Special Assistant to the President on the Millennium\n",
    "> Development Goals (OSSAP-MDGs) & Columbia University\n",
    "\n",
    "For ease of use we’ve packaged this data set in the `pods` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.nigeria_nmis()['Y']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can access the data directly with the following\n",
    "commands.\n",
    "\n",
    "``` python\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://energydata.info/dataset/f85d1796-e7f2-4630-be84-79420174e3bd/resource/6e640a13-cab4-457b-b9e6-0336051bac27/download/healthmopupandbaselinenmisfacility.csv', 'healthmopupandbaselinenmisfacility.csv')\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('healthmopupandbaselinenmisfacility.csv')\n",
    "```\n",
    "\n",
    "Once it is loaded in the data can be summarized using the `describe`\n",
    "method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python and the Jupyter notebook it is possible to see a list of all\n",
    "possible functions and attributes by typing the name of the object\n",
    "followed by `.<Tab>` for example in the above case if we type\n",
    "`data.<Tab>` it show the columns available (these are attributes in\n",
    "pandas dataframes) such as `num_nurses_fulltime`, and also functions,\n",
    "such as `.describe()`.\n",
    "\n",
    "For functions we can also see the documentation about the function by\n",
    "following the name with a question mark. This will open a box with\n",
    "documentation at the bottom which can be closed with the x button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.plot(data.longitude, data.latitude, 'ro', alpha=0.01)\n",
    "ax.set_xlabel('longitude')\n",
    "ax.set_ylabel('latitude')\n",
    "\n",
    "mlai.write_figure('nigerian-health-facilities.png', directory='./ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"https://mlatcl.github.io/mlai/./slides/diagrams//ml/nigerian-health-facilities.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Location of the over thirty-four thousand health facilities\n",
    "registered in the NMIS data across Nigeria. Each facility plotted\n",
    "according to its latitude and longitude.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nigeria NMIS Data Classification\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/nigeria-nmis-data-classification.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/nigeria-nmis-data-classification.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Our aim will be to predict whether a center has maternal health delivery\n",
    "services given the attributes in the data. We will predict of the number\n",
    "of nurses, the number of doctors, location etc.\n",
    "\n",
    "Now we will convert this data into a form which we can use as inputs\n",
    "`X`, and labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~pd.isnull(data['maternal_health_delivery_services'])]\n",
    "data = data.dropna() # Remove entries with missing values\n",
    "X = data[['emergency_transport',\n",
    "          'num_chews_fulltime', \n",
    "          'phcn_electricity',\n",
    "          'child_health_measles_immun_calc',\n",
    "          'num_nurses_fulltime',\n",
    "          'num_doctors_fulltime', \n",
    "          'improved_water_supply', \n",
    "          'improved_sanitation',\n",
    "          'antenatal_care_yn', \n",
    "          'family_planning_yn',\n",
    "          'malaria_treatment_artemisinin', \n",
    "          'latitude', \n",
    "          'longitude']].copy()\n",
    "y = data['maternal_health_delivery_services']==True  # set label to be whether there's a maternal health delivery service\n",
    "\n",
    "# Create series of health center types with the relevant index\n",
    "s = data['facility_type_display'].apply(pd.Series, 1).stack() \n",
    "s.index = s.index.droplevel(-1) # to line up with df's index\n",
    "\n",
    "# Extract from the series the unique list of types.\n",
    "types = s.unique()\n",
    "\n",
    "# For each type extract the indices where it is present and add a column to X\n",
    "type_names = []\n",
    "for htype in types:\n",
    "    index = s[s==htype].index.tolist()\n",
    "    type_col=htype.replace(' ', '_').replace('/','-').lower()\n",
    "    type_names.append(type_col)\n",
    "    X.loc[:, type_col] = 0.0 \n",
    "    X.loc[index, type_col] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has given us a new data frame `X` which contains the different\n",
    "facility types in different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes NMIS\n",
    "\n",
    "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
    "class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigeria-nmis-data-naive-bayes.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/nigeria-nmis-data-naive-bayes.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We can now specify the naive Bayes model. For the genres we want to\n",
    "model the data as Bernoulli distributed, and for the year and body count\n",
    "we want to model the data as Gaussian distributed. We set up two data\n",
    "frames to contain the parameters for the rows and the columns below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume data is binary or real.\n",
    "# this list encodes whether it is binary or real (1 for binary, 0 for real)\n",
    "binary_columns = ['emergency_transport',\n",
    "          'phcn_electricity',\n",
    "          'child_health_measles_immun_calc',\n",
    "          'improved_water_supply', \n",
    "          'improved_sanitation',\n",
    "          'antenatal_care_yn', \n",
    "          'family_planning_yn',\n",
    "          'malaria_treatment_artemisinin'] + type_names\n",
    "real_columns = ['num_chews_fulltime', \n",
    "                'num_nurses_fulltime', \n",
    "                'num_doctors_fulltime', \n",
    "                'latitude', \n",
    "                'longitude']\n",
    "Bernoulli = pd.DataFrame(data=np.zeros((2,len(binary_columns))), columns=binary_columns, index=['theta_0', 'theta_1'])\n",
    "Gaussian = pd.DataFrame(data=np.zeros((4,len(real_columns))), columns=real_columns, index=['mu_0', 'sigma2_0', 'mu_1', 'sigma2_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in a form ready for analysis, let’s construct our\n",
    "data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 20000\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train:]\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]==True\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now train the model. For each feature we can make the fit\n",
    "independently. The fit is given by either counting the number of\n",
    "positives (for binary data) which gives us the maximum likelihood\n",
    "solution for the Bernoulli. Or by computing the empirical mean and\n",
    "variance of the data for the Gaussian, which also gives us the maximum\n",
    "likelihood solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X_train:\n",
    "    if column in Gaussian:\n",
    "        Gaussian[column]['mu_0'] = X_train[column][~y_train].mean()\n",
    "        Gaussian[column]['mu_1'] = X_train[column][y_train].mean()\n",
    "        Gaussian[column]['sigma2_0'] = X_train[column][~y_train].var(ddof=0)\n",
    "        Gaussian[column]['sigma2_1'] = X_train[column][y_train].var(ddof=0)\n",
    "    if column in Bernoulli:\n",
    "        Bernoulli[column]['theta_0'] = X_train[column][~y_train].sum()/(~y_train).sum()\n",
    "        Bernoulli[column]['theta_1'] = X_train[column][y_train].sum()/(y_train).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the nature of the distributions we’ve fitted to the model\n",
    "by looking at the entries in these data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions show the parameters of the *independent* class\n",
    "conditional probabilities for no maternity services. It is a Bernoulli\n",
    "distribution with the parameter, $\\pi$, given by (`theta_0`) for the\n",
    "facilities without maternity services and `theta_1` for the facilities\n",
    "with maternity services. The parameters whow that, facilities with\n",
    "maternity services also are more likely to have other services such as\n",
    "grid electricity, emergency transport, immunization programs etc.\n",
    "\n",
    "The naive Bayes assumption says that the joint probability for these\n",
    "services is given by the product of each of these Bernoulli\n",
    "distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have modelled the numbers in our table with a Gaussian density. Since\n",
    "several of these numbers are counts, a more appropriate distribution\n",
    "might be the Poisson distribution. But here we can see that the average\n",
    "number of nurses, healthworkers and doctors is *higher* in the\n",
    "facilities with maternal services (`mu_1`) than those without maternal\n",
    "services (`mu_0`). There is also a small difference between the mean\n",
    "latitude and longitudes. However, the *standard deviation* which would\n",
    "be given by the square root of the variance parameters (`sigma_0` and\n",
    "`sigma_1`) is large, implying that a difference in latitude and\n",
    "longitude may be due to sampling error. To be sure more analysis would\n",
    "be required.\n",
    "\n",
    "The final model parameter is the prior probability of the positive\n",
    "class, $\\pi$, which is computed by maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = float(y_train.sum())/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability tells us that slightly more facilities have\n",
    "maternity services than those that don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Naive Bayes has given us the class conditional densities:\n",
    "$p(\\mathbf{ x}_i | y_i, \\boldsymbol{ \\theta})$. To make predictions with\n",
    "these densities we need to form the distribution given by $$\n",
    "P(y^*| \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*, \\boldsymbol{ \\theta})\n",
    "$$ This can be computed by using the product rule. We know that $$\n",
    "P(y^*| \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*, \\boldsymbol{ \\theta})p(\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*|\\boldsymbol{ \\theta}) = p(y*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta})\n",
    "$$ implying that $$\n",
    "P(y^*| \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*, \\boldsymbol{ \\theta}) = \\frac{p(y*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta})}{p(\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*|\\boldsymbol{ \\theta})}\n",
    "$$ and we’ve already defined\n",
    "$p(y^*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta})$\n",
    "using our conditional independence assumptions above $$\n",
    "p(y^*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta}) = \\prod_{j=1}^{p} p(x^*_{j}|y^*, \\boldsymbol{ \\theta})p(y^*|\\pi)\\prod_{i=1}^{n} \\prod_{j=1}^{p} p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i|\\pi)\n",
    "$$ The other required density is $$\n",
    "p(\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*|\\boldsymbol{ \\theta})\n",
    "$$ which can be found from\n",
    "$$p(y^*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta})$$\n",
    "using the *sum rule* of probability, $$\n",
    "p(\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*|\\boldsymbol{ \\theta}) = \\sum_{y^*=0}^1 p(y^*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta}).\n",
    "$$ Because of our independence assumptions that is simply equal to $$\n",
    "p(\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*| \\boldsymbol{ \\theta}) = \\sum_{y^*=0}^1 \\prod_{j=1}^{p} p(x^*_{j}|y^*_i, \\boldsymbol{ \\theta})p(y^*|\\pi)\\prod_{i=1}^{n} \\prod_{j=1}^{p} p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i|\\pi).\n",
    "$$ Substituting both forms in to recover our distribution over the test\n",
    "label conditioned on the training data we have, $$\n",
    "P(y^*| \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*, \\boldsymbol{ \\theta}) = \\frac{\\prod_{j=1}^{p} p(x^*_{j}|y^*_i, \\boldsymbol{ \\theta})p(y^*|\\pi)\\prod_{i=1}^{n} \\prod_{j=1}^{p} p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i|\\pi)}{\\sum_{y^*=0}^1 \\prod_{j=1}^{p} p(x^*_{j}|y^*_i, \\boldsymbol{ \\theta})p(y^*|\\pi)\\prod_{i=1}^{n} \\prod_{j=1}^{p} p(x_{i,j}|y_i, \\boldsymbol{ \\theta})p(y_i|\\pi)}\n",
    "$$ and we notice that all the terms associated with the training data\n",
    "actually cancel, the test prediction is *conditionally independent* of\n",
    "the training data *given* the parameters. This is a result of our\n",
    "conditional independence assumptions over the data points. $$\n",
    "p(y^*| \\mathbf{ x}^*, \\boldsymbol{ \\theta}) = \\frac{\\prod_{j=1}^{p} p(x^*_{j}|y^*_i,\n",
    "\\boldsymbol{ \\theta})p(y^*|\\pi)}{\\sum_{y^*=0}^1 \\prod_{j=1}^{p} p(x^*_{j}|y^*_i, \\boldsymbol{ \\theta})p(y^*|\\pi)}\n",
    "$$ This formula is also fairly straightforward to implement. First we\n",
    "implement the log probabilities for the Gaussian density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian(x, mu, sigma2):\n",
    "    return -0.5* np.log(2*np.pi*sigma2)-((x-mu)**2)/(2*sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for any test point we compute the joint distribution of the Gaussian\n",
    "features by *summing* their log probabilities. Working in log space can\n",
    "be a considerable advantage over computing the probabilities directly:\n",
    "as the number of features we include goes up, because all the\n",
    "probabilities are less than 1, the joint probability will become smaller\n",
    "and smaller, and may be difficult to represent accurately (or even\n",
    "underflow). Working in log space can ameliorate this problem. We can\n",
    "also compute the log probability for the Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bernoulli(x, theta):\n",
    "    return x*np.log(theta) + (1-x)*np.log(1-theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "\n",
    "Before we proceed, let’s just pause and think for a moment what will\n",
    "happen if `theta` here is either zero or one. This will result in\n",
    "$\\log 0 = -\\infty$ and cause numerical problems. This definitely can\n",
    "happen in practice. If some of the features are rare or very common\n",
    "across the data set then the maximum likelihood solution could find\n",
    "values of zero or one respectively. Such values are problematic because\n",
    "they cause posterior probabilities of class membership of either one or\n",
    "zero. In practice we deal with this using *Laplace smoothing* (which\n",
    "actually has an interpretation as a Bayesian fit of the Bernoulli\n",
    "distribution. Laplace used an example of the sun rising each day, and a\n",
    "wish to predict the sun rise the following day to describe his idea of\n",
    "smoothing, which can be found at the bottom of following page from\n",
    "Laplace’s ‘Essai Philosophique …’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='1YQPAAAAQAAJ', page='PA16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace suggests that when computing the probability of an event where a\n",
    "success or failure is rare (he uses an example of the sun rising across\n",
    "the last 5,000 years or 1,826,213 days) that even though only successes\n",
    "have been observed (in the sun rising case) that the odds for tomorrow\n",
    "shouldn’t be given as $$\n",
    "\\frac{1,826,213}{1,826,213} = 1\n",
    "$$ but rather by adding one to the numerator and two to the denominator,\n",
    "$$\n",
    "\\frac{1,826,213 + 1}{1,826,213 + 2} = 0.99999945.\n",
    "$$ This technique is sometimes called a ‘pseudocount technique’ because\n",
    "it has an intepretation of assuming some observations before you start,\n",
    "it’s as if instead of observing $\\sum_{i}y_i$ successes you have an\n",
    "additional success, $\\sum_{i}y_i + 1$ and instead of having observed $n$\n",
    "events you’ve observed $n+ 2$. So we can think of Laplace’s idea saying\n",
    "(before we start) that we have ‘two observations worth of belief, that\n",
    "the odds are 50/50,’ because before we start (i.e. when $n=0$) our\n",
    "estimate is 0.5, yet because the effective $n$ is only 2, this estimate\n",
    "is quickly overwhelmed by data. Laplace used ideas like this a lot, and\n",
    "it is known as his ‘principle of insufficient reason.’ His idea was that\n",
    "in the absence of knowledge (i.e. before we start) we should assume that\n",
    "all possible outcomes are equally likely. This idea has a modern\n",
    "counterpart, known as the [principle of maximum\n",
    "entropy](http://en.wikipedia.org/wiki/Principle_of_maximum_entropy). A\n",
    "lot of the theory of this approach was developed by [Ed\n",
    "Jaynes](http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes), who\n",
    "according to his erstwhile collaborator and friend, John Skilling,\n",
    "learnt French as an undergraduate by reading the works of Laplace.\n",
    "Although John also related that Jaynes’s spoken French was not up to the\n",
    "standard of his scientific French. For me Ed Jaynes’s work very much\n",
    "carries on the tradition of Laplace into the modern era, in particular\n",
    "his focus on Bayesian approaches. I’m very proud to have met those that\n",
    "knew and worked with him. It turns out that Laplace’s idea also has a\n",
    "Bayesian interpretation (as Laplace understood), it comes from assuming\n",
    "a particular prior density for the parameter $\\pi$, but we won’t explore\n",
    "that interpretation for the moment, and merely choose to estimate the\n",
    "probability as, $$\n",
    "\\pi = \\frac{\\sum_{i=1}^{n} y_i + 1}{n+ 2}\n",
    "$$ to prevent problems with certainty causing numerical issues and\n",
    "misclassifications. Let’s refit the Bernoulli features now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Bernoulli with Laplace smoothing.\n",
    "for column in X_train:\n",
    "    if column in Bernoulli:\n",
    "        Bernoulli[column]['theta_0'] = (X_train[column][~y_train].sum() + 1)/((~y_train).sum() + 2)\n",
    "        Bernoulli[column]['theta_1'] = (X_train[column][y_train].sum() + 1)/((y_train).sum() + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That places us in a position to write the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, Gaussian, Bernoulli, prior):\n",
    "    log_positive = pd.Series(data = np.zeros(X_test.shape[0]), index=X_test.index)\n",
    "    log_negative = pd.Series(data = np.zeros(X_test.shape[0]), index=X_test.index)\n",
    "    for column in X_test.columns:\n",
    "        if column in Gaussian:\n",
    "            log_positive += log_gaussian(X_test[column], Gaussian[column]['mu_1'], Gaussian[column]['sigma2_1'])\n",
    "            log_negative += log_gaussian(X_test[column], Gaussian[column]['mu_0'], Gaussian[column]['sigma2_0'])\n",
    "        elif column in Bernoulli:\n",
    "            log_positive += log_bernoulli(X_test[column], Bernoulli[column]['theta_1'])\n",
    "            log_negative += log_bernoulli(X_test[column], Bernoulli[column]['theta_0'])\n",
    "            \n",
    "    v = np.zeros_like(log_positive.values)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        v[i] = np.exp(log_positive.values[i] + np.log(prior))/(np.exp(log_positive.values[i] + np.log(prior)) \n",
    "                                                               + np.exp(log_negative.values[i] + np.log(1-prior)))\n",
    "    return v\n",
    "    #return np.exp(log_positive + np.log(prior))/(np.exp(log_positive + np.log(prior)) + np.exp(log_negative + np.log(1-prior)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are in a position to make the predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y = predict(X_test, Gaussian, Bernoulli, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the quality of the predictions in the following way.\n",
    "Firstly, we can threshold our probabilities at 0.5, allocating points\n",
    "with greater than 50% probability of membership of the positive class to\n",
    "the positive class. We can then compare to the true values, and see how\n",
    "many of these values we got correct. This is our total number correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = y_test.eq(p_y>0.5)\n",
    "total_correct = sum(correct)\n",
    "print(\"Total correct\", total_correct, \" out of \", len(y_test), \"which is\", float(total_correct)/len(y_test), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also now plot the [confusion\n",
    "matrix](http://en.wikipedia.org/wiki/Confusion_matrix). A confusion\n",
    "matrix tells us where we are making mistakes. Along the diagonal it\n",
    "stores the *true positives*, the points that were positive class that we\n",
    "classified correctly, and the *true negatives*, the points that were\n",
    "negative class and that we classified correctly. The off diagonal terms\n",
    "contain the false positives and the false negatives. Along the rows of\n",
    "the matrix we place the actual class, and along the columns we place our\n",
    "predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.DataFrame(data=np.zeros((2,2)), \n",
    "                                columns=['predicted no maternity', 'predicted maternity'],\n",
    "                                index =['actual no maternity','actual maternity'])\n",
    "confusion_matrix['predicted maternity']['actual maternity'] = (y_test & (p_y>0.5)).sum()\n",
    "confusion_matrix['predicted maternity']['actual no maternity'] = (~y_test & (p_y>0.5)).sum()\n",
    "confusion_matrix['predicted no maternity']['actual maternity'] = (y_test & ~(p_y>0.5)).sum()\n",
    "confusion_matrix['predicted no maternity']['actual no maternity'] = (~y_test & ~(p_y>0.5)).sum()\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "How can you improve your classification, are all the features equally\n",
    "valid? Are some features more helpful than others? What happens if you\n",
    "remove features that appear to be less helpful. How might you select\n",
    "such features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Answer\n",
    "\n",
    "Write your answer to Exercise 2 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "We have decided to classify positive if probability of maternity is\n",
    "greater than 0.5. This has led us to accidentally classify some\n",
    "facilities as havien’t facilities for maternity when in fact they don’t.\n",
    "Imagine you wish to ensure that a facility handles maternity. With your\n",
    "test set how low do you have to set the threshold to avoid all the false\n",
    "negatives (i.e. facilities where you predicted there was no maternity,\n",
    "but in actuality there were?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Answer\n",
    "\n",
    "Write your answer to Exercise 3 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Naive Bayes has given us the class conditional densities:\n",
    "$p(\\mathbf{ x}_i | y_i, \\boldsymbol{ \\theta})$. To make predictions with\n",
    "these densities we need to form the distribution given by $$\n",
    "P(y^*| \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}^*, \\boldsymbol{ \\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Write down the negative log likelihood of the Gaussian density over a\n",
    "vector of variables $\\mathbf{ x}$. Assume independence between each\n",
    "variable. Minimize this objective to obtain the maximum likelihood\n",
    "solution of the form. $$\n",
    "\\mu = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "$$ $$\n",
    "\\sigma^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Answer\n",
    "\n",
    "Write your answer to Exercise 4 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input data was *binary* then we could also make use of the\n",
    "Bernoulli distribution for the features. For that case we would have the\n",
    "form, $$\n",
    "p(x_{i, j} | y_i,\\boldsymbol{ \\theta}) = \\theta_{y_i, j}^{x_{i, j}}(1-\\theta_{y_i, j})^{(1-x_{i,j})},\n",
    "$$ where $\\theta_{1, j}$ is the probability that the $j$th feature is on\n",
    "if $y_i$ is 1.\n",
    "\n",
    "In either case, maximum likelihood fitting would proceed in the same\n",
    "way. The objective has the form, $$\n",
    "E(\\boldsymbol{ \\theta}) = -\\sum_{j=1}^{p} \\sum_{i=1}^{n} \\log p(x_{i,j} |y_i, \\boldsymbol{ \\theta}),\n",
    "$$ and if, as above, the parameters of the distributions are specific to\n",
    "each feature vector (we had means and variances for each continuous\n",
    "feature, and a probability for each binary feature) then we can use the\n",
    "fact that these parameters separate into disjoint subsets across the\n",
    "features to write, $$\n",
    "\\begin{align*}\n",
    "E(\\boldsymbol{ \\theta}) &= -\\sum_{j=1}^{p} \\sum_{i=1}^{n} \\log\n",
    "p(x_{i,j} |y_i, \\boldsymbol{ \\theta}_j)\\\\\n",
    "& \\sum_{j=1}^{p}\n",
    "E(\\boldsymbol{ \\theta}_j),\n",
    "\\end{align*}\n",
    "$$ which means we can minimize our objective on each feature\n",
    "independently.\n",
    "\n",
    "These characteristics mean that naive Bayes scales very well with big\n",
    "data. To fit the model we consider each feature in turn, we select the\n",
    "positive class and fit parameters for that class, then we select each\n",
    "negative class and fit features for that class. We have code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Summary\n",
    "\n",
    "Naive Bayes is making very simple assumptions about the data, in\n",
    "particular it is modeling the full *joint* probability of the data set,\n",
    "$p(\\mathbf{ y}, \\mathbf{X}| \\boldsymbol{ \\theta}, \\pi)$ by very strong\n",
    "assumptions about factorizations that are unlikely to be true in\n",
    "practice. The data conditional independence assumption is common, and\n",
    "relies on a rich parameter vector to absorb all the information in the\n",
    "training data. The additional assumption of naive Bayes is that features\n",
    "are conditional independent given the class label $y_i$ (and the\n",
    "parameter vector, $\\boldsymbol{ \\theta}$. This is quite a strong\n",
    "assumption. However, it causes the objective function to decompose into\n",
    "parts which can be independently fitted to the different feature\n",
    "vectors, meaning it is very easy to fit the model to large data. It is\n",
    "also clear how we should handle *streaming* data and *missing* data.\n",
    "This means that the model can be run ‘live,’ adapting parameters and\n",
    "information as it arrives. Indeed, the model is even capable of dealing\n",
    "with new *features* that might arrive at run time. Such is the strength\n",
    "of the modeling the joint probability density. However, the\n",
    "factorization assumption that allows us to do this efficiently is very\n",
    "strong and may lead to poor decision boundaries in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "-   Chapter 5 up to pg 179 (Section 5.1, and 5.2 up to 5.2.2) of Rogers\n",
    "    and Girolami (2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
    "Press.\n",
    "\n",
    "The Office of the Senior Special Assistant to the President on the\n",
    "Millennium Development Goals (OSSAP-MDGs), Columbia University, 2014.\n",
    "Nigeria NMIS facility database."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
