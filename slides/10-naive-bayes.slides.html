<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Probabilistic Classification: Naive Bayes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Probabilistic Classification: Naive Bayes</h1>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="review" class="slide level2">
<h2>Review</h2>
<ul>
<li>Last time: Looked at unsupervised learning.</li>
<li>Introduced latent variables, dimensionality reduction and
clustering.</li>
<li>This time: Classification with Naive Bayes</li>
</ul>
</section>
<section id="introduction-to-classification" class="slide level2">
<h2>Introduction to Classification</h2>
</section>
<section id="classification" class="slide level2">
<h2>Classification</h2>
<ul>
<li><p><em>Wake word</em> classification (<a
href="https://radio.unglobalpulse.net/uganda/">Global Pulse
Project</a>).</p></li>
<li><p>Breakthrough in 2012 with ImageNet result of <a
href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-">Alex
Krizhevsky, Ilya Sutskever and Geoff Hinton</a></p></li>
<li><p>We are given a data set containing ‘inputs’, <span
class="math inline">\(\mathbf{X}\)</span> and ‘targets’, <span
class="math inline">\(\mathbf{ y}\)</span>.</p></li>
<li><p>Each data point consists of an input vector <span
class="math inline">\(\mathbf{ x}_i\)</span> and a class label, <span
class="math inline">\(y_i\)</span>.</p></li>
<li><p>For binary classification assume <span
class="math inline">\(y_i\)</span> should be either <span
class="math inline">\(1\)</span> (yes) or <span
class="math inline">\(-1\)</span> (no).</p></li>
<li><p>Input vector can be thought of as features.</p></li>
</ul>
</section>
<section id="discrete-probability" class="slide level2">
<h2>Discrete Probability</h2>
<ul>
<li>Algorithms based on <em>prediction</em> function and
<em>objective</em> function.</li>
<li>For regression the <em>codomain</em> of the functions, <span
class="math inline">\(f(\mathbf{X})\)</span> was the real numbers or
sometimes real vectors.</li>
<li>In classification we are given an input vector, <span
class="math inline">\(\mathbf{ x}\)</span>, and an associated label,
<span class="math inline">\(y\)</span> which either takes the value
<span class="math inline">\(-1\)</span> or <span
class="math inline">\(1\)</span>.</li>
</ul>
</section>
<section id="classification-1" class="slide level2">
<h2>Classification</h2>
<ul>
<li>Inputs, <span class="math inline">\(\mathbf{ x}\)</span>, mapped to
a label, <span class="math inline">\(y\)</span>, through a function
<span class="math inline">\(f(\cdot)\)</span> dependent on parameters,
<span class="math inline">\(\mathbf{ w}\)</span>, <span
class="math display">\[
y= f(\mathbf{ x}; \mathbf{ w}).
\]</span></li>
<li><span class="math inline">\(f(\cdot)\)</span> is known as the
<em>prediction function</em>.</li>
</ul>
</section>
<section id="classification-examples" class="slide level2">
<h2>Classification Examples</h2>
<ul>
<li>Classifiying hand written digits from binary images (automatic zip
code reading)</li>
<li>Detecting faces in images (e.g. digital cameras).</li>
<li>Who a detected face belongs to (e.g. Facebook, DeepFace)</li>
<li>Classifying type of cancer given gene expression data.</li>
<li>Categorization of document types (different types of news article on
the internet)</li>
</ul>
</section>
<section id="reminder-on-the-term-bayesian" class="slide level2">
<h2>Reminder on the Term “Bayesian”</h2>
<ul>
<li>We use Bayes’ rule to invert probabilities in the Bayesian approach.
<ul>
<li>Bayesian is not named after Bayes’ rule (v. common confusion).</li>
<li>The term Bayesian refers to the treatment of the parameters as
stochastic variables.</li>
<li>Proposed by <span class="citation"
data-cites="Laplace:memoire74">Laplace (1774)</span> and <span
class="citation" data-cites="Bayes:doctrine63">Bayes (1763)</span>
independently.</li>
<li>For early statisticians this was very controversial (Fisher et
al).</li>
</ul></li>
</ul>
</section>
<section id="reminder-on-the-term-bayesian-1" class="slide level2">
<h2>Reminder on the Term “Bayesian”</h2>
<ul>
<li>The use of Bayes’ rule does <em>not</em> imply you are being
Bayesian.
<ul>
<li>It is just an application of the product rule of probability.</li>
</ul></li>
</ul>
</section>
<section id="bernoulli-distribution" class="slide level2">
<h2>Bernoulli Distribution</h2>
<ul>
<li>Binary classification: need a probability distribution for discrete
variables.</li>
<li>Discrete probability is in some ways easier: <span
class="math inline">\(P(y=1) = \pi\)</span> &amp; specify distribution
as a table.</li>
<li>Instead of <span class="math inline">\(y=-1\)</span> for negative
class we take <span class="math inline">\(y=0\)</span>.</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span
class="math inline">\(y\)</span></th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">\(P(y)\)</span></td>
<td style="text-align: center;"><span
class="math inline">\((1-\pi)\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\pi\)</span></td>
</tr>
</tbody>
</table>
<p>This is the <a
href="http://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distribution</a>.</p>
</section>
<section id="mathematical-switch" class="slide level2">
<h2>Mathematical Switch</h2>
<ul>
<li><p>The Bernoulli distribution <span class="math display">\[
P(y) = \pi^y(1-\pi)^{(1-y)}
\]</span></p></li>
<li><p>Is a clever trick for switching probabilities, as code it would
be</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli(y_i, pi):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y_i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pi</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>pi</span></code></pre></div>
</section>
<section id="jacob-bernoullis-bernoulli" class="slide level2">
<h2>Jacob Bernoulli’s Bernoulli</h2>
<ul>
<li>Bernoulli described the Bernoulli distribution in terms of an ‘urn’
filled with balls.</li>
<li>There are red and black balls. There is a fixed number of balls in
the urn.</li>
<li>The portion of red balls is given by <span
class="math inline">\(\pi\)</span>.</li>
<li>For this reason in Bernoulli’s distribution there is
<em>epistemic</em> uncertainty about the distribution parameter.</li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=CF4UAAAAQAAJ&amp;pg=PA87"><img
data-src="https://mlatcl.github.io/mlai/./slides/diagrams//books/CF4UAAAAQAAJ-PA87.png" /></a>
</div>
</section>
<section id="jacob-bernoullis-bernoulli-1" class="slide level2">
<h2>Jacob Bernoulli’s Bernoulli</h2>
<div class="figure">
<div id="bernoulli-urn-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bernoulli-urn.svg" width="40%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Jacob Bernoulli described the Bernoulli distribution through an urn in
which there are black and red balls.
</aside>
</section>
<section id="thomas-bayess-bernoulli" class="slide level2">
<h2>Thomas Bayes’s Bernoulli</h2>
<ul>
<li>Bayes described the Bernoulli distribution (he didn’t call it that!)
in terms of a table and two balls.</li>
<li>Each ball is rolled so it comes to rest at a uniform distribution
across the table.</li>
<li>The first ball comes to rest at a position that is a <span
class="math inline">\(\pi\)</span> times the width of table.</li>
<li>After placing the first ball you consider whether a second would
land to the left or the right.</li>
<li>For this reason in Bayes’s distribution there is considered to be
<em>aleatoric</em> uncertainty about the distribution parameter.</li>
</ul>
</section>
<section id="thomas-bayes-bernoulli" class="slide level2">
<h2>Thomas Bayes’ Bernoulli</h2>
<script>
showDivs(1, 'bayes_billiard');
</script>
<p><small></small>
<input id="range-bayes_billiard" type="range" min="1" max="10" value="1" onchange="setDivs('bayes_billiard')" oninput="setDivs('bayes_billiard')">
<button onclick="plusDivs(-1, 'bayes_billiard')">❮</button>
<button onclick="plusDivs(1, 'bayes_billiard')">❯</button></p>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard000.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard001.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard002.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard003.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard004.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard005.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard006.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard007.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard008.svg" width="40%" style=" ">
</object>
</div>
<div class="bayes_billiard" style="text-align:center;">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/bayes-billiard009.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="maximum-likelihood-in-the-bernoulli" class="slide level2">
<h2>Maximum Likelihood in the Bernoulli</h2>
<ul>
<li>Assume data, <span class="math inline">\(\mathbf{ y}\)</span> is
binary vector length <span class="math inline">\(n\)</span>.</li>
<li>Assume each value was sampled independently from the Bernoulli
distribution, given probability <span class="math inline">\(\pi\)</span>
<span class="math display">\[
p(\mathbf{ y}|\pi) = \prod_{i=1}^{n} \pi^{y_i} (1-\pi)^{1-y_i}.
\]</span></li>
</ul>
</section>
<section id="negative-log-likelihood" class="slide level2">
<h2>Negative Log Likelihood</h2>
<ul>
<li>Minimize the negative log likelihood <span
class="math display">\[\begin{align*}
E(\pi)&amp; = -\log p(\mathbf{ y}|\pi)\\
                   &amp; = -\sum_{i=1}^{n} y_i \log \pi - \sum_{i=1}^{n}
(1-y_i) \log(1-\pi),
\end{align*}\]</span></li>
<li>Take gradient with respect to the parameter <span
class="math inline">\(\pi\)</span>. <span
class="math display">\[\frac{\text{d}E(\pi)}{\text{d}\pi} =
-\frac{\sum_{i=1}^{n} y_i}{\pi}  + \frac{\sum_{i=1}^{n}
(1-y_i)}{1-\pi},\]</span></li>
</ul>
</section>
<section id="fixed-point" class="slide level2">
<h2>Fixed Point</h2>
<ul>
<li><p>Stationary point: set derivative to zero <span
class="math display">\[0 = -\frac{\sum_{i=1}^{n} y_i}{\pi}  +
\frac{\sum_{i=1}^{n} (1-y_i)}{1-\pi},\]</span></p></li>
<li><p>Rearrange to form <span
class="math display">\[(1-\pi)\sum_{i=1}^{n} y_i =   \pi\sum_{i=1}^{n}
(1-y_i),\]</span></p></li>
<li><p>Giving <span class="math display">\[\sum_{i=1}^{n} y_i
=   \pi\left(\sum_{i=1}^{n} (1-y_i) + \sum_{i=1}^{n}
y_i\right),\]</span></p></li>
</ul>
</section>
<section id="solution" class="slide level2">
<h2>Solution</h2>
<ul>
<li><p>Recognise that <span class="math inline">\(\sum_{i=1}^{n} (1-y_i)
+ \sum_{i=1}^{n} y_i = n\)</span> so we have <span
class="math display">\[\pi = \frac{\sum_{i=1}^{n}
y_i}{n}\]</span></p></li>
<li><p>Estimate the probability associated with the Bernoulli by setting
it to the number of observed positives, divided by the total length of
<span class="math inline">\(y\)</span>.</p></li>
<li><p>Makes intiutive sense.</p></li>
<li><p>What’s your best guess of probability for coin toss is heads when
you get 47 heads from 100 tosses?</p></li>
</ul>
</section>
<section id="bayes-rule-reminder" class="slide level2">
<h2>Bayes’ Rule Reminder</h2>
<p><span class="math display">\[
\text{posterior} =
\frac{\text{likelihood}\times\text{prior}}{\text{marginal likelihood}}
\]</span></p>
<p>Four components:</p>
<ol type="1">
<li>Prior distribution</li>
<li>Likelihood</li>
<li>Posterior distribution</li>
<li>Marginal likelihood</li>
</ol>
</section>
<section id="naive-bayes-classifiers" class="slide level2">
<h2>Naive Bayes Classifiers</h2>
<ul>
<li><p>Probabilistic Machine Learning: place probability distributions
(or densities) over all the variables of interest.</p></li>
<li><p>In <em>naive Bayes</em> this is exactly what we do.</p></li>
<li><p>Form a classification algorithm by modelling the <em>joint</em>
density of our observations.</p></li>
<li><p>Need to make assumption about joint density.</p></li>
</ul>
</section>
<section id="assumptions-about-density" class="slide level2">
<h2>Assumptions about Density</h2>
<ul>
<li>Make assumptions to reduce the number of parameters we need to
optimise.</li>
<li>Given label data <span class="math inline">\(\mathbf{ y}\)</span>
and the inputs <span class="math inline">\(\mathbf{X}\)</span> could
specify joint density of all potential values of <span
class="math inline">\(\mathbf{ y}\)</span> and <span
class="math inline">\(\mathbf{X}\)</span>, <span
class="math inline">\(p(\mathbf{ y}, \mathbf{X})\)</span>.</li>
<li>If <span class="math inline">\(\mathbf{X}\)</span> and <span
class="math inline">\(\mathbf{ y}\)</span> are training data.</li>
<li>If <span class="math inline">\(\mathbf{ x}^*\)</span> is a test
input and <span class="math inline">\(y^*\)</span> a test location we
want <span class="math display">\[
p(y^*|\mathbf{X}, \mathbf{ y}, \mathbf{ x}^*),
\]</span></li>
</ul>
</section>
<section id="answer-from-rules-of-probability" class="slide level2">
<h2>Answer from Rules of Probability</h2>
<ul>
<li>Compute this distribution using the product and sum rules.</li>
<li>Need the probability associated with all possible combinations of
<span class="math inline">\(\mathbf{ y}\)</span> and <span
class="math inline">\(\mathbf{X}\)</span>.</li>
<li>There are <span class="math inline">\(2^{n}\)</span> possible
combinations for the vector <span class="math inline">\(\mathbf{
y}\)</span></li>
<li>Probability for each of these combinations must be jointly specified
along with the joint density of the matrix <span
class="math inline">\(\mathbf{X}\)</span>,</li>
<li>Also need to <em>extend</em> the density for any chosen test
location <span class="math inline">\(\mathbf{ x}^*\)</span>.</li>
</ul>
</section>
<section id="naive-bayes-assumptions" class="slide level2">
<h2>Naive Bayes Assumptions</h2>
<ul>
<li>In <em>naive Bayes</em> we make certain simplifying assumptions that
allow us to perform all of the above in practice.</li>
</ul>
<ol type="1">
<li>Data Conditional Independence</li>
<li>Feature conditional independence</li>
<li>Marginal density for <span class="math inline">\(y\)</span>.</li>
</ol>
</section>
<section id="data-conditional-independence" class="slide level2">
<h2>Data Conditional Independence</h2>
<ul>
<li><p>Given model parameters <span class="math inline">\(\boldsymbol{
\theta}\)</span> we assume that all data points in the model are
independent. <span class="math display">\[
p(y^*, \mathbf{ x}^*, \mathbf{ y}, \mathbf{X}|\boldsymbol{ \theta}) =
p(y^*, \mathbf{ x}^*|\boldsymbol{ \theta})\prod_{i=1}^{n} p(y_i,
\mathbf{ x}_i | \boldsymbol{ \theta}).
\]</span></p></li>
<li><p>This is a conditional independence assumption.</p></li>
<li><p>We also make similar assumptions for regression (where <span
class="math inline">\(\boldsymbol{ \theta}= \left\{\mathbf{
w},\sigma^2\right\}\)</span>).</p></li>
<li><p>Here we assume <em>joint</em> density of <span
class="math inline">\(\mathbf{ y}\)</span> and <span
class="math inline">\(\mathbf{X}\)</span> is independent across the data
given the parameters.</p></li>
</ul>
</section>
<section id="bayes-classifier" class="slide level2">
<h2>Bayes Classifier</h2>
<p>Computing posterior distribution in this case becomes easier, this is
known as the ‘Bayes classifier’.</p>
</section>
<section id="feature-conditional-independence" class="slide level2">
<h2>Feature Conditional Independence</h2>
<ul>
<li>Particular to naive Bayes: assume <em>features</em> are also
conditionally independent, given param <em>and</em> the label. <span
class="math display">\[p(\mathbf{ x}_i | y_i, \boldsymbol{ \theta}) =
\prod_{j=1}^{p} p(x_{i,j}|y_i,\boldsymbol{ \theta})\]</span> where <span
class="math inline">\(p\)</span> is the dimensionality of our
inputs.</li>
<li>This is known as the <em>naive Bayes</em> assumption.</li>
<li>Bayes classifier + feature conditional independence.</li>
</ul>
</section>
<section id="marginal-density-for-y_i" class="slide level2">
<h2>Marginal Density for <span class="math inline">\(y_i\)</span></h2>
<ul>
<li><p>To specify the joint distribution we also need the marginal for
<span class="math inline">\(p(y_i)\)</span> <span
class="math display">\[p(x_{i,j},y_i| \boldsymbol{ \theta}) =
p(x_{i,j}|y_i, \boldsymbol{ \theta})p(y_i).\]</span></p></li>
<li><p>Because <span class="math inline">\(y_i\)</span> is binary the
<em>Bernoulli</em> density makes a suitable choice for our prior over
<span class="math inline">\(y_i\)</span>, <span
class="math display">\[p(y_i|\pi) = \pi^{y_i} (1-\pi)^{1-y_i}\]</span>
where <span class="math inline">\(\pi\)</span> now has the
interpretation as being the <em>prior</em> probability that the
classification should be positive.</p></li>
</ul>
</section>
<section id="joint-density-for-naive-bayes" class="slide level2">
<h2>Joint Density for Naive Bayes</h2>
<ul>
<li>This allows us to write down the full joint density of the training
data, <span class="math display">\[
p(\mathbf{ y}, \mathbf{X}|\boldsymbol{ \theta}, \pi) = \prod_{i=1}^{n}
\prod_{j=1}^{p} p(x_{i,j}|y_i, \boldsymbol{ \theta})p(y_i|\pi)
\]</span> which can now be fit by maximum likelihood.</li>
</ul>
</section>
<section id="objective-function" class="slide level2">
<h2>Objective Function</h2>
<p><span class="math display">\[\begin{align*}
E(\boldsymbol{ \theta}, \pi)&amp; =  -\log p(\mathbf{ y},
\mathbf{X}|\boldsymbol{ \theta}, \pi) \\ &amp;= -\sum_{i=1}^{n}
\sum_{j=1}^{p} \log p(x_{i, j}|y_i, \boldsymbol{ \theta})
-  \sum_{i=1}^{n} \log p(y_i|\pi),
\end{align*}\]</span></p>
</section>
<section id="maximum-likelihood" class="slide level2">
<h2>Maximum Likelihood</h2>
</section>
<section id="fit-prior" class="slide level2">
<h2>Fit Prior</h2>
<ul>
<li>We can minimize prior. For Bernoulli likelihood over the labels we
have, <span class="math display">\[\begin{align*}
E(\pi) &amp; = - \sum_{i=1}^{n}\log p(y_i|\pi)\\ &amp; = -\sum_{i=1}^{n}
y_i \log \pi - \sum_{i=1}^{n} (1-y_i) \log (1-\pi)
\end{align*}\]</span></li>
<li>Solution from above is <span class="math display">\[
\pi = \frac{\sum_{i=1}^{n} y_i}{n}.
\]</span></li>
</ul>
</section>
<section id="fit-conditional" class="slide level2">
<h2>Fit Conditional</h2>
<ul>
<li>Minimize conditional distribution: <span class="math display">\[
E(\boldsymbol{ \theta}) = -\sum_{i=1}^{n} \sum_{j=1}^{p} \log p(x_{i, j}
|y_i, \boldsymbol{ \theta}),
\]</span></li>
<li>Implies making an assumption about it’s form.</li>
<li>The right assumption will depend on the data.</li>
<li>E.g. for real valued data, use a Gaussian <span
class="math display">\[
p(x_{i, j} | y_i,\boldsymbol{ \theta}) =
\frac{1}{\sqrt{2\pi \sigma_{y_i,j}^2}} \exp \left(-\frac{(x_{i,j} -
\mu_{y_i,
j})^2}{\sigma_{y_i,j}^2}\right),
\]</span></li>
</ul>
<!-- SECTION Nigeria NMIS Data -->
</section>
<section id="nigeria-nmis-data" class="slide level2">
<h2>Nigeria NMIS Data</h2>
</section>
<section id="nigeria-nmis-data-notebook" class="slide level2">
<h2>Nigeria NMIS Data: Notebook</h2>
<div class="figure">
<div id="nigerian-health-facilities-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//ml/nigerian-health-facilities.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Location of the over thirty-four thousand health facilities registered
in the NMIS data across Nigeria. Each facility plotted according to its
latitude and longitude.
</aside>
</section>
<section id="nigeria-nmis-data-classification" class="slide level2">
<h2>Nigeria NMIS Data Classification</h2>
<p>The distributions show the parameters of the <em>independent</em>
class conditional probabilities for no maternity services. It is a
Bernoulli distribution with the parameter, <span
class="math inline">\(\pi\)</span>, given by (<code>theta_0</code>) for
the facilities without maternity services and <code>theta_1</code> for
the facilities with maternity services. The parameters whow that,
facilities with maternity services also are more likely to have other
services such as grid electricity, emergency transport, immunization
programs etc.</p>
<p>The naive Bayes assumption says that the joint probability for these
services is given by the product of each of these Bernoulli
distributions.</p>
<p>We have modelled the numbers in our table with a Gaussian density.
Since several of these numbers are counts, a more appropriate
distribution might be the Poisson distribution. But here we can see that
the average number of nurses, healthworkers and doctors is
<em>higher</em> in the facilities with maternal services
(<code>mu_1</code>) than those without maternal services
(<code>mu_0</code>). There is also a small difference between the mean
latitude and longitudes. However, the <em>standard deviation</em> which
would be given by the square root of the variance parameters
(<code>sigma_0</code> and <code>sigma_1</code>) is large, implying that
a difference in latitude and longitude may be due to sampling error. To
be sure more analysis would be required.</p>
</section>
<section id="compute-posterior-for-test-point-label"
class="slide level2">
<h2>Compute Posterior for Test Point Label</h2>
<ul>
<li>We know that <span class="math display">\[
P(y^*| \mathbf{ y}, \mathbf{X}, \mathbf{ x}^*, \boldsymbol{
\theta})p(\mathbf{ y},\mathbf{X}, \mathbf{ x}^*|\boldsymbol{ \theta}) =
p(y*, \mathbf{ y}, \mathbf{X},\mathbf{ x}^*| \boldsymbol{ \theta})
\]</span></li>
<li>This implies <span class="math display">\[
P(y^*| \mathbf{ y}, \mathbf{X}, \mathbf{ x}^*, \boldsymbol{ \theta}) =
\frac{p(y*, \mathbf{ y}, \mathbf{X}, \mathbf{ x}^*| \boldsymbol{
\theta})}{p(\mathbf{ y}, \mathbf{X}, \mathbf{ x}^*|\boldsymbol{
\theta})}
\]</span></li>
</ul>
</section>
<section id="compute-posterior-for-test-point-label-1"
class="slide level2">
<h2>Compute Posterior for Test Point Label</h2>
<ul>
<li>From conditional independence assumptions <span
class="math display">\[
p(y^*, \mathbf{ y}, \mathbf{X}, \mathbf{ x}^*| \boldsymbol{ \theta}) =
\prod_{j=1}^{p} p(x^*_{j}|y^*, \boldsymbol{
\theta})p(y^*|\pi)\prod_{i=1}^{n} \prod_{j=1}^{p} p(x_{i,j}|y_i,
\boldsymbol{ \theta})p(y_i|\pi)
\]</span></li>
<li>We also need <span class="math display">\[
p(\mathbf{ y}, \mathbf{X}, \mathbf{ x}^*|\boldsymbol{ \theta})\]</span>
which can be found from <span class="math display">\[p(y^*, \mathbf{ y},
\mathbf{X}, \mathbf{ x}^*| \boldsymbol{ \theta})
\]</span></li>
<li>Using the <em>sum rule</em> of probability, <span
class="math display">\[
p(\mathbf{ y}, \mathbf{X}, \mathbf{ x}^*|\boldsymbol{ \theta}) =
\sum_{y^*=0}^1 p(y^*, \mathbf{ y}, \mathbf{X}, \mathbf{ x}^*|
\boldsymbol{ \theta}).
\]</span></li>
</ul>
</section>
<section id="independence-assumptions" class="slide level2">
<h2>Independence Assumptions</h2>
<ul>
<li>From independence assumptions <span class="math display">\[
p(\mathbf{ y}, \mathbf{X}, \mathbf{ x}^*| \boldsymbol{ \theta}) =
\sum_{y^*=0}^1 \prod_{j=1}^{p} p(x^*_{j}|y^*_i, \boldsymbol{
\theta})p(y^*|\pi)\prod_{i=1}^{n} \prod_{j=1}^{p} p(x_{i,j}|y_i,
\boldsymbol{ \theta})p(y_i|\pi).
\]</span></li>
<li>Substitute both forms to recover, <span class="math display">\[
P(y^*| \mathbf{ y}, \mathbf{X}, \mathbf{ x}^*, \boldsymbol{ \theta})  =
\frac{\prod_{j=1}^{p} p(x^*_{j}|y^*_i, \boldsymbol{
\theta})p(y^*|\pi)\prod_{i=1}^{n} \prod_{j=1}^{p} p(x_{i,j}|y_i,
\boldsymbol{ \theta})p(y_i|\pi)}{\sum_{y^*=0}^1 \prod_{j=1}^{p}
p(x^*_{j}|y^*_i, \boldsymbol{ \theta})p(y^*|\pi)\prod_{i=1}^{n}
\prod_{j=1}^{p} p(x_{i,j}|y_i, \boldsymbol{ \theta})p(y_i|\pi)}
\]</span></li>
</ul>
</section>
<section id="cancelation" class="slide level2">
<h2>Cancelation</h2>
<ul>
<li>Note training data terms cancel. <span class="math display">\[
p(y^*| \mathbf{ x}^*, \boldsymbol{ \theta}) = \frac{\prod_{j=1}^{p}
p(x^*_{j}|y^*_i, \boldsymbol{ \theta})p(y^*|\pi)}{\sum_{y^*=0}^1
\prod_{j=1}^{p} p(x^*_{j}|y^*_i, \boldsymbol{ \theta})p(y^*|\pi)}
\]</span></li>
<li>This formula is also fairly straightforward to implement for
different class conditional distributions.</li>
</ul>
</section>
<section id="laplace-smoothing" class="slide level2">
<h2>Laplace Smoothing</h2>
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PA16"><img
data-src="https://mlatcl.github.io/mlai/./slides/diagrams//books/1YQPAAAAQAAJ-PA16.png" /></a>
</div>
</section>
<section id="pseudo-counts" class="slide level2">
<h2>Pseudo Counts</h2>
<p><span class="math display">\[
\pi = \frac{\sum_{i=1}^{n} y_i + 1}{n+ 2}
\]</span></p>
</section>
<section id="naive-bayes-summary" class="slide level2">
<h2>Naive Bayes Summary</h2>
<ul>
<li>Model <em>full</em> joint distribution of data, <span
class="math inline">\(p(\mathbf{ y}, \mathbf{X}| \boldsymbol{ \theta},
\pi)\)</span></li>
<li>Make conditional independence assumptions about the data.
<ul>
<li>feature conditional independence</li>
<li>data conditional independence</li>
</ul></li>
<li>Fast to implement, works on very large data.</li>
<li>Despite simple assumptions can perform better than expected.</li>
</ul>
</section>
<section id="further-reading" class="slide level2 scrollable">
<h2 class="scrollable">Further Reading</h2>
<ul>
<li>Chapter 5 up to pg 179 (Section 5.1, and 5.2 up to 5.2.2) of <span
class="citation" data-cites="Rogers:book11">Rogers and Girolami
(2011)</span></li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a
href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></p></li>
<li><p>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></p></li>
<li><p>blog posts:</p>
<p><a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Bayes:doctrine63" class="csl-entry" role="listitem">
Bayes, T., 1763. An essay towards solving a problem in the doctrine of
chances. Philosophical Transactions of the Royal Society 53, 370–418. <a
href="https://doi.org/10.1098/rstl.1763.0053">https://doi.org/10.1098/rstl.1763.0053</a>
</div>
<div id="ref-Laplace:memoire74" class="csl-entry" role="listitem">
Laplace, P.S., 1774. Mémoire sur la probabilité des causes par les
évènemens, in: Mémoires de Mathèmatique Et de Physique, Presentés à
lAcadémie Royale Des Sciences, Par Divers Savans, &amp; Lù Dans Ses
Assemblées 6. pp. 621–656.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="listitem">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC
Press.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
