<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Logistic Regression and GLMs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Logistic Regression and GLMs</h1>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
</section>
<section id="review" class="slide level2">
<h2>Review</h2>
<ul>
<li>Last week: Specified Class Conditional Distributions, <span class="math inline">\(p(\mathbf{ x}_i|y_i, \boldsymbol{ \theta})\)</span>.</li>
<li>Used Bayes Classifier + naive Bayes model to specify joint distribution.</li>
<li>Used Bayes rule to compute posterior probability of class membership.</li>
<li>This week:
<ul>
<li>direct estimation of probability of class membership.</li>
<li>introduction of generalised linear models.</li>
</ul></li>
</ul>
</section>
<section id="logistic-regression-and-glms" class="slide level2">
<h2>Logistic Regression and GLMs</h2>
<ul>
<li>Modelling entire density allows any question to be answered (also missing data).</li>
<li>Comes at the possible expense of <em>strong</em> assumptions about data generation distribution.</li>
<li>In regression we model probability of <span class="math inline">\(y_i |\mathbf{ x}_i\)</span> directly.
<ul>
<li><strong>Allows less flexibility in the question, but more flexibility in the model assumptions.</strong></li>
</ul></li>
<li>Can do this not just for regression, but classification.</li>
<li>Framework is known as <em>generalized linear models</em>.</li>
</ul>
</section>
<section id="log-odds" class="slide level2">
<h2>Log Odds</h2>
<ul>
<li>model the <em>log-odds</em> with the basis functions.</li>
<li><a href="http://en.wikipedia.org/wiki/Odds">odds</a> are defined as the ratio of the probability of a positive outcome, to the probability of a negative outcome.</li>
<li>Probability is between zero and one, odds are: <span class="math display">\[ \frac{\pi}{1-\pi} \]</span></li>
<li>Odds are between <span class="math inline">\(0\)</span> and <span class="math inline">\(\infty\)</span>.</li>
<li>Logarithm of odds maps them to <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>.</li>
</ul>
</section>
<section id="logit-link-function" class="slide level2">
<h2>Logit Link Function</h2>
<ul>
<li>The <a href="http://en.wikipedia.org/wiki/Logit">Logit function</a>, <span class="math display">\[g^{-1}(\pi_i) = \log\frac{\pi_i}{1-\pi_i}.\]</span> This function is known as a <em>link function</em>.</li>
<li>For a standard regression we take, <span class="math display">\[f(\mathbf{ x}_i) = \mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i),\]</span></li>
<li>For classification we perform a logistic regression. <span class="math display">\[\log \frac{\pi_i}{1-\pi_i} = \mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\]</span></li>
</ul>
</section>
<section id="inverse-link-function" class="slide level2">
<h2>Inverse Link Function</h2>
<p>We have defined the link function as taking the form <span class="math inline">\(g^{-1}(\cdot)\)</span> implying that the inverse link function is given by <span class="math inline">\(g(\cdot)\)</span>. Since we have defined, <span class="math display">\[
g^{-1}(\pi(\mathbf{ x})) = \mathbf{ w}^\top\boldsymbol{ \phi}(\mathbf{ x})
\]</span> we can write <span class="math inline">\(\pi\)</span> in terms of the <em>inverse link</em> function, <span class="math inline">\(g(\cdot)\)</span> as <span class="math display">\[
\pi(\mathbf{ x}) = g(\mathbf{ w}^\top\boldsymbol{ \phi}(\mathbf{ x})).
\]</span></p>
</section>
<section id="logistic-function" class="slide level2">
<h2>Logistic function</h2>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Logistic_function">Logistic</a> (or sigmoid) squashes real line to between 0 &amp; 1. Sometimes also called a ‘squashing function.’ <object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/logistic.svg" width="" style=" "></object></li>
</ul>
</section>
<section id="basis-function" class="slide level2">
<h2>Basis Function</h2>
</section>
<section id="prediction-function" class="slide level2">
<h2>Prediction Function</h2>
<ul>
<li>Can now write <span class="math inline">\(\pi\)</span> as a function of the input and the parameter vector as, <span class="math display">\[\pi(\mathbf{ x},\mathbf{ w}) = \frac{1}{1+
\exp\left(-\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})\right)}.\]</span></li>
<li>Compute the output of a standard linear basis function composition (<span class="math inline">\(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})\)</span>, as we did for linear regression)</li>
<li>Apply the inverse link function, <span class="math inline">\(g(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}))\)</span>.</li>
<li>Use this value in a Bernoulli distribution to form the likelihood.</li>
</ul>
</section>
<section id="bernoulli-reminder" class="slide level2">
<h2>Bernoulli Reminder</h2>
<ul>
<li><p>From last time <span class="math display">\[P(y_i|\mathbf{ w}, \mathbf{ x}) = \pi_i^{y_i} (1-\pi_i)^{1-y_i}\]</span></p></li>
<li><p>Trick for switching betwen probabilities</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli(y, pi):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pi</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>pi</span></code></pre></div>
</section>
<section id="maximum-likelihood" class="slide level2">
<h2>Maximum Likelihood</h2>
<ul>
<li>Conditional independence of data: <span class="math display">\[P(\mathbf{ y}|\mathbf{ w}, \mathbf{X}) = \prod_{i=1}^nP(y_i|\mathbf{ w},
\mathbf{ x}_i). \]</span></li>
</ul>
</section>
<section id="log-likelihood" class="slide level2">
<h2>Log Likelihood</h2>
<p><span class="math display">\[\begin{align*}
  \log P(\mathbf{ y}|\mathbf{ w}, \mathbf{X}) = &amp;
  \sum_{i=1}^n\log P(y_i|\mathbf{ w}, \mathbf{ x}_i) \\ = &amp;\sum_{i=1}^ny_i \log
  \pi_i \\ &amp; + \sum_{i=1}^n(1-y_i)\log (1-\pi_i)
\end{align*}\]</span></p>
</section>
<section id="objective-function" class="slide level2">
<h2>Objective Function</h2>
<ul>
<li>Probability of positive outcome for the <span class="math inline">\(i\)</span>th data point <span class="math display">\[\pi_i = g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\right),\]</span> where <span class="math inline">\(g(\cdot)\)</span> is the <em>inverse</em> link function</li>
<li>Objective function of the form <span class="math display">\[\begin{align*}
  E(\mathbf{ w}) = &amp; -  \sum_{i=1}^ny_i \log
  g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\right) \\&amp; -
  \sum_{i=1}^n(1-y_i)\log \left(1-g\left(\mathbf{ w}^\top
  \boldsymbol{ \phi}(\mathbf{ x}_i)\right)\right).
 \end{align*}\]</span></li>
</ul>
</section>
<section id="minimize-objective" class="slide level2">
<h2>Minimize Objective</h2>
<ul>
<li>Grdient wrt <span class="math inline">\(\pi(\mathbf{ x};\mathbf{ w})\)</span> <span class="math display">\[\begin{align*}
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = &amp;
-\sum_{i=1}^n\frac{y_i}{g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i) \\ &amp; +  \sum_{i=1}^n
\frac{1-y_i}{1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i)
\end{align*}\]</span></li>
</ul>
</section>
<section id="link-function-gradient" class="slide level2">
<h2>Link Function Gradient</h2>
<ul>
<li>Also need gradient of inverse link function wrt parameters. <span class="math display">\[\begin{align*}
g(f_i) &amp;= \frac{1}{1+\exp(-f_i)}\\
&amp;=(1+\exp(-f_i))^{-1}
\end{align*}\]</span> and the gradient can be computed as <span class="math display">\[\begin{align*}
\frac{\text{d}g(f_i)}{\text{d} f_i} &amp; =
\exp(-f_i)(1+\exp(-f_i))^{-2}\\
&amp; = \frac{1}{1+\exp(-f_i)}
\frac{\exp(-f_i)}{1+\exp(-f_i)} \\
&amp; = g(f_i) (1-g(f_i))
\end{align*}\]</span></li>
</ul>
</section>
<section id="objective-gradient" class="slide level2">
<h2>Objective Gradient</h2>
<p><span class="math display">\[\begin{align*}
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = &amp; -\sum_{i=1}^n
y_i\left(1-g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i) \\ &amp; + \sum_{i=1}^n
(1-y_i)\left(g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i).
\end{align*}\]</span></p>
</section>
<section id="optimization-of-the-function" class="slide level2">
<h2>Optimization of the Function</h2>
<ul>
<li>Can’t find a stationary point of the objective function analytically.</li>
<li>Optimization has to proceed by <em>numerical methods</em>.
<ul>
<li><a href="http://en.wikipedia.org/wiki/Newton%27s_method">Newton’s method</a> or</li>
<li><a href="http://en.wikipedia.org/wiki/Gradient_method">gradient based optimization methods</a></li>
</ul></li>
<li>Similarly to matrix factorization, for large data <em>stochastic gradient descent</em> (Robbins Munro <span class="citation" data-cites="Robbins:stoch51">(Robbins and Monro, 1951)</span> optimization procedure) works well.</li>
</ul>
</section>
<section id="ad-matching-for-facebook" class="slide level2">
<h2>Ad Matching for Facebook</h2>
<ul>
<li>This approach used in many internet companies.</li>
<li>Example: ad matching for Facebook.
<ul>
<li>Millions of advertisers</li>
<li>Billions of users</li>
<li>How do you choose who to show what?</li>
</ul></li>
<li>Logistic regression used in combination with <a href="">decision trees</a></li>
<li><a href="http://www.herbrich.me/papers/adclicksfacebook.pdf">Paper available here</a></li>
</ul>
</section>
<section id="going-further-optimization" class="slide level2">
<h2>Going Further: Optimization</h2>
<p>Other optimization techniques for generalized linear models include <a href="http://en.wikipedia.org/wiki/Newton%27s_method">Newton’s method</a>, it requires you to compute the Hessian, or second derivative of the objective function.</p>
<p>Methods that are based on gradients only include <a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> and <a href="http://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate gradients</a>. Can you find these in python? Are they suitable for very large data sets? }</p>
</section>
<section id="other-glms" class="slide level2">
<h2>Other GLMs</h2>
<ul>
<li>Logistic regression is part of a family known as <em>generalized linear models</em></li>
<li>They all take the form <span class="math display">\[g^{-1}(f_i(x)) = \mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\]</span></li>
<li>Other examples include <em>Poisson regression</em>.</li>
</ul>
</section>
<section id="poisson-distribution" class="slide level2">
<h2>Poisson Distribution</h2>
<ul>
<li>Poisson distribution is used for ‘count data.’ For non-negative integers, <span class="math inline">\(y\)</span>, <span class="math display">\[P(y) = \frac{\lambda^y}{y!}\exp(-y)\]</span></li>
<li>Here <span class="math inline">\(\lambda\)</span> is a <em>rate</em> parameter that can be thought of as the number of arrivals per unit time.</li>
<li>Poisson distributions can be used for disease count data. E.g. number of incidence of malaria in a district.</li>
</ul>
</section>
<section id="poisson-distribution-1" class="slide level2">
<h2>Poisson Distribution</h2>
<div class="figure">
<div id="the-poisson-distribution-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/poisson.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Poisson distribution.
</aside>
</section>
<section id="poisson-regression" class="slide level2">
<h2>Poisson Regression</h2>
<ul>
<li>In a Poisson regression make rate a function of space/time. <span class="math display">\[\log \lambda(\mathbf{ x}, t) = \mathbf{ w}_x^\top
\boldsymbol{ \phi}_x(\mathbf{ x}) + \mathbf{ w}_t^\top \boldsymbol{ \phi}_t(t)\]</span></li>
<li>This is known as a <em>log linear</em> or <em>log additive</em> model.</li>
<li>The link function is a logarithm.</li>
<li>We can rewrite such a function as <span class="math display">\[\log \lambda(\mathbf{ x}, t) = f_x(\mathbf{ x}) + f_t(t)\]</span></li>
</ul>
</section>
<section id="multiplicative-model" class="slide level2">
<h2>Multiplicative Model</h2>
<ul>
<li>Be careful though … a log additive model is really multiplicative. <span class="math display">\[\log \lambda(\mathbf{ x}, t) = f_x(\mathbf{ x}) + f_t(t)\]</span></li>
<li>Becomes <span class="math display">\[\lambda(\mathbf{ x}, t) = \exp(f_x(\mathbf{ x}) + f_t(t))\]</span></li>
<li>Which is equivalent to <span class="math display">\[\lambda(\mathbf{ x}, t) = \exp(f_x(\mathbf{ x}))\exp(f_t(t))\]</span></li>
<li>Link functions can be deceptive in this way.</li>
</ul>
</section>
<section id="bayesian-approaches" class="slide level2">
<h2>Bayesian Approaches</h2>
</section>
<section id="further-reading" class="slide level2 scrollable">
<h2 class="scrollable">Further Reading</h2>
<ul>
<li>Section 5.2.2 up to pg 182 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Robbins:stoch51" class="csl-entry" role="doc-biblioentry">
Robbins, H., Monro, S., 1951. A stochastic approximation method. Annals of Mathematical Statistics 22, 400–407.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="doc-biblioentry">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
