<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2015-11-17">
  <title>Dimensionality Reduction: Latent Variable Modelling</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Dimensionality Reduction: Latent Variable Modelling</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2015-11-17</time></p>
  <p class="venue" style="text-align:center">University of Sheffield</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<p>import pods import mlai import numpy as np import matplotlib.pyplot as plt %matplotlib inline</p>
</section>
<section id="review" class="slide level2">
<h2>Review</h2>
<ul>
<li>Last time: Looked at Bayesian Regression.</li>
<li>Introduced priors and marginal likelihoods.</li>
<li>This time: Unsupervised Learning</li>
</ul>
</section>
<section id="unsupervised-learning" class="slide level2">
<h2>Unsupervised Learning</h2>
<ul>
<li>Supervised learning is learning where each data has a label (e.g. regression output)</li>
<li>In unsupervised learning we have no labels for the data.</li>
<li>Often thought of as structure discovery.
<ul>
<li>Finding features in the data</li>
<li>Exploratory data analysis</li>
</ul></li>
</ul>
</section>
<section id="clustering" class="slide level2">
<h2>Clustering</h2>
<ul>
<li>One common approach, not deeply covered in this course.</li>
<li>Associate each data point, <span class="math inline">\(\dataVector_{i, :}\)</span> with one of <span class="math inline">\(k\)</span> different discrete groups.</li>
<li>For example:
<ul>
<li>Clustering animals into discrete groups. Are animals discrete or continuous?</li>
<li>Clustering into different different <em>political</em> affiliations.</li>
</ul></li>
<li>Humans do seem to like clusters:
<ul>
<li>Very useful when interacting with biologists.</li>
</ul></li>
<li>Subtle difference between clustering and <em>vector quantisation</em></li>
</ul>
</section>
<section id="trying-to-teach-about-infinity" class="slide level2">
<h2>Trying to Teach About Infinity</h2>
<ul>
<li>Little anecdote.</li>
</ul>
</section>
<section id="clustering-and-vector-quantisation" class="slide level2">
<h2>Clustering and Vector Quantisation</h2>
<ul>
<li>To my mind difference is in clustering there should be a reduction in data density between samples.</li>
<li>This definition is not universally applied.</li>
<li>For today’s purposes we merge them:
<ul>
<li>Determine how to allocate each point to a group and <em>harder</em> total number of groups.</li>
</ul></li>
</ul>
</section>
<section id="k-means-clustering" class="slide level2">
<h2><span class="math inline">\(k\)</span>-means Clustering</h2>
<ul>
<li>Simple algorithm for allocating points to groups.</li>
<li><em>Require</em>: Set of <span class="math inline">\(k\)</span> cluster centres &amp; assignment of each points to a cluster.</li>
</ul>
<ol type="1">
<li>Initialize cluster centres as randomly selected data points.
<ol start="2" type="1">
<li>Assign each data point to <em>nearest</em> cluster centre.</li>
<li>Update each cluster centre by setting it to the mean of assigned data points.</li>
<li>Repeat 2 and 3 until cluster allocations do not change.</li>
</ol></li>
</ol>
</section>
<section id="objective-function" class="slide level2">
<h2>Objective Function</h2>
<ul>
<li>This minimizes the objective <span class="math display">\[
E=\sum_{j=1}^K \sum_{i\ \text{allocated to}\ j}  \left(\dataVector_{i, :} - \meanVector_{j, :}\right)^\top\left(\dataVector_{i, :} - \meanVector_{j, :}\right)
\]</span> <em>i.e.</em> it minimizes thesum of Euclidean squared distances betwen points and their associated centres.</li>
<li>The minimum is <em>not</em> guaranteed to be <em>global</em> or <em>unique</em>.</li>
<li>This objective is a non-convex optimization problem.</li>
</ul>
</section>
<section id="other-clustering-approaches" class="slide level2">
<h2>Other Clustering Approaches</h2>
<ul>
<li>Spectral clustering (<span class="citation" data-cites="Shi:normalized00">Shi and Malik (2000)</span>,<span class="citation" data-cites="Ng:spectral02">Ng et al. (n.d.)</span>)
<ul>
<li>Allows clusters which aren’t convex hulls.</li>
</ul></li>
<li>Dirichlet process
<ul>
<li>A probabilistic formulation for a clustering algorithm that is <em>non-parametric</em>.</li>
<li>Loosely speaking it allows infinite clusters</li>
<li>In practice useful for dealing with previously unknown species (e.g. a “Black Swan Event”).</li>
</ul></li>
</ul>
</section>
<section id="high-dimensional-data" class="slide level2">
<h2>High Dimensional Data</h2>
<ul>
<li>USPS Data Set Handwritten Digit</li>
<li>3648 dimensions (64 rows, 57 columns)</li>
<li>Space contains much more than just this digit.</li>
</ul>
<p>from ipywidgets import IntSlider</p>
</section>
<section id="usps-samples" class="slide level2">
<h2>USPS Samples</h2>
<ul>
<li>Even if we sample every nanonsecond from now until end of universe you won’t see original six!</li>
</ul>
</section>
<section id="simple-model-of-digit" class="slide level2">
<h2>Simple Model of Digit</h2>
<ul>
<li>Rotate a prototype</li>
</ul>
<p>from scipy.misc import imrotate</p>
<p>six_image = np.hstack([np.zeros((rows, 3)), six_image, np.zeros((rows, 4))]) dim_one = np.asarray(six_image.shape) angles = range(360) i = 0 Y = np.zeros((len(angles), np.prod(dim_one))) for angle in angles: rot_image = imrotate(six_image, angle, interp=‘nearest’) dim_two = np.asarray(rot_image.shape) start = [int(round((dim_two[0] - dim_one[0])/2)), int(round((dim_two[1] - dim_one[1])/2))] crop_image = rot_image[start[0]+np.array(range(dim_one[0])), start[1]+np.array(range(dim_one[1]))] Y[i, :] = crop_image.flatten()</p>
</section>
<section id="low-dimensional-manifolds" class="slide level2">
<h2>Low Dimensional Manifolds</h2>
<ul>
<li>Pure rotation is too simple
<ul>
<li>In practice data may undergo several distortions.</li>
</ul></li>
<li>For high dimensional data with <em>structure</em>:
<ul>
<li>We expect fewer distortions than dimensions;</li>
<li>Therefore we expect the data to live on a lower dimensional manifold.</li>
<li>Conclusion: Deal with high dimensional data by looking for a lower dimensional non-linear embedding.</li>
</ul></li>
</ul>
</section>
<section id="principal-component-analysis" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>PCA (<span class="citation" data-cites="Hotelling:analysis33">Hotelling (1933)</span>) is a linear embedding.</li>
<li>Today its presented as:
<ul>
<li>Rotate to find ‘directions’ in data with maximal variance.</li>
<li>How do we find these directions?</li>
</ul></li>
<li>Algorithmically we do this by diagonalizing the sample covariance matrix <span class="math display">\[
\mathbf{S}=\frac{1}{\numData}\sum_{i=1}^\numData \left(\dataVector_{i, :}-\meanVector\right)\left(\dataVector_{i, :} - \meanVector\right)^\top
\]</span></li>
</ul>
</section>
<section id="principal-component-analysis-1" class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li>Find directions in the data, <span class="math inline">\(\latentVector = \mathbf{U}\dataVector\)</span>, for which variance is maximized.</li>
</ul>
</section>
<section id="lagrangian" class="slide level2">
<h2>Lagrangian</h2>
<ul>
<li><p>Solution is found via constrained optimisation (which uses <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>): <span class="math display">\[
L\left(\mathbf{u}_{1},\lambda_{1}\right)=\mathbf{u}_{1}^{\top}\mathbf{S}\mathbf{u}_{1}+\lambda_{1}\left(1-\mathbf{u}_{1}^{\top}\mathbf{u}_{1}\right)
\]</span></p></li>
<li>Gradient with respect to <span class="math inline">\(\mathbf{u}_{1}\)</span> <span class="math display">\[\frac{\text{d}L\left(\mathbf{u}_{1},\lambda_{1}\right)}{\text{d}\mathbf{u}_{1}}=2\mathbf{S}\mathbf{u}_{1}-2\lambda_{1}\mathbf{u}_{1}\]</span> rearrange to form <span class="math display">\[\mathbf{S}\mathbf{u}_{1}=\lambda_{1}\mathbf{u}_{1}.\]</span> Which is known as an <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><em>eigenvalue problem</em></a>.</li>
<li><p>Further directions that are <em>orthogonal</em> to the first can also be shown to be eigenvectors of the covariance.</p></li>
</ul>
</section>
<section id="linear-dimensionality-reduction" class="slide level2">
<h2>Linear Dimensionality Reduction</h2>
<ul>
<li>Represent data, <span class="math inline">\(\dataMatrix\)</span>, with a lower dimensional set of latent variables <span class="math inline">\(\latentMatrix\)</span>.</li>
<li>Assume a linear relationship of the form <span class="math display">\[
\dataVector_{i,:}=\mappingMatrix\latentVector_{i,:}+\noiseVector_{i,:},
\]</span> where <span class="math display">\[
\noiseVector_{i,:} \sim \gaussianSamp{\zerosVector}{\noiseStd^2\eye}
\]</span></li>
</ul>
</section>
<section id="linear-latent-variable-model" class="slide level2">
<h2>Linear Latent Variable Model</h2>
<p><strong>Probabilistic PCA</strong></p>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent variables and data.</li>
<li><strong>Standard</strong> Latent variable approach:
<ul>
<li>Define Gaussian prior over <em>latent space</em>, <span class="math inline">\(\latentMatrix\)</span>.</li>
</ul></li>
<li>Integrate out <em>latent variables</em>.</li>
</ul>

<p><span class="math display">\[
p\left(\dataMatrix|\latentMatrix,\mappingMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\dataVector_{i,:}}{\mappingMatrix\latentVector_{i,:}}{\noiseStd^2\eye}
\]</span></p>
<p><span class="math display">\[
p\left(\latentMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\latentVector_{i,:}}{\zerosVector}{\eye}
\]</span></p>
<p><span class="math display">\[
p\left(\dataMatrix|\mappingMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\dataVector_{i,:}}{\zerosVector}{\mappingMatrix\mappingMatrix^{\top}+\noiseStd^{2}\eye}
\]</span></p>
</section>
<section id="computation-of-the-marginal-likelihood" class="slide level2">
<h2>Computation of the Marginal Likelihood</h2>
<p><span class="math display">\[
\dataVector_{i,:}=\mappingMatrix\latentVector_{i,:}+\noiseVector_{i,:},\quad \latentVector_{i,:} \sim \gaussianSamp{\zerosVector}{\eye}, \quad \noiseVector_{i,:} \sim \gaussianSamp{\zerosVector}{\noiseStd^{2}\eye}
\]</span></p>
<p><span class="math display">\[
\mappingMatrix\latentVector_{i,:} \sim \gaussianSamp{\zerosVector}{\mappingMatrix\mappingMatrix^\top}
\]</span></p>
<p><span class="math display">\[
\mappingMatrix\latentVector_{i, :} + \noiseVector_{i, :} \sim \gaussianSamp{\zerosVector}{\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye}
\]</span></p>
</section>
<section id="linear-latent-variable-model-ii" class="slide level2">
<h2>Linear Latent Variable Model II</h2>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span class="citation" data-cites="Tipping:probpca99">Tipping and Bishop (1999)</span>)</p>
%\includegraphics&lt;1&gt;[width=0.25]{../../../gplvm/tex/diagrams/ppcaGraph}

<p><span class="math display">\[p\left(\dataMatrix|\mappingMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\dataVector_{i, :}}{\zerosVector}{\mappingMatrix\mappingMatrix^{\top}+\noiseStd^{2}\eye}\]</span></p>
</section>
<section id="linear-latent-variable-model-ii-1" class="slide level2">
<h2>Linear Latent Variable Model II</h2>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span class="citation" data-cites="Tipping:probpca99">Tipping and Bishop (1999)</span>) <span class="math display">\[
  p\left(\dataMatrix|\mappingMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\dataVector_{i,:}}{\zerosVector}{\covarianceMatrix},\quad \covarianceMatrix=\mappingMatrix\mappingMatrix^{\top}+\noiseStd^{2}\eye
  \]</span> <span class="math display">\[
  \log p\left(\dataMatrix|\mappingMatrix\right)=-\frac{\numData}{2}\log\left|\covarianceMatrix\right|-\frac{1}{2}\text{tr}\left(\covarianceMatrix^{-1}\dataMatrix^{\top}\dataMatrix\right)+\text{const.}
  \]</span> If <span class="math inline">\(\mathbf{U}_{q}\)</span> are first <span class="math inline">\(q\)</span> principal eigenvectors of <span class="math inline">\(n^{-1}\dataMatrix^{\top}\dataMatrix\)</span> and the corresponding eigenvalues are <span class="math inline">\(\boldsymbol{\Lambda}_{q}\)</span>, <span class="math display">\[
  \mappingMatrix=\mathbf{U}_{q}\mathbf{L}\mathbf{R}^{\top},\quad\mathbf{L}=\left(\boldsymbol{\Lambda}_{q}-\noiseStd^{2}\eye\right)^{\frac{1}{2}}
  \]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix.</p>
</section>
<section id="further-reading" class="slide level2 scrollable">
<h2>Further Reading</h2>
<ul>
<li>Chapter 7 up to pg 249 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></li>
</ul>


<!-- SECTION Latent Variables -->
</section>
<section id="latent-variables" class="slide level2">
<h2>Latent Variables</h2>






<!-- SECTION Your Personality -->
</section>
<section id="your-personality" class="slide level2">
<h2>Your Personality</h2>

</section>
<section id="factor-analysis-model" class="slide level2">
<h2>Factor Analysis Model</h2>

<p><span class="math display">\[
\dataVector = \mathbf{f}(\latentVector) + \noiseVector,
\]</span>  <span class="math display">\[
\mathbf{f}(\latentVector) = \mappingMatrix\latentVector
\]</span>  <span class="math display">\[
\mathbf{f}(\latentVector) =
\begin{bmatrix} f_1(\latentVector) \\ f_2(\latentVector) \\ \vdots \\
f_p(\latentVector)\end{bmatrix}
\]</span>  <span class="math display">\[
f_j(\latentVector) =
\weightVector_{j, :}^\top \latentVector,
\]</span>  <span class="math display">\[
\epsilon_j \sim \gaussianSamp{0}{\noiseStd^2_j}.
\]</span>  <span class="math display">\[
\dataMatrix
= \begin{bmatrix} \dataVector_{1, :}^\top \\ \dataVector_{2, :}^\top \\ \vdots \\
\dataVector_{n, :}^\top\end{bmatrix},
\]</span>  <span class="math display">\[
\mathbf{F} = \latentMatrix\mappingMatrix^\top,
\]</span> </p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p>Exercise 1</p>
<p>Show that, given all the definitions above, if, <span class="math display">\[
\mathbf{F} = \latentMatrix\mappingMatrix^\top
\]</span> and the elements of the vector valued function <span class="math inline">\(\mathbf{F}\)</span> are given by <span class="math display">\[
f_{i, j} = f_j(\latentVector_{i, :}),
\]</span> where <span class="math inline">\(\latentVector_{i, :}\)</span> is the <span class="math inline">\(i\)</span>th row of the latent variables, <span class="math inline">\(\latentMatrix\)</span>, then show that <span class="math display">\[
f_j(\latentVector_{i, :}) = \weightVector_{j, :}^\top
\latentVector_{i, :}
\]</span></p>
</section>
<section id="latent-variables-1" class="slide level2">
<h2>Latent Variables</h2>
<p>The difference between this model and a multiple output regression is that in the regression case we are provided with the covariates <span class="math inline">\(\latentMatrix\)</span>, here they are <em>latent variables</em>. These variables are unknown. Just as we have done in the past for unknowns, we now treat them with a probability distribution. In <em>factor analysis</em> we assume that the latent variables have a Gaussian density which is independent across both across the latent variables associated with the different data points, and across those associated with different data features, so we have, <span class="math display">\[
x_{i,j} \sim
\gaussianSamp{0}{1},
\]</span> and we can write the density governing the latent variable associated with a single point as, <span class="math display">\[
\latentVector_{i, :} \sim \gaussianSamp{\zerosVector}{\eye}.
\]</span> If we consider the values of the function for the <span class="math inline">\(i\)</span>th data point as <span class="math display">\[
\mathbf{f}_{i, :} =
\mathbf{f}(\latentVector_{i, :}) = \mappingMatrix\latentVector_{i, :} 
\]</span> then we can use the rules for multivariate Gaussian relationships to write that <span class="math display">\[
\mathbf{f}_{i, :} \sim \gaussianSamp{\zerosVector}{\mappingMatrix\mappingMatrix^\top}
\]</span> which implies that the distribution for <span class="math inline">\(\dataVector_{i, :}\)</span> is given by <span class="math display">\[
\dataVector_{i, :} = \sim \gaussianSamp{\zerosVector}{\mappingMatrix\mappingMatrix^\top + \boldsymbol{\Sigma}}
\]</span> where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> the covariance of the noise variable, <span class="math inline">\(\epsilon_{i, :}\)</span> which for factor analysis is a diagonal matrix (because we have assumed that the noise was <em>independent</em> across the features), <span class="math display">\[
\boldsymbol{\Sigma} = \begin{bmatrix}\noiseStd^2_{1} &amp; 0 &amp; 0 &amp; 0\\
0 &amp; \noiseStd^2_{2} &amp; 0 &amp; 0\\
                                     0 &amp; 0 &amp; \ddots &amp;
0\\
                                     0 &amp; 0 &amp; 0 &amp; \noiseStd^2_p\end{bmatrix}.
\]</span> For completeness, we could also add in a <em>mean</em> for the data vector <span class="math inline">\(\meanVector\)</span>, <span class="math display">\[
\dataVector_{i, :} = \mappingMatrix \latentVector_{i, :} +
\meanVector + \noiseVector_{i, :}
\]</span> which would give our marginal distribution for <span class="math inline">\(\dataVector_{i, :}\)</span> a mean <span class="math inline">\(\meanVector\)</span>. However, the maximum likelihood solution for <span class="math inline">\(\meanVector\)</span> turns out to equal the empirical mean of the data, <span class="math display">\[
\meanVector = \frac{1}{\numData} \sum_{i=1}^\numData
\dataVector_{i, :},
\]</span> <em>regardless</em> of the form of the covariance, <span class="math inline">\(\covarianceMatrix = \mappingMatrix\mappingMatrix^\top + \boldsymbol{\Sigma}\)</span>. As a result it is very common to simply preprocess the data and ensure it is zero mean. We will follow that convention for this session.</p>
<p>The prior density over latent variables is independent, and the likelihood is independent, that means that the marginal likelihood here is also independent over the data points. Factor analysis was developed mainly in psychology and the social sciences for understanding personality and intelligence. <a href="http://en.wikipedia.org/wiki/Charles_Spearman">Charles Spearman</a> was concerned with the measurements of “the abilities of man” and is credited with the earliest version of factor analysis.</p>
<!-- SECTION Principal Component Analysis -->
</section>
<section id="principal-component-analysis-2" class="slide level2">
<h2>Principal Component Analysis</h2>
<p>In 1933 <a href="http://en.wikipedia.org/wiki/Harold_Hotelling">Harold Hotelling</a> published on <em>principal component analysis</em> the first mention of this approach. Hotelling’s inspiration was to provide mathematical foundation for factor analysis methods that were by then widely used within psychology and the social sciences. His model was a factor analysis model, but he considered the noiseless ‘limit’ of the model. In other words he took <span class="math inline">\(\noiseStd^2_i \rightarrow 0\)</span> so that he had <span class="math display">\[
\dataVector_{i, :} \sim \lim_{\noiseStd^2 \rightarrow 0} \gaussianSamp{\zerosVector}{\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye}.
\]</span> The paper had two unfortunate effects. Firstly, the resulting model is no longer valid probablistically, because the covariance of this Gaussian is ‘degenerate’. Because <span class="math inline">\(\mappingMatrix\mappingMatrix^\top\)</span> has rank of at most <span class="math inline">\(q\)</span> where <span class="math inline">\(q&lt;p\)</span> (due to the dimensionality reduction) the determinant of the covariance is zero, meaning the inverse doesn’t exist so the density, <span class="math display">\[
p(\dataVector_{i, :}|\mappingMatrix) =
\lim_{\noiseStd^2 \rightarrow 0} \frac{1}{(2\pi)^\frac{p}{2}
|\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye|^{-1}}
\exp\left(-\frac{1}{2}\dataVector_{i, :}\left[\mappingMatrix\mappingMatrix^\top+ \noiseStd^2
\eye\right]^{-1}\dataVector_{i, :}\right),
\]</span> is <em>not</em> valid for <span class="math inline">\(q&lt;p\)</span> (where <span class="math inline">\(\mappingMatrix\in \Re^{p\times q}\)</span>). This mathematical consequence is a probability density which has no ‘support’ in large regions of the space for <span class="math inline">\(\dataVector_{i, :}\)</span>. There are regions for which the probability of <span class="math inline">\(\dataVector_{i, :}\)</span> is zero. These are any regions that lie off the hyperplane defined by mapping from <span class="math inline">\(\latentVector\)</span> to <span class="math inline">\(\dataVector\)</span> with the matrix <span class="math inline">\(\mappingMatrix\)</span>. In factor analysis the noise corruption, <span class="math inline">\(\noiseVector\)</span>, allows for points to be found away from the hyperplane. In Hotelling’s PCA the noise variance is zero, so there is only support for points that fall precisely on the hyperplane. Secondly, Hotelling explicity chose to rename factor analysis as principal component analysis, arguing that the factors social scientist sought were different in nature to the concept of a mathematical factor. This was unfortunate because the factor loadings, <span class="math inline">\(\mappingMatrix\)</span> can also be seen as factors in the mathematical sense because the model Hotelling defined is a Gaussian model with covariance given by <span class="math inline">\(\covarianceMatrix = \mappingMatrix\mappingMatrix^\top\)</span> so <span class="math inline">\(\mappingMatrix\)</span> is a <em>factor</em> of the covariance in the mathematical sense, as well as a factor loading.</p>
<p>However, the paper had one great advantage over standard approaches to factor analysis. Despite the fact that the model was a special case that is subsumed by the more general approach of factor analysis it is this special case that leads to a particular algorithm, namely that the factor loadings (or principal components as Hotelling referred to them) are given by an <em>eigenvalue decomposition</em> of the empirical covariance matrix.</p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p>Eigenvalue Decomposition</p>
<p>Eigenvalue problems are widespreads in physics and mathematics, they are often written as a matrix/vector equation but we prefer to write them as a full matrix equation. In an eigenvalue problem you are looking to find a matrix of eigenvectors, <span class="math inline">\(\mathbf{U}\)</span> and a <em>diagonal</em> matrix of eigenvalues, <span class="math inline">\(\boldsymbol{\Lambda}\)</span> that satisfy the <em>matrix</em> equation <span class="math display">\[
\mathbf{A}\mathbf{U} = \mathbf{U}\boldsymbol{\Lambda}.
\]</span> where <span class="math inline">\(\mathbf{A}\)</span> is your matrix of interest. This equation is not trivially solvable through matrix inverse because matrix multiplication is not <a href="http://en.wikipedia.org/wiki/Commutative_property">commutative</a>, so premultiplying by <span class="math inline">\(\mathbf{U}^{-1}\)</span> gives <span class="math display">\[
\mathbf{U}^{-1}\mathbf{A}\mathbf{U}
= \boldsymbol{\Lambda}, 
\]</span> where we remember that <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a <em>diagonal</em> matrix, so the eigenvectors can be used to <em>diagonalise</em> the matrix. When performing the eigendecomposition on a Gaussian covariances, diagonalisation is very important because it returns the covariance to a form where there is no correlation between points.</p>
</section>
<section id="positive-definite" class="slide level2">
<h2>Positive Definite</h2>
<p>We are interested in the case where <span class="math inline">\(\mathbf{A}\)</span> is a covariance matrix, which implies it is <em>positive definite</em>. A positive definite matrix is one for which the inner product, <span class="math display">\[
\weightVector^\top \covarianceMatrix\weightVector
\]</span> is positive for <em>all</em> values of the vector <span class="math inline">\(\weightVector\)</span> other than the zero vector. One way of creating a positive definite matrix is to assume that the symmetric and positive definite matrix <span class="math inline">\(\covarianceMatrix\in \Re^{p\times p}\)</span> is factorised into, <span class="math inline">\(\mathbf{A}in \Re^{p\times p}\)</span>, a <em>full rank</em> matrix, so that <span class="math display">\[
\covarianceMatrix = \mathbf{A}^\top
\mathbf{A}.
\]</span> This ensures that <span class="math inline">\(\covarianceMatrix\)</span> must be positive definite because <span class="math display">\[
\weightVector^\top \covarianceMatrix\weightVector=\weightVector^\top
\mathbf{A}^\top\mathbf{A}\weightVector 
\]</span> and if we now define a new <em>vector</em> <span class="math inline">\(\mathbf{b}\)</span> as <span class="math display">\[
\mathbf{b} = \mathbf{A}\weightVector
\]</span> we can now rewrite as <span class="math display">\[
\weightVector^\top \covarianceMatrix\weightVector = \mathbf{b}^\top\mathbf{b} = \sum_{i}
b_i^2
\]</span> which, since it is a sum of squares, is positive or zero. The constraint that <span class="math inline">\(\mathbf{A}\)</span> must be <em>full rank</em> ensures that there is no vector <span class="math inline">\(\weightVector\)</span>, other than the zero vector, which causes the vector <span class="math inline">\(\mathbf{b}\)</span> to be all zeros.</p>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<p>Exercise 1</p>
<p>If <span class="math inline">\(\covarianceMatrix=\mathbf{A}^\top \mathbf{A}\)</span> then express <span class="math inline">\(c_{i,j}\)</span>, the value of the element at the <span class="math inline">\(i\)</span>th row and the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\covarianceMatrix\)</span>, in terms of the columns of <span class="math inline">\(\mathbf{A}\)</span>. Use this to show that (i) the matrix is symmetric and (ii) the matrix has positive elements along its diagonal.</p>
<!-- SECTION Eigenvectors of a Symmetric Matric -->
</section>
<section id="eigenvectors-of-a-symmetric-matric" class="slide level2">
<h2>Eigenvectors of a Symmetric Matric</h2>
<p>Symmetric matrices have <em>orthonormal</em> eigenvectors. This means that <span class="math inline">\(\mathbf{U}\)</span> is an <a href="http://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a>, <span class="math inline">\(\mathbf{U}^\top\mathbf{U} = \eye\)</span>. This implies that <span class="math inline">\(\mathbf{u}_{:, i} ^\top \mathbf{u}_{:, j}\)</span> is equal to 0 if <span class="math inline">\(i\neq j\)</span> and 1 if <span class="math inline">\(i=j\)</span>.</p>
<!-- SECTION Probabilistic PCA -->
</section>
<section id="probabilistic-pca" class="slide level2">
<h2>Probabilistic PCA</h2>
<p>In 1997 <a href="http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf">Tipping and Bishop</a> and <a href="https://www.cs.nyu.edu/~roweis/papers/empca.pdf">Roweis</a> independently revisited Hotelling’s model and considered the case where the noise variance was finite, but <em>shared</em> across all output dimensons. Their model can be thought of as a factor analysis where <span class="math display">\[
\boldsymbol{\Sigma} = \noiseStd^2 \eye.
\]</span> This leads to a marginal likelihood of the form <span class="math display">\[
p(\dataMatrix|\mappingMatrix, \noiseStd^2)
= \prod_{i=1}^\numData\gaussianDist{\dataVector_{i, :}}{\zerosVector}{\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye}
\]</span> where the limit of <span class="math inline">\(\noiseStd^2\rightarrow 0\)</span> is <em>not</em> taken. This defines a proper probabilistic model. Tippping and Bishop then went on to prove that the <em>maximum likelihood</em> solution of this model with respect to <span class="math inline">\(\mappingMatrix\)</span> is given by an eigenvalue problem. In the probabilistic PCA case the eigenvalues and eigenvectors are given as follows. <span class="math display">\[
\mappingMatrix = \mathbf{U}\mathbf{L} \mathbf{R}^\top
\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is the eigenvectors of the empirical covariance matrix <span class="math display">\[
\mathbf{S} = \sum_{i=1}^\numData (\dataVector_{i, :} - \meanVector)(\dataVector_{i,
:} - \meanVector)^\top,
\]</span> which can be written <span class="math inline">\(\mathbf{S} = \frac{1}{\numData} \dataMatrix^\top\dataMatrix\)</span> if the data is zero mean. The matrix <span class="math inline">\(\mathbf{L}\)</span> is diagonal and is dependent on the <em>eigenvalues</em> of <span class="math inline">\(\mathbf{S}\)</span>, <span class="math inline">\(\boldsymbol{\Lambda}\)</span>. If the <span class="math inline">\(i\)</span>th diagonal element of this matrix is given by <span class="math inline">\(\lambda_i\)</span> then the corresponding element of <span class="math inline">\(\mathbf{L}\)</span> is <span class="math display">\[
\ell_i =
\sqrt{\lambda_i - \noiseStd^2}
\]</span> where <span class="math inline">\(\noiseStd^2\)</span> is the noise variance. Note that if <span class="math inline">\(\noiseStd^2\)</span> is larger than any particular eigenvalue, then that eigenvalue (along with its corresponding eigenvector) is <em>discarded</em> from the solution.</p>
</section>
<section id="python-implementation-of-probabilistic-pca" class="slide level2">
<h2>Python Implementation of Probabilistic PCA</h2>
<p>We will now implement this algorithm in python.</p>
</section>
<section><section id="probabilistic-pca-algorithm" class="title-slide slide level1"><h1>probabilistic PCA algorithm</h1></section><section id="posterior-for-principal-component-analysis" class="slide level2">
<h2>Posterior for Principal Component Analysis</h2>
<p>Under the latent variable model justification for principal component analysis, we are normally interested in inferring something about the latent variables given the data. This is the distribution, <span class="math display">\[
p(\latentVector_{i, :} | \dataVector_{i, :})
\]</span> for any given data point. Determining this density turns out to be very similar to the approach for determining the Bayesian posterior of <span class="math inline">\(\weightVector\)</span> in Bayesian linear regression, only this time we place the prior density over <span class="math inline">\(\latentVector_{i, :}\)</span> instead of <span class="math inline">\(\weightVector\)</span>. The posterior is proportional to the joint density as follows, <span class="math display">\[
p(\latentVector_{i, :} | \dataVector_{i, :}) \propto p(\dataVector_{i,
:}|\mappingMatrix, \latentVector_{i, :}, \noiseStd^2) p(\latentVector_{i, :})
\]</span> And as in the Bayesian linear regression case we first consider the log posterior, <span class="math display">\[
\log
p(\latentVector_{i, :} | \dataVector_{i, :}) = \log p(\dataVector_{i, :}|\mappingMatrix,
\latentVector_{i, :}, \noiseStd^2) + \log p(\latentVector_{i, :}) + \text{const}
\]</span> where the constant is not dependent on <span class="math inline">\(\latentVector\)</span>. As before we collect the quadratic terms in <span class="math inline">\(\latentVector_{i, :}\)</span> and we assemble them into a Gaussian density over <span class="math inline">\(\latentVector\)</span>. <span class="math display">\[
\log p(\latentVector_{i, :} | \dataVector_{i, :}) =
-\frac{1}{2\noiseStd^2} (\dataVector_{i, :} - \mappingMatrix\latentVector_{i,
:})^\top(\dataVector_{i, :} - \mappingMatrix\latentVector_{i, :}) - \frac{1}{2}
\latentVector_{i, :}^\top \latentVector_{i, :} + \text{const}
\]</span></p>
</section><section id="section-3" class="slide level2">
<h2></h2>
<p>Exercise 1</p>
<p>Multiply out the terms in the brackets. Then collect the quadratic term and the linear terms together. Show that the posterior has the form <span class="math display">\[
\latentVector_{i, :} | \mappingMatrix \sim \gaussianSamp{\meanVector_x}{\covarianceMatrix_x}
\]</span> where <span class="math display">\[
\covarianceMatrix_x = \left(\noiseStd^{-2}
\mappingMatrix^\top\mappingMatrix + \eye\right)^{-1}
\]</span> and <span class="math display">\[
\meanVector_x
= \covarianceMatrix_x \noiseStd^{-2}\mappingMatrix^\top \dataVector_{i, :} 
\]</span> Compare this to the posterior for the Bayesian linear regression from last week, do they have similar forms? What matches and what differs?</p>
</section><section id="python-implementation-of-the-posterior" class="slide level2">
<h2>Python Implementation of the Posterior</h2>
<p>Now let’s implement the system in code.</p>
</section><section id="section-4" class="slide level2">
<h2></h2>
<p>Exercise 2</p>
<p>Use the values for <span class="math inline">\(\mappingMatrix\)</span> and <span class="math inline">\(\noiseStd^2\)</span> you have computed, along with the data set <span class="math inline">\(\dataMatrix\)</span> to compute the posterior density over <span class="math inline">\(\latentMatrix\)</span>. Write a function of the form</p>
<p>python mu_x, C_x = posterior(Y, W, sigma2)} where <code>mu_x</code> and <code>C_x</code> are the posterior mean and posterior covariance for the given <span class="math inline">\(\dataMatrix\)</span>.</p>
<p>Don’t forget to subtract the mean of the data <code>Y</code> inside your function before computing the posterior: remember we assumed at the beginning of our analysis that the data had been centred (i.e. the mean was removed).}{20}</p>
</section></section>
<section><section id="question-4-answer-code" class="title-slide slide level1"><h1>Question 4 Answer Code</h1></section></section>
<section><section id="write-code-for-you-answer-to-this-question-in-this-box" class="title-slide slide level1"><h1>Write code for you answer to this question in this box</h1></section></section>
<section><section id="do-not-delete-these-comments-otherwise-you-will-get-zero-for-this-answer." class="title-slide slide level1"><h1>Do not delete these comments, otherwise you will get zero for this answer.</h1></section></section>
<section><section id="make-sure-your-code-has-run-and-the-answer-is-correct-before-submitting-your-notebook-for-marking." class="title-slide slide level1"><h1>Make sure your code has run and the answer is correct <em>before</em> submitting your notebook for marking.</h1></section></section>
<section><section id="numerically-stable-and-efficient-version" class="title-slide slide level1"><h1>Numerically Stable and Efficient Version</h1></section><section id="solution-for-mappingmatrix" class="slide level2">
<h2>Solution for <span class="math inline">\(\mappingMatrix\)</span></h2>
<p>Given the singular value decomposition of <span class="math inline">\(\dataMatrix\)</span> then we have <span class="math display">\[
\mappingMatrix =
\mathbf{U}\mathbf{L}\mathbf{R}^\top
\]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an arbitrary rotation matrix. This implies that the posterior is given by <span class="math display">\[
\covarianceMatrix_x =
\left[\noiseStd^{-2}\mathbf{R}\mathbf{L}^2\mathbf{R}^\top + \eye\right]^{-1}
\]</span> because <span class="math inline">\(\mathbf{U}^\top \mathbf{U} = \eye\)</span>. Since, by convention, we normally take <span class="math inline">\(\mathbf{R} = \eye\)</span> to ensure that the principal components are orthonormal we can write <span class="math display">\[
\covarianceMatrix_x = \left[\noiseStd^{-2}\mathbf{L}^2 +
\eye\right]^{-1}
\]</span> which implies that <span class="math inline">\(\covarianceMatrix_x\)</span> is actually diagonal with elements given by <span class="math display">\[
c_i = \frac{\noiseStd^2}{\noiseStd^2 + \ell^2_i}
\]</span> and allows us to write <span class="math display">\[
\meanVector_x = [\mathbf{L}^2 + \noiseStd^2
\eye]^{-1} \mathbf{L} \mathbf{U}^\top \dataVector_{i, :}
\]</span> <span class="math display">\[
\meanVector_x = \mathbf{D}\mathbf{U}^\top \dataVector_{i, :}
\]</span> where <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with diagonal elements given by <span class="math inline">\(d_{i} = \frac{\ell_i}{\noiseStd^2 + \ell_i^2}\)</span>.</p>
<p>import scipy as sp import numpy as np</p>
</section></section>
<section><section id="probabilistic-pca-algorithm-using-svd" class="title-slide slide level1"><h1>probabilistic PCA algorithm using SVD</h1></section><section id="examples" class="slide level2">
<h2>Examples</h2>
<p>For our first example we’ll consider some motion capture data of a man breaking into a run. <a href="http://en.wikipedia.org/wiki/Motion_capture">Motion capture data</a> involves capturing a 3-d point cloud to represent a character, often by an underlying skeleton. For this data set, from Ohio State University, we have 54 frame of motion capture, each frame containing 102 values, which are the 3-d locations of 34 different points from the subjects skeleton.</p>
<p>import pods</p>
<p>data = pods.datasets.osu_run1() Y = data[‘Y’]</p>
<p>Once the data is loaded in we can examine the first two principal components as follows,</p>
<p>q = 2 U, ell, sigma2 = ppca(Y, q) mu_x, C_x = posterior(Y, U, ell, sigma2)</p>
<p>Here because the data is a time course, we have connected points that are neighbouring in time. This highlights the form of the run, which involves 3 paces. This projects in our low dimensional space to 3 loops. We can examin how much residual variance there is in the system by looking at <code>sigma2</code>.</p>
<p>print(sigma2)</p>
<!-- SECTION Robot Navigation Example -->
</section><section id="robot-navigation-example" class="slide level2">
<h2>Robot Navigation Example</h2>
<p>In the next example we will load in data from a robot navigation problem. The data consists of wireless access point strengths as recorded by a robot performing a loop around the University of Washington’s Computer Science department in Seattle. The robot records all the wireless access points it can cache and stores their signal strength.</p>
<p>data = pods.datasets.robot_wireless() Y = data[‘Y’] Y.shape</p>
<p>There are 215 observations of 30 different access points. In this case the model is suggesting that the access point signal strength should be linearly dependent on the location in the map. In other words we are expecting the access point strength for the <span class="math inline">\(j\)</span>th access point at robot position <span class="math inline">\(x_{i, :}\)</span> to be represented by <span class="math inline">\(y_{i, j} = \weightVector_{j, :}^\top \latentVector_{i, :} + \epsilon_{i,j}\)</span>.</p>
<p>q = 2 U, ell, sigma2 = ppca(Y, q) mu_x, C_x = posterior(Y, U, ell, sigma2) import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(8,8)) ax.plot(mu_x[:, 0], mu_x[:, 1], ‘rx-’) ax.set_title(‘Latent Variable: Robot Inferred Locations’) fig, ax = plt.subplots(figsize=(8,8)) W = U*ell[None, :] ax.plot(W[:, 0], W[:, 1], ‘bo’) ax.set_title(‘Access Point Inferred Locations’)</p>
<p>U, ell, sigma2 = ppca(Y.T, q)</p>
<!-- SECTION Relationship to Matrix Factorization -->
</section><section id="relationship-to-matrix-factorization" class="slide level2">
<h2>Relationship to Matrix Factorization</h2>
<p>We can use the robot naviation example to realise that PCA (and factor analysis) are very reminiscient of the  that we used for introducing objective functions. In that system we used slightly different notation, <span class="math inline">\(\mathbf{u}_{i, :}\)</span> for <em>user</em> location in our metaphorical library and <span class="math inline">\(\mathbf{v}_{j, :}\)</span> for <em>item</em> location in our metaphorical library. To see how these systems are somewhat analagous, now let us think about the user as the robot and the items as the wifi access points. We can plot the relative location of both. This process is known as “SLAM”: simultaneous <em>localisation</em> and <em>mapping</em>. A latent variable model of the type we have developed is one way of performing SLAM. We have an estimate of the <em>landmarks</em> in the system (in this case WIFI access points) and we have an estimate of the robot position. These are analagous to the estimate of the user’s position and the estimate of the items positions in the library. In the matrix factorisation example users are informing us what items they are ‘close’ to by expressing their preferences, in the robot localization example the robot is informing us what access point it is close to by measuring signal strength.</p>
<p>From a personal perspective, I find this analogy quite comforting. I think it is very arguable that one of the mechanisms through which we (as humans) may have developed higher reasoning is through the need to navigate around our environment, identifying landmarks and associating them with our search for food. If such a system were to exist, the idea that it could be readily adapted to other domains such as categorising the nature of the different foodstuffs we were able to forage is intriguing.</p>
<p>From an algorithmic perspective, we also can now realise that matrix factorization and latent variable modelling are effectively the same thing. The only difference is the objective function and our probabilistic (or lack of probabilistic) treatment of the variables. But the prediction functoin for both systems, <span class="math display">\[
f_{i, j} =
\mathbf{u}_{i, :}^\top \mathbf{v}_{j, :} 
\]</span> for matrix factorization or <span class="math display">\[
f_{i, j} = \latentVector_{i, :}^\top \weightVector_{j, :} 
\]</span> for probabilistic PCA and factor analysis are the same.</p>
<!-- SECTION Other Interpretations of PCA: Separating Model and Algorithm -->
</section><section id="other-interpretations-of-pca-separating-model-and-algorithm" class="slide level2">
<h2>Other Interpretations of PCA: Separating Model and Algorithm</h2>
<p>Since Hotelling first introduced his perspective on factor analysis as PCA there has been somewhat of a conflation of the idea of the model underlying PCA (for which it was very clear that Hotelling was inspired by Factor Analysis) and the algorithm that is used to fit that model: the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors of an ellipsoid have been known since the middle of the 19th century as the principal axes of the elipsoid, and they arise through the following additional ideas: seeking the orthogonal directions of <em>maximum variance</em> in a dataset. Pearson in 1901 arrived at the same algorithm driven by a desire to seek a <em>symmetric regression</em> between two covariate/response variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. He is, therefore, often credited with the invention of principal component analysis, but to me this seems disengenous. His aim was very different from Hotellings, it was just happened that the optimal solution for his model was coincident with that of Hotelling. The approach is also known as the <a href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem">Karhunen Loeve Transform</a> in stochastic process theory and in classical multidimensional scaling the same operation can be shown to be minimising a particular objective function based on interpoint distances in the data and the latent space (see the section on Classical Multidimensional Scaling in <a href="http://store.elsevier.com/Multivariate-Analysis/Kanti-%20Mardia/isbn-9780124712522/">Mardia, Kent and Bibby</a>). One of my own contributions to machine learning was deriving yet another model whose linear variant was solved by finding the principal subspace of the covariance matrix (an approach I termed dual probabilistic PCA or probabilistic principal coordinate analysis). Finally, the approach is sometimes referred to simply as singular value decomposition (SVD). The singular value decomposition of a data set has the following form, <span class="math display">\[
\dataMatrix = \mathbf{V} \boldsymbol{\Lambda} \mathbf{U}^\top
\]</span> where <span class="math inline">\(\mathbf{V}\in\Re^{n\times n}\)</span> and <span class="math inline">\(\mathbf{U}^\in \Re^{p\times p}\)</span> are square orthogonal matrices and <span class="math inline">\(\mathbf{\Lambda}^{n \times p}\)</span> is zero apart from its first <span class="math inline">\(p\)</span> diagonal entries. Singularvalue decomposition gives a diagonalisation of the covariance matrix, because under the SVD we have <span class="math display">\[
\dataMatrix^\top\dataMatrix =
\mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top\mathbf{V} \boldsymbol{\Lambda}
\mathbf{U}^\top = \mathbf{U}\boldsymbol{\Lambda}^2 \mathbf{U}^\top
\]</span> where <span class="math inline">\(\boldsymbol{\Lambda}^2\)</span> is now the eigenvalues of the covariane matrix and <span class="math inline">\(\mathbf{U}\)</span> are the eigenvectors. So performing the SVD can simply be seen as another approach to determining the principal components.</p>
</section><section id="separating-model-and-algorithm" class="slide level2">
<h2>Separating Model and Algorithm</h2>
<p>I’ve given a fair amount of personal thought to this situation and my own opinion that this confusion about method arises because of a conflation of model and algorithm. The model of Hotelling, that which he termed principal component analysis, was really a variant of factor analysis, and it was unfortunate that he chose to rename it. However, the algorithm he derived was a very convenient way of optimising a (simplified) factor analysis, and it’s therefore become very popular. The algorithm is also the optimal solution for many other models of the data, even some which might seem initally to be unrelated (e.g. seeking directions of maximum variance). It is only through the mathematics of this linear system (which also contains some intersting symmetries) that all these ides become related. However, as soon as we choose to non-linearise the system (e.g. through basis functions) we find that each of the non-linear intepretations we can derive for the different models each leads to a very different algorithm (if such an algorithm is possible). For example <a href="http://web.stanford.edu/~hastie/Papers/Principal_Curves.pdf">principal curves</a> of Hastie and Stuezle attempt to non-linearise the maximum variance interpretation, <a href="http://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel PCA</a> of Schoelkopf, Smola and Mueller uses basis functions to form the eigenvalue problem in a nonlinear space, and my own work in this area <a href="http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf">non-linearises the dual probabilistic PCA</a>.</p>
<p>My conclusion is that when you are doing machine learning you should always have it clear in your mind what your <em>model</em> is and what your <em>algorithm</em> is. You can recognise your model because it normally contains a prediction function and an objective function. The algorithm on the other hand is the sequence of steps you implement on the computer to solve for the parameters of this model. For efficient implementation, we often modify our model to allow for faster algorithms, and this is a perfectly valid pragmatist’s approach, so conflation of model and algorithm is not always a bad thing. But for clarity of thinking and understanding it is necessary to maintain the separation and to maintain a handle on when and why we perform the conflation.</p>
<!-- SECTION PCA in Practice -->
</section><section id="pca-in-practice" class="slide level2">
<h2>PCA in Practice</h2>
<p>Principal component analysis is so effective in practice that there has almost developed a mini-industry in renaming the method itself (which is ironic, given its origin). In particular <a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent Semantic Indexing</a> in text processing is simply PCA on a particular representation of the term frequencies of the document. There is a particular fad to rename the eigenvectors after the nature of the data you are examining, perhaps initially triggered by <a href="http://www.face-rec.org/algorithms/PCA/jcn.pdf">Turk and Pentland’s</a> paper on eigenfaces, but also with <a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1_2007_8/kuhn-%20junqua-eigenvoice-icslp1998.pdf">eigenvoices</a> and <a href="http://www.biomedcentral.com/1752-0509/1/54">eigengenes</a>. This seems to be an instantiation of a wider, and hopefully subconcious, tendency in academia to attempt to differentiate one idea from the same idea in related fields in order to emphasise the novelty. The unfortunate result is somewhat of a confusing literature for relatively simple model. My recommendations would be as follows. If you have multivariate data, applying some form of principal component would seem to be a very good idea as a first step. Even if you intend to later perform classification or regression on your data, it can give you understanding of the structure of the underlying data and help you to develop your intuitions about the nature of your data. Intelligent plotting and interaction with your data is always a good think, and for high dimensional data that means that you need some way of visualisation, PCA is typically a good starting point.</p>
<!-- SECTION Marginal Likelihood -->
</section><section id="marginal-likelihood" class="slide level2">
<h2>Marginal Likelihood</h2>
<p>We have developed the posterior density over the latent variables given the data and the parameters, and due to symmetries in the underlying prediction function, it has a very similar form to its sister density, the posterior of the weights given the data from Bayesian regression. Two key differences are as follows. If we were to do a Bayesian multiple output regression we would find that the marginal likelihood of the data is independent across the features and correlated across the data, <span class="math display">\[
p(\dataMatrix|\latentMatrix)
= \prod_{j=1}^p \gaussianDist{\dataVector_{:, j}}{\zerosVector}{
\alpha\latentMatrix\latentMatrix^\top + \noiseStd^2 \eye}
\]</span> where <span class="math inline">\(\dataVector_{:, j}\)</span> is a column of the data matrix and the independence is across the <em>features</em>, in probabilistic PCA the marginal likelihood has the form, <span class="math display">\[
p(\dataMatrix|\mappingMatrix) = \prod_{i=1}^\numData \gaussianDist{\dataVector_{i,
:}}{\zerosVector}{\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye}
\]</span> where <span class="math inline">\(\dataVector_{i, :}\)</span> is a row of the data matrix <span class="math inline">\(\dataMatrix\)</span> and the independence is across the data points.</p>
<!-- SECTION Computation of the Log Likelihood -->
</section><section id="computation-of-the-log-likelihood" class="slide level2">
<h2>Computation of the Log Likelihood</h2>
<p>The quality of the model can be assessed using the log likelihood of this Gaussian form. <span class="math display">\[
\log p(\dataMatrix|\mappingMatrix) = -\frac{\numData}{2} \log \left|
\mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye \right| -\frac{1}{2}
\sum_{i=1}^\numData \dataVector_{i, :}^\top \left(\mappingMatrix\mappingMatrix^\top + \noiseStd^2
\eye\right)^{-1} \dataVector_{i, :} +\text{const}
\]</span> but this can be computed more rapidly by exploiting the low rank form of the covariance covariance, <span class="math inline">\(\covarianceMatrix = \mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye\)</span> and the fact that <span class="math inline">\(\mappingMatrix = \mathbf{U}\mathbf{L}\mathbf{R}^\top\)</span>. Specifically, we first use the decomposition of <span class="math inline">\(\mappingMatrix\)</span> to write: <span class="math display">\[
-\frac{\numData}{2} \log \left| \mappingMatrix\mappingMatrix^\top + \noiseStd^2 \eye \right|
= -\frac{\numData}{2} \sum_{i=1}^q \log (\ell_i^2 + \noiseStd^2) - \frac{n(p-q)}{2}\log
\noiseStd^2,
\]</span> where <span class="math inline">\(\ell_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\mathbf{L}\)</span>. Next, we use the <a href="http://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> which allows us to write the inverse as a quantity which contains another inverse in a smaller matrix: <span class="math display">\[
(\noiseStd^2 \eye + \mappingMatrix\mappingMatrix^\top)^{-1} =
\noiseStd^{-2}\eye-\noiseStd^{-4}\mappingMatrix{\underbrace{(\eye+\noiseStd^{-2}\mappingMatrix^\top\mappingMatrix)}_{\covarianceMatrix_x}}^{-1}\mappingMatrix^\top
\]</span> So, it turns out that the original inversion of the <span class="math inline">\(p \times p\)</span> matrix can be done by forming a quantity which contains the inversion of a <span class="math inline">\(q \times q\)</span> matrix which, moreover, turns out to be the quantity <span class="math inline">\(\covarianceMatrix_x\)</span> of the posterior.</p>
<p>Now, we put everything together to obtain: <span class="math display">\[
\log p(\dataMatrix|\mappingMatrix) = -\frac{\numData}{2} \sum_{i=1}^q
\log (\ell_i^2 + \noiseStd^2)
- \frac{n(p-q)}{2}\log \noiseStd^2 - \frac{1}{2} \trace{\dataMatrix^\top \left(
\noiseStd^{-2}\eye-\noiseStd^{-4}\mappingMatrix \covarianceMatrix_x
\mappingMatrix^\top \right) \dataMatrix} + \text{const},
\]</span> where we used the fact that a scalar sum can be written as <span class="math inline">\(\sum_{i=1}^\numData \dataVector_{i,:}^\top \kernelMatrix \dataVector_{i,:} = \trace{\dataMatrix^\top \kernelMatrix \dataMatrix}\)</span>, for any matrix <span class="math inline">\(\kernelMatrix\)</span> of appropriate dimensions. We now use the properties of the trace <span class="math inline">\(\trace{\mathbf{A}+\mathbf{B}}=\trace{\mathbf{A}}+\trace{\mathbf{B}}\)</span> and <span class="math inline">\(\trace{c \mathbf{A}} = c \trace{\mathbf{A}}\)</span>, where <span class="math inline">\(c\)</span> is a scalar and <span class="math inline">\(\mathbf{A},\mathbf{B}\)</span> matrices of compatible sizes. Therefore, the final log likelihood takes the form: <span class="math display">\[
\log p(\dataMatrix|\mappingMatrix) = -\frac{\numData}{2}
\sum_{i=1}^q \log (\ell_i^2 + \noiseStd^2) - \frac{\numData(p-q)}{2}\log \noiseStd^2 -
\frac{\noiseStd^{-2}}{2} \trace{\dataMatrix^\top \dataMatrix}
+\frac{\noiseStd^{-4}}{2} \trace{\mathbf{B}\covarianceMatrix_x\mathbf{B}^\top} +
\text{const}
\]</span> where we also defined <span class="math inline">\(\mathbf{B}=\dataMatrix^\top\mappingMatrix\)</span>. Finally, notice that <span class="math inline">\(\trace{\dataMatrix\dataMatrix^\top}=\trace{\dataMatrix^\top\dataMatrix}\)</span> can be computed faster as the sum of all the elements of <span class="math inline">\(\dataMatrix\circ\dataMatrix\)</span>, where <span class="math inline">\(\circ\)</span> denotes the element-wise (or <a href="http://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard</a> product.</p>
</section><section id="reconstruction-of-the-data" class="slide level2">
<h2>Reconstruction of the Data</h2>
<p>Given any posterior projection of a data point, we can replot the original data as a function of the input space.</p>
<p>We will now try to reconstruct the motion capture figure form some different places in the latent plot.</p>
</section><section id="section-5" class="slide level2">
<h2></h2>
<p>Exercise 1</p>
<p>Project the motion capture data onto its principal components, and then use the <em>mean posterior estimate</em> to reconstruct the data from the latent variables at the data points. Use two latent dimensions. What is the sum of squares error for the reconstruction?</p>
</section><section id="other-data-sets-to-explore" class="slide level2">
<h2>Other Data Sets to Explore</h2>
<p>Below there are a few other data sets from <code>pods</code> you might want to explore with PCA. Both of them have <span class="math inline">\(p\)</span>&gt;<span class="math inline">\(n\)</span> so you need to consider how to do the larger eigenvalue probleme efficiently without large demands on computer memory.</p>
<p>The data is actually quite high dimensional, and solving the eigenvalue problem in the high dimensional space can take some time. At this point we turn to a neat trick, you don’t have to solve the full eigenvalue problem in the <span class="math inline">\(\dataDim\times \dataDim\)</span> covariance, you can choose instead to solve the related eigenvalue problem in the <span class="math inline">\(\numData \times \numData\)</span> space, and in this case <span class="math inline">\(\numData=200\)</span> which is much smaller than <span class="math inline">\(\dataDim\)</span>.</p>
<p>The original eigenvalue problem has the form <span class="math display">\[
\dataMatrix^\top\dataMatrix \mathbf{U} = \mathbf{U}\boldsymbol{\Lambda}
\]</span> But if we premultiply by <span class="math inline">\(\dataMatrix\)</span> then we can solve, <span class="math display">\[
\dataMatrix\dataMatrix^\top\dataMatrix \mathbf{U} = \dataMatrix\mathbf{U}\boldsymbol{\Lambda}
\]</span> but it turns out that we can write <span class="math display">\[
\mathbf{U}^\prime = \dataMatrix \mathbf{U} \Lambda^{\frac{1}{2}}
\]</span> where <span class="math inline">\(\mathbf{U}^\prime\)</span> is an orthorormal matrix because <span class="math display">\[
\left.\mathbf{U}^\prime\right.^\top\mathbf{U}^\prime = \Lambda^{-\frac{1}{2}}\mathbf{U}\dataMatrix^\top\dataMatrix \mathbf{U} \Lambda^{-\frac{1}{2}}
\]</span> and since <span class="math inline">\(\mathbf{U}\)</span> diagonalises <span class="math inline">\(\dataMatrix^\top\dataMatrix\)</span>, <span class="math display">\[
\mathbf{U}\dataMatrix^\top\dataMatrix \mathbf{U} = \Lambda
\]</span> then <span class="math display">\[
\left.\mathbf{U}^\prime\right.^\top\mathbf{U}^\prime = \eye
\]</span></p>
</section><section id="olivetti-faces" class="slide level2">
<h2>Olivetti Faces</h2>
<p>You too can create your own eigenfaces. In this example we load in the ‘Olivetti Face’ data set, a small data set of 200 faces from the <a href="http://en.wikipedia.org/wiki/Olivetti_Research_Laboratory">Olivetti Research Laboratory</a>. Below we load in the data and display an image of the second face in the data set (i.e., indexed by 1).</p>
<p>Note that to display the face we had to reshape the appropriate row of the data matrix. This is because the images are turned into vectors by stacking columns of the image on top of each other to form a vector. The operation</p>
<p><code>im = np.reshape(Y[1, :].flatten(), (64, 64)).T}</code></p>
<p>recovers the original image into a matrix <code>im</code> by using the <code>np.reshape</code> function to return the vector to a matrix.</p>
</section><section id="visualizing-the-eigenvectors" class="slide level2">
<h2>Visualizing the Eigenvectors</h2>
<p>Each retained eigenvector is stored in the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{U}\)</span>. Each of these eigenvectors is associated with particular directions of variation in the original data. Principal component analysis implies that we can reconstruct any face by using a weighted sum of these eigenvectors where the weights for each face are given by the relevant vector of the latent variables, <span class="math inline">\(\latentVector_{i, :}\)</span> and the diagonal elements of the matrix <span class="math inline">\(\mathbf{L}\)</span>. We can visualize the eigenvectors <span class="math inline">\(\mathbf{U}\)</span> as images by performing the same reshape operation we used to recover the image associated with a data point above. Below we do this for the first nine eigenvectors of the Olivetti Faces data.</p>
</section><section id="reconstruction" class="slide level2">
<h2>Reconstruction</h2>
<p>We can now attempt to reconstruct a given training point from these eigenvectors. As we mentioned above, the reconstruction is dependent on the value of the latent variable and the weights from the matrix <span class="math inline">\(\mathbf{L}\)</span>. First let’s compute the value of the latent variables for the point we want to construct. Then we’ll use them to compute the weightings of each of the eigenvectors.</p>
<p>mu_x, C_x = posterior(Y, U, ell, sigma2) reconstruction_weights = mu_x[display_index, :]*ell print(reconstruction_weights)</p>
<p>This vector of reconstruction weights is applied to the ‘template images’ given by the eigenvectors to give us our reconstruction. Below we weight these templates and combine to form the reconstructed image, and show the comparison to the original image.</p>
<p>The quality of the reconstruction is a bit blurry, it can be improved by increasing the number of template images used (i.e. increasing the <em>latent dimensionality</em>).</p>
</section><section id="gene-expression" class="slide level2">
<h2>Gene Expression</h2>
<p>Each of the cells in your body stores your entire genetic code in your DNA, but at any one moment it is only ‘expressing’ a small portion of that code. Your cells are mainly constructed of protein, and these proteins are manufactured by first transcribing the DNA to RNA and then translating the RNA to protein. If the DNA is the cells hard drive, then one role of the RNA is to act like a cache of data that is being read from the hard drive at any one time. Gene expression arrays allow us to measure the quantity of different types of RNA in the cell, effectively analyzing what’s in the cache (although we have to destroy the cell or the tissue to access it). A gene expression experiment often consists of a time course or a series of experiments that characterise the gene expression of cells at any given time.</p>
<p>We will now load in one of the earliest gene expression data sets from a <a href="http://www.ncbi.nlm.nih.gov/pubmed/9843569">1998 paper by Spellman et al.</a>, it consists of gene expression measurements of over six thousand genes in a range of conditions for brewer’s yeast. The experiment was designed for understanding the cell cycle of the genes. The expectation is that there should be oscillating signals inside the cell.</p>
<p>First we extract the principale components of the gene expression.</p>
</section></section>
<section><section id="load-in-data-and-replace-missing-values-with-zero" class="title-slide slide level1"><h1>load in data and replace missing values with zero</h1></section></section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
