<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2015-10-06">
  <title>Objective Functions: A Simple Example with Matrix Factorisation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Objective Functions: A Simple Example with Matrix Factorisation</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2015-10-06</time></p>
  <p class="venue" style="text-align:center">University of Sheffield</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<p>%pip install –upgrade git+https://github.com/sods/ods</p>
<p>import urllib.request</p>
<p>urllib.request.urlretrieve(‘https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py’,‘mlai.py’)</p>
<p>urllib.request.urlretrieve(‘https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py’,‘teaching_plots.py’)</p>
<p>urllib.request.urlretrieve(‘https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py’,‘gp_tutorial.py’)</p>
</section>
<section id="objective-function" class="slide level2">
<h2>Objective Function</h2>
<ul>
<li>Last week we motivated the importance of probability.</li>
<li>This week we motivate the idea of the ‘objective function’.</li>
</ul>
</section>
<section id="introduction-to-classification" class="slide level2">
<h2>Introduction to Classification</h2>
</section>
<section id="classification" class="slide level2">
<h2>Classification</h2>
<ul>
<li><em>Wake word</em> classification (<a href="https://radio.unglobalpulse.net/uganda/">Global Pulse Project</a>).</li>
<li>Breakthrough in 2012 with ImageNet result of <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-">Alex Krizhevsky, Ilya Sutskever and Geoff Hinton</a></li>
</ul>
<ul>
<li>We are given a data set containing ‘inputs’, <span class="math inline">\(\inputMatrix\)</span> and ‘targets’, <span class="math inline">\(\dataVector\)</span>.</li>
<li>Each data point consists of an input vector <span class="math inline">\(\inputVector_i\)</span> and a class label, <span class="math inline">\(\dataScalar_i\)</span>.</li>
<li>For binary classification assume <span class="math inline">\(\dataScalar_i\)</span> should be either <span class="math inline">\(1\)</span> (yes) or <span class="math inline">\(-1\)</span> (no).</li>
<li>Input vector can be thought of as features.</li>
</ul>
</section>
<section id="discrete-probability" class="slide level2">
<h2>Discrete Probability</h2>
<ul>
<li>Algorithms based on <em>prediction</em> function and <em>objective</em> function.</li>
<li>For regression the <em>codomain</em> of the functions, <span class="math inline">\(f(\inputMatrix)\)</span> was the real numbers or sometimes real vectors.</li>
<li>In classification we are given an input vector, <span class="math inline">\(\inputVector\)</span>, and an associated label, <span class="math inline">\(\dataScalar\)</span> which either takes the value <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>.</li>
</ul>
</section>
<section id="classification-1" class="slide level2">
<h2>Classification</h2>
<ul>
<li>Inputs, <span class="math inline">\(\inputVector\)</span>, mapped to a label, <span class="math inline">\(\dataScalar\)</span>, through a function <span class="math inline">\(\mappingFunction(\cdot)\)</span> dependent on parameters, <span class="math inline">\(\weightVector\)</span>, <span class="math display">\[
\dataScalar = \mappingFunction(\inputVector; \weightVector).
\]</span></li>
<li><span class="math inline">\(\mappingFunction(\cdot)\)</span> is known as the <em>prediction function</em>. </li>
</ul>
</section>
<section id="classification-examples" class="slide level2">
<h2>Classification Examples</h2>
<ul>
<li>Classifiying hand written digits from binary images (automatic zip code reading)</li>
<li>Detecting faces in images (e.g. digital cameras).</li>
<li>Who a detected face belongs to (e.g. Facebook, DeepFace)</li>
<li>Classifying type of cancer given gene expression data.</li>
<li>Categorization of document types (different types of news article on the internet)</li>
</ul>
</section>
<section id="hyperplane" class="slide level2">
<h2>Hyperplane</h2>
<ul>
<li><p>predict the class label, <span class="math inline">\(\dataScalar_i\)</span>, given the features associated with that data point, <span class="math inline">\(\inputVector_i\)</span>, using the <em>prediction function</em>:</p>
<p><span class="math display">\[\mappingFunction(\inputScalar_i) = \text{sign}\left(\mappingVector^\top \inputVector_i + b\right)\]</span></p></li>
<li><p>Decision boundary for the classification is given by a <em>hyperplane</em>.</p></li>
<li><p>Vector <span class="math inline">\(\mappingVector\)</span> is the [normal vector](http://en.wikipedia.org/wiki/Normal_(geometry) to the hyperplane.</p></li>
<li><p>Hyperplane is described by the formula <span class="math inline">\(\mappingVector^\top \inputVector = -b\)</span></p></li>
</ul>
</section>
<section id="toy-data" class="slide level2">
<h2>Toy Data</h2>
<ul>
<li>Need to draw a decision boundary that separates red crosses from green circles.</li>
</ul>
</section>
<section id="mathematical-drawing-of-decision-boundary" class="slide level2">
<h2>Mathematical Drawing of Decision Boundary</h2>
<p><strong>Refresher</strong>: draw a hyper plane at decision boundary. - <em>Decision boundary</em>: plane where a point moves from being classified as -1 to +1. - We have</p>
<p><span class="math display">\[\text{sign}(\inputVector^\top \mappingVector) = \text{sign}(w_0 + w_1\inputScalar_{i,1} + w_2 \inputScalar_{i, 2})\]</span></p>
<p><span class="math inline">\(\inputScalar_{i, 1}\)</span> is first feature <span class="math inline">\(\inputScalar_{i, 2}\)</span> is second feature assume <span class="math inline">\(\inputScalar_{0,i}=1\)</span>.</p>
<ul>
<li><p>Set <span class="math inline">\(w_0 = b\)</span> we have</p>
<p><span class="math display">\[\text{sign}\left(w_1 \inputScalar_{i, 1} + w_2 \inputScalar_{i, 2} + b\right)\]</span></p></li>
</ul>
</section>
<section id="equation-of-plane" class="slide level2">
<h2>Equation of Plane</h2>
<p><span class="math display">\[\text{sign}\left(w_1 \inputScalar_{i, 1} + w_2 \inputScalar_{i, 2} + b\right)\]</span></p>
<ul>
<li><p>Equation of plane is</p>
<p><span class="math display">\[w_1 \inputScalar_{i, 1} + w_2 \inputScalar_{i, 2} + b = 0\]</span></p>
<p>or</p>
<p><span class="math display">\[w_1 \inputScalar_{i, 1} + w_2 \inputScalar_{i, 2} = -b\]</span></p></li>
<li><p>Next we will initialise the model and draw a decision boundary.</p></li>
</ul>
</section>
<section id="perceptron-algorithm-initialisation-maths" class="slide level2">
<h2>Perceptron Algorithm: Initialisation Maths</h2>
<ul>
<li><p>For a randomly chosen data point, <span class="math inline">\(i\)</span>, set <span class="math display">\[\mappingVector = \dataScalar_i \inputVector_i\]</span></p></li>
<li><p>If predicted label of the <span class="math inline">\(i\)</span>th point is <span class="math display">\[\text{sign}(\mappingVector^\top\inputVector_i)\]</span></p></li>
<li><p>Setting <span class="math inline">\(\mappingVector\)</span> to <span class="math inline">\(\dataScalar_i\inputVector_i\)</span> implies <span class="math display">\[\text{sign}(\mappingVector^\top\inputVector_i) = \text{sign}(\dataScalar_i\inputVector_i^\top \inputVector_i) = \dataScalar_i\]</span></p></li>
</ul>
<p>%load -s init_perceptron mlai.py</p>
</section>
<section id="computing-decision-boundary" class="slide level2">
<h2>Computing Decision Boundary</h2>
</section>
<section id="drawing-decision-boundary" class="slide level2">
<h2>Drawing Decision Boundary</h2>

</section>
<section id="switching-formulae" class="slide level2">
<h2>Switching Formulae</h2>

</section>
<section id="code-for-perceptron" class="slide level2">
<h2>Code for Perceptron</h2>
<p>%load -s update_perceptron mlai.py</p>
<p>import pods</p>
<script>
showDivs(0, 'perceptron');
</script>
<p><small></small> <input id="range-perceptron" type="range" min="0" max="14" value="0" onchange="setDivs('perceptron')" oninput="setDivs('perceptron')"> <button onclick="plusDivs(-1, 'perceptron')">❮</button> <button onclick="plusDivs(1, 'perceptron')">❯</button></p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron000.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron001.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron002.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron003.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron004.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron005.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron006.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron007.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron008.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron009.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron010.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron011.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron012.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron013.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="perceptron" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron014.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="objective-functions-and-regression" class="slide level2">
<h2>Objective Functions and Regression</h2>
<ul>
<li><p>Classification: map feature to class label.</p></li>
<li><p>Regression: map feature to real value our <em>prediction function</em> is</p>
<p><span class="math display">\[\mappingFunction(\inputScalar_i) = m\inputScalar_i + c\]</span></p></li>
<li><p>Need an <em>algorithm</em> to fit it.</p></li>
<li><p>Least squares: minimize an error.</p></li>
</ul>
<p><span class="math display">\[\errorFunction(m, c) = \sum_{i=1}^\numData (\dataScalar_i * \mappingFunction(\inputScalar_i))^2\]</span></p>
</section>
<section id="regression" class="slide level2">
<h2>Regression</h2>
<ul>
<li>Create an artifical data set.</li>
</ul>
<p>import numpy as np import matplotlib.pyplot as plt import mlai</p>
<p>x = np.random.normal(size=4)</p>
<p>We now need to decide on a <em>true</em> value for <span class="math inline">\(m\)</span> and a <em>true</em> value for <span class="math inline">\(c\)</span> to use for generating the data.</p>
<p>m_true = 1.4 c_true = -3.1</p>
<p>We can use these values to create our artificial data. The formula <span class="math display">\[\dataScalar_i = m\inputScalar_i + c\]</span> is translated to code as follows:</p>
<p>y = m_true*x+c_true</p>
</section>
<section id="plot-of-data" class="slide level2">
<h2>Plot of Data</h2>
<p>We can now plot the artifical data we’ve created.</p>
<div class="figure">
<div id="linear-regression-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/regression.svg" width="60%" style=" ">
</object>
</div>
</div>
<p>These points lie exactly on a straight line, that’s not very realistic, let’s corrupt them with a bit of Gaussian ‘noise’.</p>
</section>
<section id="noise-corrupted-plot" class="slide level2">
<h2>Noise Corrupted Plot</h2>
<p>noise = np.random.normal(scale=0.5, size=4) # standard deviation of the noise is 0.5 y = m_true*x + c_true + noise plt.plot(x, y, ‘r.’, markersize=10) plt.xlim([-3, 3]) mlai.write_figure(filename=“../slides/diagrams/ml/regression_noise.svg”, transparent=True)</p>
<div class="figure">
<div id="linear-regression-noise-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/regression_noise.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="contour-plot-of-error-function" class="slide level2">
<h2>Contour Plot of Error Function</h2>
<ul>
<li>Visualise the error function surface, create vectors of values.</li>
</ul>
</section>
<section>
<section id="create-an-array-of-linearly-separated-values-around-m_true" class="title-slide slide level1">
<h1>create an array of linearly separated values around m_true</h1>
<p>m_vals = np.linspace(m_true-3, m_true+3, 100) # create an array of linearly separated values ae c_vals = np.linspace(c_true-3, c_true+3, 100)</p>
<ul>
<li>create a grid of values to evaluate the error function in 2D.</li>
</ul>
<p>m_grid, c_grid = np.meshgrid(m_vals, c_vals)</p>
<ul>
<li>compute the error function at each combination of <span class="math inline">\(c\)</span> and <span class="math inline">\(m\)</span>.</li>
</ul>
<p>E_grid = np.zeros((100, 100)) for i in range(100): for j in range(100): E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()</p>
</section>
<section id="contour-plot-of-error" class="slide level2">
<h2>Contour Plot of Error</h2>
<ul>
<li>We can now make a contour plot.</li>
</ul>
<p>%load -s regression_contour teaching_plots.py</p>
<div class="figure">
<div id="regression-contour-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="steepest-descent" class="slide level2">
<h2>Steepest Descent</h2>
<ul>
<li>Minimize the sum of squares error function.</li>
<li>One way of doing that is gradient descent.</li>
<li>Initialize with a guess for <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span></li>
<li>update that guess by subtracting a portion of the gradient from the guess.</li>
<li>Like walking down a hill in the steepest direction of the hill to get to the bottom.</li>
</ul>
</section>
<section id="algorithm" class="slide level2">
<h2>Algorithm</h2>
<ul>
<li>We start with a guess for <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>.</li>
</ul>
<p>m_star = 0.0 c_star = -5.0</p>
</section>
<section id="offset-gradient" class="slide level2">
<h2>Offset Gradient</h2>
<ul>
<li><p>Now we need to compute the gradient of the error function, firstly with respect to <span class="math inline">\(c\)</span>,</p>
<p><span class="math display">\[\frac{\text{d}\errorFunction(m, c)}{\text{d} c} =
-2\sum_{i=1}^\numData (\dataScalar_i - m\inputScalar_i - c)\]</span></p></li>
<li><p>This is computed in python as follows</p></li>
</ul>
<p>c_grad = -2<em>(y-m_star</em>x - c_star).sum() print(“Gradient with respect to c is”, c_grad)</p>
</section>
<section id="deriving-the-gradient" class="slide level2">
<h2>Deriving the Gradient</h2>
<p>To see how the gradient was derived, first note that the <span class="math inline">\(c\)</span> appears in every term in the sum. So we are just differentiating <span class="math inline">\((\dataScalar_i - m\inputScalar_i - c)^2\)</span> for each term in the sum. The gradient of this term with respect to <span class="math inline">\(c\)</span> is simply the gradient of the outer quadratic, multiplied by the gradient with respect to <span class="math inline">\(c\)</span> of the part inside the quadratic. The gradient of a quadratic is two times the argument of the quadratic, and the gradient of the inside linear term is just minus one. This is true for all terms in the sum, so we are left with the sum in the gradient.</p>
</section>
<section id="slope-gradient" class="slide level2">
<h2>Slope Gradient</h2>
<p>The gradient with respect tom <span class="math inline">\(m\)</span> is similar, but now the gradient of the quadratic’s argument is <span class="math inline">\(-\inputScalar_i\)</span> so the gradient with respect to <span class="math inline">\(m\)</span> is</p>
<p><span class="math display">\[\frac{\text{d}\errorFunction(m, c)}{\text{d} m} = -2\sum_{i=1}^\numData \inputScalar_i(\dataScalar_i - m\inputScalar_i -
c)\]</span></p>
<p>which can be implemented in python (numpy) as</p>
<p>m_grad = -2<em>(x</em>(y-m_star*x - c_star)).sum() print(“Gradient with respect to m is”, m_grad)</p>
</section>
<section id="update-equations" class="slide level2">
<h2>Update Equations</h2>
<ul>
<li>Now we have gradients with respect to <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>.</li>
<li>Can update our inital guesses for <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> using the gradient.</li>
<li>We don’t want to just subtract the gradient from <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>,</li>
<li>We need to take a <em>small</em> step in the gradient direction.</li>
<li>Otherwise we might overshoot the minimum.</li>
<li>We want to follow the gradient to get to the minimum, the gradient changes all the time.</li>
</ul>
</section>
<section id="move-in-direction-of-gradient" class="slide level2">
<h2>Move in Direction of Gradient</h2>
<div class="figure">
<div id="regression-contour-step-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_step001.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="update-equations-1" class="slide level2">
<h2>Update Equations</h2>
<ul>
<li><p>The step size has already been introduced, it’s again known as the learning rate and is denoted by <span class="math inline">\(\learnRate\)</span>. <span class="math display">\[
c_\text{new}\leftarrow c_{\text{old}} - \learnRate \frac{\text{d}\errorFunction(m, c)}{\text{d}c}
\]</span></p></li>
<li><p>gives us an update for our estimate of <span class="math inline">\(c\)</span> (which in the code we’ve been calling <code>c_star</code> to represent a common way of writing a parameter estimate, <span class="math inline">\(c^*\)</span>) and <span class="math display">\[
m_\text{new} \leftarrow m_{\text{old}} - \learnRate \frac{\text{d}\errorFunction(m, c)}{\text{d}m}
\]</span></p></li>
<li><p>Giving us an update for <span class="math inline">\(m\)</span>.</p></li>
</ul>
</section>
<section id="update-code" class="slide level2">
<h2>Update Code</h2>
<ul>
<li>These updates can be coded as</li>
</ul>
<p>print(“Original m was”, m_star, “and original c was”, c_star) learn_rate = 0.01 c_star = c_star - learn_rate<em>c_grad m_star = m_star - learn_rate</em>m_grad print(“New m is”, m_star, “and new c is”, c_star)</p>
<!-- SECTION Iterating Updates -->
</section>
<section id="iterating-updates" class="slide level2">
<h2>Iterating Updates</h2>
<ul>
<li>Fit model by descending gradient.</li>
</ul>
</section>
<section id="gradient-descent-algorithm" class="slide level2">
<h2>Gradient Descent Algorithm</h2>
<p>num_plots = plot.regression_contour_fit(x, y, diagrams=‘../slides/diagrams/ml’)</p>
<script>
showDivs(1, 'regression_contour_fit');
</script>
<p><small></small> <input id="range-regression_contour_fit" type="range" min="1" max="28" value="1" onchange="setDivs('regression_contour_fit')" oninput="setDivs('regression_contour_fit')"> <button onclick="plusDivs(-1, 'regression_contour_fit')">❮</button> <button onclick="plusDivs(1, 'regression_contour_fit')">❯</button></p>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit000.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit001.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit002.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit003.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit004.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit005.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit006.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit007.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit008.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit009.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit010.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit011.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit012.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit013.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-29" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit014.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit015.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit016.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-32" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit017.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-33" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit018.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-34" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit019.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-35" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit020.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-36" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit021.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-37" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit022.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-38" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit023.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-39" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit024.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-40" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit025.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-41" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit026.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-42" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit027.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-43" class="slide level2">
<h2></h2>
<div class="regression_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit028.svg" width="60%" style=" ">
</object>
</div>
\notes{
<div class="figure">
<div id="regression-contour-fit-28-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/regression_contour_fit028.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="stochastic-gradient-descent" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<ul>
<li>If <span class="math inline">\(\numData\)</span> is small, gradient descent is fine.</li>
<li>But sometimes (e.g. on the internet <span class="math inline">\(\numData\)</span> could be a billion.</li>
<li>Stochastic gradient descent is more similar to perceptron.</li>
<li>Look at gradient of one data point at a time rather than summing across <em>all</em> data points)</li>
<li>This gives a stochastic estimate of gradient.</li>
</ul>
</section>
<section id="stochastic-gradient-descent-1" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<ul>
<li><p>The real gradient with respect to <span class="math inline">\(m\)</span> is given by</p>
<p><span class="math display">\[\frac{\text{d}\errorFunction(m, c)}{\text{d} m} = -2\sum_{i=1}^\numData \inputScalar_i(\dataScalar_i -
m\inputScalar_i - c)\]</span></p>
<p>but it has <span class="math inline">\(\numData\)</span> terms in the sum. Substituting in the gradient we can see that the full update is of the form</p>
<p><span class="math display">\[m_\text{new} \leftarrow
m_\text{old} + 2\learnRate \left[\inputScalar_1 (\dataScalar_1 - m_\text{old}\inputScalar_1 - c_\text{old}) + (\inputScalar_2 (\dataScalar_2 -   m_\text{old}\inputScalar_2 - c_\text{old}) + \dots + (\inputScalar_n (\dataScalar_n - m_\text{old}\inputScalar_n - c_\text{old})\right]\]</span></p>
<p>This could be split up into lots of individual updates <span class="math display">\[m_1 \leftarrow m_\text{old} + 2\learnRate \left[\inputScalar_1 (\dataScalar_1 - m_\text{old}\inputScalar_1 -
c_\text{old})\right]\]</span> <span class="math display">\[m_2 \leftarrow m_1 + 2\learnRate \left[\inputScalar_2 (\dataScalar_2 -
m_\text{old}\inputScalar_2 - c_\text{old})\right]\]</span> <span class="math display">\[m_3 \leftarrow m_2 + 2\learnRate
\left[\dots\right]\]</span> <span class="math display">\[m_n \leftarrow m_{n-1} + 2\learnRate \left[\inputScalar_n (\dataScalar_n -
m_\text{old}\inputScalar_n - c_\text{old})\right]\]</span></p></li>
</ul>
<p>which would lead to the same final update.</p>
</section>
<section id="updating-c-and-m" class="slide level2">
<h2>Updating <span class="math inline">\(c\)</span> and <span class="math inline">\(m\)</span></h2>
<ul>
<li>In the sum we don’t <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> we use for computing the gradient term at each update.</li>
<li>In stochastic gradient descent we <em>do</em> change them.</li>
<li>This means it’s not quite the same as steepest desceint.</li>
<li>But we can present each data point in a random order, like we did for the perceptron.</li>
<li>This makes the algorithm suitable for large scale web use (recently this domain is know as ‘Big Data’) and algorithms like this are widely used by Google, Microsoft, Amazon, Twitter and Facebook.</li>
</ul>
</section>
<section id="stochastic-gradient-descent-2" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<ul>
<li>Or more accurate, since the data is normally presented in a random order we just can write <span class="math display">\[
m_\text{new} = m_\text{old} + 2\learnRate\left[\inputScalar_i (\dataScalar_i - m_\text{old}\inputScalar_i - c_\text{old})\right]
\]</span></li>
</ul>
</section></section>
<section>
<section id="choose-a-random-point-for-the-update" class="title-slide slide level1">
<h1>choose a random point for the update</h1>
<p>i = np.random.randint(x.shape[0]-1) # update m m_star = m_star + 2<em>learn_rate</em>(x[i]<em>(y[i]-m_star</em>x[i] - c_star)) # update c c_star = c_star + 2<em>learn_rate</em>(y[i]-m_star*x[i] - c_star)</p>
</section>
<section id="sgd-for-linear-regression" class="slide level2">
<h2>SGD for Linear Regression</h2>
<p>Putting it all together in an algorithm, we can do stochastic gradient descent for our regression data.</p>
<script>
showDivs(0, 'regression_sgd_contour_fit');
</script>
<p><small></small> <input id="range-regression_sgd_contour_fit" type="range" min="0" max="58" value="0" onchange="setDivs('regression_sgd_contour_fit')" oninput="setDivs('regression_sgd_contour_fit')"> <button onclick="plusDivs(-1, 'regression_sgd_contour_fit')">❮</button> <button onclick="plusDivs(1, 'regression_sgd_contour_fit')">❯</button></p>
</section>
<section id="section-44" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit000.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-45" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit001.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-46" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit002.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-47" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit003.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-48" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit004.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-49" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit005.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-50" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit006.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-51" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit007.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-52" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit008.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-53" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit009.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-54" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit010.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-55" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit011.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-56" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit012.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-57" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit013.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-58" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit014.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-59" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit015.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-60" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit016.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-61" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit017.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-62" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit018.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-63" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit019.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-64" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit020.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-65" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit021.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-66" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit022.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-67" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit023.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-68" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit024.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-69" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit025.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-70" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit026.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-71" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit027.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-72" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit028.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-73" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit029.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-74" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit030.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-75" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit031.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-76" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit032.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-77" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit033.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-78" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit034.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-79" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit035.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-80" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit036.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-81" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit037.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-82" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit038.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-83" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit039.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-84" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit040.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-85" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit041.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-86" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit042.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-87" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit043.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-88" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit044.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-89" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit045.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-90" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit046.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-91" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit047.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-92" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit048.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-93" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit049.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-94" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit050.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-95" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit051.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-96" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit052.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-97" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit053.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-98" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit054.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-99" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit055.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-100" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit056.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-101" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit057.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="section-102" class="slide level2">
<h2></h2>
<div class="regression_sgd_contour_fit" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/regression_sgd_contour_fit058.svg" width="60%" style=" ">
</object>
</div>
</section>
<section id="reflection-on-linear-regression-and-supervised-learning" class="slide level2">
<h2>Reflection on Linear Regression and Supervised Learning</h2>
<p>Think about:</p>
<ol type="1">
<li><p>What effect does the learning rate have in the optimization? What’s the effect of making it too small, what’s the effect of making it too big? Do you get the same result for both stochastic and steepest gradient descent?</p></li>
<li><p>The stochastic gradient descent doesn’t help very much for such a small data set. It’s real advantage comes when there are many, you’ll see this in the lab.</p></li>
</ol>
</section>
<section id="lab-class" class="slide level2">
<h2>Lab Class</h2>
<ul>
<li>You will take the ideas you have learnt.</li>
<li>You will apply them in the domain of <em>matrix factorisation</em>.</li>
<li>Matrix factorization presents a different error function.</li>
</ul>
<p>for loss functions.</p>
</section>
<section id="further-reading" class="slide level2 scrollable">
<h2 class="scrollable">Further Reading</h2>
<ul>
<li>Section 1.1.3 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></li>
</ul>
<p>import pods</p>
<!-- SECTION Movie Body Count Example -->
</section>
<section id="movie-body-count-example" class="slide level2">
<h2>Movie Body Count Example</h2>
<p>data = pods.datasets.movie_body_count()[‘Y’] data.head()</p>
<p>data.describe()</p>
<p>data.describe?</p>
<p>print(data[‘Year’]) #print(data[‘Body_Count’])</p>
</section></section>
<section>
<section id="this-ensures-the-plot-appears-in-the-web-browser" class="title-slide slide level1">
<h1>this ensures the plot appears in the web browser</h1>
<p>%matplotlib inline import matplotlib.pyplot as plt # this imports the plotting library in python</p>
<p>plt.plot(data[‘Year’], data[‘Body_Count’], ‘rx’)</p>
<p>plt.plot?</p>
<p>data[data[‘Body_Count’]&gt;200]</p>
<p>data[data[‘Body_Count’]&gt;200].sort_values(by=‘Body_Count’, ascending=False)</p>
<p>data[‘Body_Count’].hist(bins=20) # histogram the data with 20 bins. plt.title(‘Histogram of Film Kill Count’)</p>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise 1</h2>
<p>Read on the internet about the following python libraries: <code>numpy</code>, <code>matplotlib</code>, <code>scipy</code> and <code>pandas</code>. What functionality does each provide python. What is the <code>pylab</code> library and how does it relate to the other libraries?</p>
<p>plt.plot(data[‘Year’], data[‘Body_Count’], ‘rx’) ax = plt.gca() # obtain a handle to the current axis ax.set_yscale(‘log’) # use a logarithmic death scale # give the plot some titles and labels plt.title(‘Film Deaths against Year’) plt.ylabel(‘deaths’) plt.xlabel(‘year’)</p>
</section>
<section id="exercise-1-1" class="slide level2">
<h2>Exercise 1</h2>
<p>Data ethics. If you find data available on the internet, can you simply use it without consequence? If you are given data by a fellow researcher can you publish that data on line?</p>
<!-- SECTION Recommender Systems -->
</section>
<section id="recommender-systems" class="slide level2">
<h2>Recommender Systems</h2>
<div class="figure">
<div id="carthaginian-empire-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/mlai/carthaginian-empire.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="inner-products-for-representing-similarity" class="slide level2">
<h2>Inner Products for Representing Similarity</h2>

</section>
<section id="the-library-on-an-infinite-plane" class="slide level2">
<h2>The Library on an Infinite Plane</h2>
<p><span class="math display">\[
\mathbf{v}_j = \begin{bmatrix} v_{j,1} \\ v_{j,2}\end{bmatrix},
\]</span>  <span class="math display">\[
\mathbf{u}_i = \begin{bmatrix} u_{i,1} \\ u_{i,2}\end{bmatrix}.
\]</span> </p>
</section>
<section id="obtaining-the-data" class="slide level2">
<h2>Obtaining the Data</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> pods</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4"></a>user_data <span class="op">=</span> pd.DataFrame(index<span class="op">=</span>movies.index, columns<span class="op">=</span>[<span class="st">&#39;title&#39;</span>, <span class="st">&#39;year&#39;</span>, <span class="st">&#39;rating&#39;</span>, <span class="st">&#39;prediction&#39;</span>])</span>
<span id="cb1-5"><a href="#cb1-5"></a>user_data[<span class="st">&#39;title&#39;</span>]<span class="op">=</span>movies.Film</span>
<span id="cb1-6"><a href="#cb1-6"></a>user_data[<span class="st">&#39;year&#39;</span>]<span class="op">=</span>movies.Year</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>accumulator<span class="op">=</span>pods.lab.distributor(spreadsheet_title<span class="op">=</span><span class="st">&#39;COM4509/6509 Movie Ratings:&#39;</span>, user_sep<span class="op">=</span><span class="st">&#39;</span><span class="ch">\t</span><span class="st">&#39;</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># function to apply to data before giving it to user </span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co"># Select 50 movies at random and order them according to year.</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>max_movies <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>function <span class="op">=</span> <span class="kw">lambda</span> x: x.loc[np.random.permutation(x.index)[:max_movies]].sort(columns<span class="op">=</span><span class="st">&#39;year&#39;</span>)</span>
<span id="cb1-13"><a href="#cb1-13"></a>accumulator.write(data_frame<span class="op">=</span>user_data, comment<span class="op">=</span><span class="st">&#39;Film Ratings&#39;</span>, </span>
<span id="cb1-14"><a href="#cb1-14"></a>                  function<span class="op">=</span>function)</span>
<span id="cb1-15"><a href="#cb1-15"></a>accumulator.write_comment(<span class="st">&#39;Rate Movie Here (score 1-5)&#39;</span>, row<span class="op">=</span><span class="dv">1</span>, column<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb1-16"><a href="#cb1-16"></a>accumulator.share(share_type<span class="op">=</span><span class="st">&#39;writer&#39;</span>, send_notifications<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>Once you have placed your ratings we can download the data from your spreadsheets to a central file where the ratings of the whole class can be stored. We will build an algorithm on these ratings and use them to make predictions for the rest of the class. Firstly, here’s the code for reading the ratings from each of the spreadsheets.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> os</span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a>accumulator <span class="op">=</span> pods.lab.distributor(user_sep<span class="op">=</span><span class="st">&#39;</span><span class="ch">\t</span><span class="st">&#39;</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a>data <span class="op">=</span> accumulator.read(usecols<span class="op">=</span>[<span class="st">&#39;rating&#39;</span>], dtype<span class="op">=</span>{<span class="st">&#39;index&#39;</span>:<span class="bu">int</span>, <span class="st">&#39;rating&#39;</span>:np.float64}, header<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="cf">for</span> user <span class="kw">in</span> data:</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="cf">if</span> data[user].rating.count()<span class="op">&gt;</span><span class="dv">0</span>: <span class="co"># only add user if they rated something</span></span>
<span id="cb2-10"><a href="#cb2-10"></a>        <span class="co"># add a new field to movies with that user&#39;s ratings.</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>        movies[user] <span class="op">=</span> data[user][<span class="st">&#39;rating&#39;</span>]</span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co"># Store the csv on disk where it will be shared through dropbox.</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>movies.to_csv(os.path.join(pods.lab.class_dir,<span class="st">&#39;movies.csv&#39;</span>), index_label<span class="op">=</span><span class="st">&#39;index&#39;</span>)</span></code></pre></div>
<p>Now we will convert our data structure into a form that is appropriate for processing. We will convert the <code>movies</code> object into a data base which contains the movie, the user and the score using the following command.</p>
<p>import pandas as pd import os</p>
</section></section>
<section>
<section id="uncomment-the-line-below-if-you-are-doing-this-task-by-self-study." class="title-slide slide level1">
<h1>uncomment the line below if you are doing this task by self study.</h1>
<p>pods.util.download_url(‘https://dl.dropboxusercontent.com/u/4347554/mlai_movies.csv’, store_directory = ‘class_movie’, save_name=‘movies.csv’) #pods.util.download_url(‘https://www.dropbox.com/s/s6gqvp9b383b59y/movies.csv?dl=0&amp;raw=1’, store_directory = ‘class_movie’, save_name=‘movies.csv’) movies = pd.read_csv(os.path.join(‘class_movie’, ‘movies.csv’),encoding=‘latin-1’).set_index(‘index’)</p>
</section>
<section id="exercise-1-2" class="slide level2">
<h2>Exercise 1</h2>
<p>The movies data is now in a data frame which contains one column for each user rating the movie. There are some entries that contain ‘NaN’. What does the ‘NaN’ mean in this context?</p>
</section>
<section id="processing-the-data" class="slide level2">
<h2>Processing the Data</h2>
<p>We will now prepare the data set for processing. To do this we are going to conver the data into a new format using the <code>melt</code> command.</p>
<p>user_names = list(set(movies.columns)-set(movies.columns[:9])) Y = pd.melt(movies.reset_index(), id_vars=[‘Film’, ‘index’], var_name=‘user’, value_name=‘rating’, value_vars=user_names) Y = Y.dropna(axis=0)</p>
</section>
<section id="what-is-a-pivot-table-what-does-the-pandas-command" class="slide level2">
<h2>What is a pivot table? What does the <code>pandas</code> command</h2>
<p><code>pd.melt</code> do?</p>
</section>
<section id="measuring-similarity" class="slide level2">
<h2>Measuring Similarity</h2>
<p><span class="math display">\[
\mathbf{a}^\top\mathbf{b} = \sum_{i} a_i b_i
\]</span>  <span class="math display">\[
\mathbf{a}^\top\mathbf{b} = |\mathbf{a}||\mathbf{b}| \cos(\theta)
\]</span> </p>
</section>
<section id="objective-function-1" class="slide level2">
<h2>Objective Function</h2>
<p>The error function (or objective function, or cost function) we will choose is known as ‘sum of squares’, we will aim to minimize the sum of squared squared error between the inner product of <span class="math inline">\(\mathbf{u}_i\)</span> and <span class="math inline">\(\mathbf{v}_i\)</span> and the observed score for the user/item pairing, given by <span class="math inline">\(y_{i, j}\)</span>.</p>
<p>The total objective function can be written as <span class="math display">\[
E(\mathbf{U}, \mathbf{V}) = \sum_{i,j} s_{i,j} (y_{i,j} - \mathbf{u}_i^\top \mathbf{v}_j)^2
\]</span> where <span class="math inline">\(s_{i,j}\)</span> is an <em>indicator</em> variable that is 1 if user <span class="math inline">\(i\)</span> has rated item <span class="math inline">\(j\)</span> and is zero otherwise. Here <span class="math inline">\(\mathbf{U}\)</span> is the matrix made up of all the vectors <span class="math inline">\(\mathbf{u}\)</span>, <span class="math display">\[
\mathbf{U} = \begin{bmatrix} \mathbf{u}_1 \dots \mathbf{u}_n\end{bmatrix}^\top
\]</span> where we note that <span class="math inline">\(i\)</span>th <em>row</em> of <span class="math inline">\(\mathbf{U}\)</span> contains the vector associated with the <span class="math inline">\(i\)</span>th user and <span class="math inline">\(n\)</span> is the total number of users. This form of matrix is known as a <em>design matrix</em>. Similarly, we define the matrix <span class="math display">\[
\mathbf{V} = \begin{bmatrix} \mathbf{v}_1 \dots \mathbf{v}_m\end{bmatrix}^\top
\]</span> where again the <span class="math inline">\(j\)</span>th row of <span class="math inline">\(\mathbf{V}\)</span> contains the vector associated with the <span class="math inline">\(j\)</span>th item and <span class="math inline">\(m\)</span> is the total number of items in the data set.</p>
</section>
<section id="objective-optimization" class="slide level2">
<h2>Objective Optimization</h2>
<p>The idea is to mimimize this objective. A standard, simple, technique for minimizing an objective is <em>gradient descent</em> or <em>steepest descent</em>. In gradient descent we simply choose to update each parameter in the model by subtracting a multiple of the objective function’s gradient with respect to the parameters. So for a parameter <span class="math inline">\(u_{i,j}\)</span> from the matrix <span class="math inline">\(\mathbf{U}\)</span> we would have an update as follows: <span class="math display">\[ 
u_{k,\ell} \leftarrow u_{k,\ell} - \eta \frac{\text{d}
E(\mathbf{U}, \mathbf{V})}{\text{d}u_{k,\ell}} 
\]</span> where <span class="math inline">\(\eta\)</span> (which is pronounced <em>eta</em> in English) is a Greek letter representing the <em>learning rate</em>.</p>
<p>We can compute the gradient of the objective function with respect to <span class="math inline">\(u_{k,\ell}\)</span> as <span class="math display">\[
\frac{\text{d}E(\mathbf{U}, \mathbf{V})}{\text{d}u_{k,\ell}} = -2
\sum_j s_{k,j}v_{j,\ell}(y_{k, j} - \mathbf{u}_k^\top\mathbf{v}_{j}).
\]</span> Similarly each parameter <span class="math inline">\(v_{i,j}\)</span> needs to be updated according to its gradient.</p>
</section>
<section id="exercise-1-3" class="slide level2">
<h2>Exercise 1</h2>
<p>What is the gradient of the objective function with respect to <span class="math inline">\(v_{k, \ell}\)</span>? Write your answer in the box below, and explain which differentiation techniques you used to get there. You will be expected to justify your answer in class by oral questioning. Create a function for computing this gradient that is used in the algorithm below.</p>
</section>
<section id="steepest-descent-algorithm" class="slide level2">
<h2>Steepest Descent Algorithm</h2>
<p>In the steepest descent algorithm we aim to minimize the objective function by subtacting the gradient of the objective function from the parameters.</p>
</section>
<section id="initialisation" class="slide level2">
<h2>Initialisation</h2>
<p>To start with though, we need initial values for the matrix <span class="math inline">\(\mathbf{U}\)</span> and the matrix <span class="math inline">\(\mathbf{V}\)</span>. Let’s create them as <code>pandas</code> data frames and initialise them randomly with small values.</p>
<p>import numpy as np</p>
<p>q = 2 # the dimension of our map of the ‘library’ learn_rate = 0.01 U = pd.DataFrame(np.random.normal(size=(len(user_names), q))<em>0.001, index=user_names) V = pd.DataFrame(np.random.normal(size=(len(movies.index), q))</em>0.001, index=movies.index)</p>
<p>We also will subtract the mean from the rating before we try and predict them predictions. Have a think about why this might be a good idea (hint, what will the gradients be if we don’t subtract the mean).</p>
<p>Y[‘rating’] -= Y[‘rating’].mean()</p>
<p>Now that we have the initial values set, we can start the optimization. First we define a function for the gradient of the objective and the objective function itself.</p>
<p>def objective_gradient(Y, U, V): gU = pd.DataFrame(np.zeros((U.shape)), index=U.index) gV = pd.DataFrame(np.zeros((V.shape)), index=V.index) obj = 0. for ind, series in Y.iterrows(): film = series[‘index’] user = series[‘user’] rating = series[‘rating’] prediction = np.dot(U.loc[user], V.loc[film]) # vTu diff = prediction - rating # vTu - y obj += diff<em>diff gU.loc[user] += 2</em>diff<em>V.loc[film] gV.loc[film] += 2</em>diff*U.loc[user] return obj, gU, gV</p>
<p>Now we can write our simple optimisation route. This allows us to observe the objective function as the optimization proceeds.</p>
<p>import sys iterations = 100 for i in range(iterations): obj, gU, gV = objective_gradient(Y, U, V) print(“Iteration”, i, “Objective function:”, obj) U -= learn_rate<em>gU V -= learn_rate</em>gV</p>
</section>
<section id="exercise-2" class="slide level2">
<h2>Exercise 2</h2>
<p>What happens as you increase the number of iterations? What happens if you increase the learning rate?</p>
</section>
<section id="stochastic-gradient-descent-or-robbins-monroe-algorithm" class="slide level2">
<h2>Stochastic Gradient Descent or Robbins Monroe Algorithm</h2>
<p>Stochastic gradient descent <span class="citation" data-cites="Robbins:stoch51">(Robbins and Monro, 1951)</span> involves updating separating each gradient update according to each separate observation, rather than summing over them all. It is an approximate optimization method, but it has proven convergence under certain conditions and can be much faster in practice. It is used widely by internet companies for doing machine learning in practice. For example, Facebook’s ad ranking algorithm uses stochastic gradient descent.</p>
</section>
<section id="exercise-3" class="slide level2">
<h2>Exercise 3</h2>
<p>Create a stochastic gradient descent version of the algorithm. Monitor the objective function after every 1000 updates to ensure that it is decreasing. When you have finished, plot the movie map and the user map in two dimensions. Label the plots with the name of the movie or user.</p>
</section>
<section id="making-predictions" class="slide level2">
<h2>Making Predictions</h2>
<p>Predictions can be made from the model of the appropriate rating for a given user, <span class="math inline">\(i\)</span>, for a given film, <span class="math inline">\(j\)</span>, by simply taking the inner product between their vectors <span class="math inline">\(\mathbf{u}_i\)</span> and <span class="math inline">\(\mathbf{v}_j\)</span>.</p>
</section>
<section id="is-our-map-enough-are-our-data-enough" class="slide level2">
<h2>Is Our Map Enough? Are Our Data Enough?</h2>
<p>Is two dimensions really enough to capture the complexity of humans and their artforms? Perhaps we need even more dimensions to capture that complexity. Extending our books analogy further, consider how we should place books that have a historical timeframe as well as some geographical location. Do we really want books from the 2nd World War to sit alongside books from the Roman Empire? Books on the American invasion of Sicily in 1943 are perhaps less related to books about Carthage than those that study the Jewish Revolt from 66-70 (in the Roman Province of Judaea). So books that relate to subjects which are closer in time should be stored together. However, a student of rebellion against empire may also be interested in the relationship between the Jewish Revolt of 66-70 and the Indian Rebellion of 1857, nearly 1800 years later. Whilst the technologies are different, the psychology of the people is shared: a rebellious nation angainst their imperial masters, triggered by misrule with a religious and cultural background. To capture such complexities we would need further dimensions in our latent representation. But are further dimensions justified by the amount of data we have? Can we really understand the facets of a film that only has at most three or four ratings?</p>
</section>
<section id="going-further" class="slide level2">
<h2>Going Further</h2>
<p>If you want to take this model further then you’ll need more data. One possible source of data is the <a href="http://grouplens.org/datasets/movielens/"><code>movielens</code> data set</a>. They have data sets containing up to ten million movie ratings. The few ratings we were able to collect in the class are not enough to capture the rich structure underlying these films. Imagine if we assume that the ratings are uniformly distributed between 1 and 5. If you know something about information theory then you could use that to work out the maximum number of <em>bits</em> of information we could gain per rating.</p>
<p>Now we’ll download the movielens 100k data and see if we can extract information about these movies.</p>
<p>import pods</p>
<p>d = pods.datasets.movielens100k() Y=d[‘Y’]</p>
</section>
<section id="exercise-4" class="slide level2">
<h2>Exercise 4</h2>
<p>Use stochastic gradient descent to make a movie map for the movielens data. Plot the map of the movies when you are finished.</p>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Robbins:stoch51">
<p>Robbins, H., Monro, S., 1951. A stochastic approximation method. Annals of Mathematical Statistics 22, 400–407.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.</p>
</div>
</div>
</section></section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
