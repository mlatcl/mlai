<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>The Data Science Process</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">The Data Science Process</h1>
</section>

<section class="slide level2">

<p>\slides{\div{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/new-flow-of-information001}{70%}{}{height:50%}}{new-flow-of-information}{max-width:100vw; max-height:100vh}}</p>
<p>\slides{\div{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/new-flow-of-information002}{70%}{}{height:50%}}{new-flow-of-information}{max-width:100vw; max-height:100vh}}</p>
<p>\notes{\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/new-flow-of-information002}{50%}}{The trinity of human, data and computer, and highlights the modern phenomenon. The communication channel between computer and data now has an extremely high bandwidth. The channel between human and computer and the channel between data and human is narrow. New direction of information flow, information is reaching us mediated by the computer.}{trinity-human-data-computer}}</p>
\table{
<table>
<tr>
<td>
</td>
<td align="center">
\includejpg{https://mlatcl.github.com/mlai/./slides/diagrams//IBM_Blue_Gene_P_supercomputer}{50%}
</td>
<td align="center">
<p>See </p>
<p>\figure{\includejpg{https://mlatcl.github.com/mlai/./slides/diagrams//Lotus_49-2}{70%}}{The Lotus 49, view from the rear. The Lotus 49 was one of the last Formula One cars before the introduction of aerodynamic aids.}{lotus-49}</p>
<p>\figure{\includejpg{https://mlatcl.github.com/mlai/./slides/diagrams//640px-Marcel_Renault_1903}{70%}}{Marcel Renault races a Renault 40 cv during the Paris-Madrid race, an early Grand Prix, in 1903. Marcel died later in the race after missing a warning flag for a sharp corner at Couhé Vérac, likely due to dust reducing visibility.}{marcel-renault}</p>
<p>\figure{\includejpg{https://mlatcl.github.com/mlai/./slides/diagrams//Caleb_McDuff_WIX_Silence_Racing_livery}{70%}}{Caleb McDuff driving for WIX Silence Racing.}{caleb-mcduff}</p>
<!--

\editme


\subsection{Societal Effects}
\slides{
* This phenomenon has already revolutionised biology.
    * Large scale data acquisition and distribution.
    * Transcriptomics, genomics, epigenomics, 'rich phenomics'.
* Great *promise* for personalized health.
}

\notes{We have already seen the effects of this changed dynamic in biology and computational biology. Improved sensorics have led to the new domains of transcriptomics, epigenomics, and 'rich phenomics' as well as considerably augmenting our capabilities in genomics.} 

\notes{Biologists have had to become data-savvy, they require a rich understanding of the available data resources and need to assimilate existing data sets in their hypothesis generation as well as their experimental design. Modern biology has become a far more quantitative science, but the quantitativeness has required new methods developed in the domains of *computational biology* and *bioinformatics*.}

\notes{There is also great promise for personalized health, but in health the wide data-sharing that has underpinned success in the computational biology community is much harder to cary out.} 


\newslide{Societal Effects}
\slides{
* Automated decision making within the computer based only on the data.
* Subjective biases need to be better understood.
* Particularly important where treatments are being prescribed.
    * Interventions could be far more subtle.
}

\notes{We can expect to see these phenomena reflected in wider society. Particularly as we make use of more automated decision making based only on data. This is leading to a requirement to better understand our own subjective biases to ensure that the human to computer interface allows domain experts to assimilate data driven conclusions in a well calibrated manner. This is particularly important where medical treatments are being prescribed. It also offers potential for different kinds of medical intervention. More subtle interventions are possible when the digital environment is able to respond to users in an bespoke manner. This has particular implications for treatment of mental health conditions.}

\newslide{Societal Effects}
\slides{
* Shift in dynamic:
    * from direct human-data to indirect human-computer-data
    * modern data analysis is mediated by the machine
* This change of dynamics gives us the modern and emerging domain of data science
}

\notes{The main phenomenon we see across the board is the shift in dynamic from the direct pathway between human and data, as traditionally mediated by classical statistcs, to a new flow of information via the computer. This change of dynamics gives us the modern and emerging domain of *data science*, where the interactions between human and data are mediated by the machine.}


-->
<!--

\editme

\subsection{Decomposition}
\slides{
* ML is not Magical Pixie Dust.
* It cannot be sprinkled thoughtlessly.
* We cannot simply *automate all decisions through data*
}
\newslide{Decomposition}
\slides{
We are constrained by:

1. Our *data*.
2. The *models*.
}

\notes{Machine learning is not magical pixie dust, we cannot simply automate all decisions through data. We are constrained by our data (see below) and the models we use.[^tribal]  Machine learning models are relatively simple function mappings that include characteristics such as smoothness. With some famous exceptions, e.g. speech and image data, inputs are constrained in the form of vectors and the model consists of a mathematically well-behaved function. This means that some careful thought has to be put in to the right sub-process to automate with machine learning. This is the challenge of *decomposition* of the machine learning system.

[^tribal]: We can also become constrained by our tribal thinking, just as each of the other groups can.
}
\newslide{Decomposition of Task}
\slides{
* Separation of Concerns
* Careful thought needs to be put into sub-processes of task.
* Any repetitive task is a candidate for automation.
}
\notes{Any repetitive task is a candidate for automation, but many of the repetitive tasks we perform as humans are more complex than any individual algorithm can replace. The selection of which task to automate becomes critical and has downstream effects on our overall system design.

Our approach to complex system design is separation of concerns, decompose the large scale system into smaller parts, each of which can be managed by a separate team.}

\subsubsection{Pigeonholing}

\figure{\includejpg{https://mlatcl.github.com/mlai/./slides/diagrams//TooManyPigeons}{60%}}{The machine learning systems decomposition process calls for separating a complex task into decomposable separate entities. A process we can think of as \href{https://en.wikipedia.org/wiki/Pigeonholing}{pigeonholing}.}{too-many-pigeons2}


\newslide{Pigeonholing}
\slides{
1. Can we decompose decision we need to repetitive sub-tasks where inputs and outputs are well defined?
2. Are those repetitive sub-tasks well represent by a mathematical mapping?
}
\notes{Some aspects to take into account are

1.  Can we refine the decision we need to a set of repetitive tasks
    where input information and output decision/value is well defined?
2.  Can we represent each sub-task we’ve defined with a mathematical
    mapping?
}
\notes{The representation necessary for the second aspect may involve massaging
of the problem: feature selection or adaptation. It may also involve
filtering out exception cases (perhaps through a pre-classification).}

\notes{All else being equal, we’d like to keep our models simple and
interpretable. If we can convert a complex mapping to a linear mapping
through clever selection of sub-tasks and features this is a big win.}

\notes{For example, Facebook have *feature engineers*, individuals whose main
role is to design features they think might be useful for one of their
tasks (e.g. newsfeed ranking, or ad matching). Facebook have a
training/testing pipeline called
[FBLearner](https://www.facebook.com/Engineering/posts/fblearner-flow-is-a-machine-learning-platform-capable-of-easily-reusing-algorith/10154077833317200/).
Facebook have predefined the sub-tasks they are interested in, and they
are tightly connected to their business model.

It is easier for Facebook to do this because their business model is
heavily focused on user interaction. A challenge for companies that have
a more diversified portfolio of activities driving their business is the
identification of the most appropriate sub-task. A potential solution to
feature and model selection is known as *AutoML* [@Feurer:automl15]. Or we
can think of it as using Machine Learning to assist Machine Learning.
It’s also called meta-learning. Learning about learning. The input to
the ML algorithm is a machine learning task, the output is a proposed
model to solve the task.}

\newslide{A Trap}
\slides{
* Over emphasis on the *type* of model we're deploying.
* Under emphasis on the appropriateness of the task decomposition.
}
\notes{One trap that is easy to fall in is too much emphasis on the type of model we have deployed rather than the appropriateness of the task decomposition we
have chosen.}

\recommendation{Conditioned on task decomposition, we should
automate the process of model improvement. Model updates should not be
discussed in management meetings, they should be deployed and updated as
a matter of course. Further details below on model deployment, but model
updating needs to be considered at design time. This is the domain of
AutoML.}

\newslide{Chicken and Egg}

\figure{\includejpg{https://mlatcl.github.com/mlai/./slides/diagrams//ai/chicken-and-egg}{50%}}{The answer to the question which comes first, the chicken or the egg is simple, they co-evolve [@Popper:conjectures63]. Similarly, when we place components together in a complex machine learning system, they will tend to co-evolve and compensate for one another.}{chicken-and-egg2}


\newslide{Co-evolution}
\slides{
* Absolute decomposition is impossible. 
* If we deploy a weak component in one place, downstream system will compensate.
* Systems *co-evolve* ... there is no *simple* solution
* Trade off between *performance* and *decomposability*.
    * Need to monitor deployment
}
\notes{To form modern decision-making systems, many components are interlinked.  We decompose our complex decision making into individual tasks, but the performance of each component is dependent on those upstream of it.}

\notes{This naturally leads to co-evolution of systems; upstream errors can be
compensated by downstream corrections.}

\notes{To embrace this characteristic, end-to-end training could be considered. Why produce the best forecast by metrics when we can just produce the best forecast for our systems? End-to-end training can lead to improvements in performance, but it would also damage our systems decomposability and its interpretability, and perhaps its adaptability.}

\notes{Poor systems decomposition, and a lack of interpretability can compound challenges around *intellectual debt*.}

\notes{The less human interpretable our systems are, the harder they are to adapt to different circumstances or diagnose when there's a challenge.  The trade-off between interpretability and performance is a constant tension which we should always retain in our minds when performing our system design.}


-->
<ul>
<li>Grade C - accessibility
<ul>
<li>Transition: data becomes electronically available</li>
</ul></li>
<li>Grade B - validity
<ul>
<li>Transition: pose a question to the data.</li>
</ul></li>
<li>Grade A - usability</li>
</ul>
<p>\figure{</p>
<p>\figure{</p>
<!--

\editme

\subsection{Deployment}

\newslide{Premise}
\slides{
Our *machine learning* is based on a *software systems* view that is 20 years out of date.
}
\notes{Much of the academic machine learning systems point of view is based on a software systems point of view that is around 20 years out of date. In particular we build machine learning models on fixed training data sets, and we test them on stationary test data sets. 

In practice modern software systems involve continuous deployment of models into an ever-evolving world of data. These changes are indicated in the software world by greater availability of technologies like *streaming* technologies.}

\subsubsection{Continuous Deployment}
\slides{
* Deployment of modeling code.
* Data dependent models in production need *continuous monitoring*.
* Continous monitoring implies *statistical tests* rather than classic software tests.
}

\notes{Once the decomposition is understood, the data is sourced and the models
are created, the model code needs to be deployed.}



\notes{To extend the USB stick analogy further, how would as software engineer deploy the code if they thought that the code might evolve in production? This is what
data does. We cannot assume that the conditions under which we trained
our model will be retained as we move forward, indeed the only constant
we have is change.}

\notes{This means that when any data dependent model is deployed into
production, it requires *continuous monitoring* to ensure the
assumptions of design have not been invalidated. Software changes are
qualified through testing, in particular a regression test ensures that
existing functionality is not broken by change. Since data is
continually evolving, machine learning systems require 'continual
regression testing': oversight by systems that ensure their existing
functionality has not been broken as the world evolves around them. An
approach we refer to as *progression testing*. Unfortunately, standards
around ML model deployment yet been developed. The modern world of
continuous deployment does rely on testing, but it does not recognize
the continuous evolution of the world around us.}

\notes{Progression tests are likely to be *statistical* tests in contrast to classical software tests. The tests should be monitoring model performance and quality measures. They could also monitor conformance to standardized *fairness* measures.}

\newslide{Continuous Monitoring}
\slides{
* Continuous deployment:
    * We've changed the code, we should test the effect.
* Continuous Monitoring:
    * The world around us is changing, we should monitor the effect.
* Update our notions of testing: *progression testing*
}

\notes{If the world has changed around our decision-making ecosystem, how are we alerted to those changes?}

\recommendation{We establish best practice around model deployment.
We need to shift our culture from standing up a software service, to
standing up a *data as a service*. Data as a Service would involve
continual monitoring of our deployed models in production. This would be
regulated by 'hypervisor' systems[^emulation] that understand the context in
which models are deployed and recognize when circumstances have changed,
and models need retraining or restructuring.

[^emulation]: Emulation, or surrogate modelling, is one very promising approach to forming such a hypervisor. Emulators are models we fit to other models, often simulations, but the could also be other machine learning models. These models operate at the meta-level, not on the systems directly. This means they can be used to model how the sub-systems interact. As well as emulators we should consider real time dash boards, anomaly detection, mutlivariate analysis, data visualization and classical statistical approaches for hypervision of our deployed systems.
}





\editme




\editme

\subsection{Data Oriented Architectures}
\slides{
* Convert data to a *first-class citizen*.
* View system as operations on *data streams*.
* Expose data operations in a programmatic way.
}
\notes{In a streaming architecture we shift from management of services, to management of data streams. Instead of worrying about availability of the services we shift to worrying about the quality of the data those services are producing.}

\newslide{Data Orientated Architectures}

\slides{
* Historically we've been *software first*
    * A necessary but not sufficient condition for *data first*
* Move from
    1. service orientated architectures
    2. *data orientated architectures*
}
\notes{Historically we've been *software first*, this is a necessary but insufficient condition for *data first*. We need to move from software-as-a-service to data-as-a-service, from service oriented architectures to *data oriented architectures*.}

\subsection{Streaming System}
\slides{
* Move from pull updates to push updates.
* Operate on rows rather than columns.
* Lead to stateless logic: persistence handled by system.
* Example Apache Kafka + Apache Flink
}
\notes{Characteristics of a streaming system include a move from *pull* updates to *push* updates, i.e. the computation is driven by a change in the input data rather than the service calling for input data when it decides to run a computation. Streaming systems operate on 'rows' of the data rather than 'columns'. This is because the full column isn't normally available as it changes over time. As an important design principle, the services themselves are stateless, they take their state from the streaming ecosystem. This ensures the inputs and outputs of given computations are easy to declare. As a result, persistence of the data is also handled by the streaming ecosystem and decisions around data retention or recomputation can be taken at the systems level rather than the component level.}


\newslide{Streaming Architectures}
\slides{
* AWS Kinesis, Apache Kafka
* Not just about streaming
    * Nodes in the architecture are *stateless* 
    * They persist through storing state on *streams*
* This brings the data *inside out*
}

\recommendation{We should consider a major re-architecting of systems around our services. In particular we should scope the use of a *streaming architecture* (such as Apache Kafka) that ensures data persistence and enables asynchronous operation of our systems.[^data-orientated-architecture] This would enable the provision of QC streams, and real time dash boards as well as hypervisors.

[^data-orientated-architecture]: These approaches are one area of focus for my own team's research. A data first architecture is a prerequisite for efficient deployment of machine learning systems.

Importantly a streaming architecture implies the services we build are
*stateless*, internal state is deployed on streams alongside external
state. This allows for rapid assessment of other services' data.
}


\notes{The philosphy of DOA is also possible with more standard data infrastructures, such as SQL data bases, but more work has to be put into place to ensure that book-keeping around data provenance and origin is stored, as well as approaches for snapshotting the data ecosystem.}






\editme

\subsection{Apache Flink}
\slides{
* Streams and transformations
* a stream is a (potentially never-ending) flow of data records
* a transformation: streams as input, produces transformed streams as output}

\notes{[Apache Flink](https://en.wikipedia.org/wiki/Apache_Flink) is a stream processing framework. Flink is a foundation for event driven processing. This gives a high throughput and low latency framework that operates on dataflows.

Data storage is handled by other systems such as Apache Kafka or AWS Kinesis.}

\newslide{Join}

```
stream.join(otherStream)
    .where(<KeySelector>)
    .equalTo(<KeySelector>)
    .window(<WindowAssigner>)
    .apply(<JoinFunction>)
```

\notes{Apache Flink allows operations on streams. For example, the join operation above. In a traditional data base management system, this join operation may be written in SQL and called on demand. In a streaming ecosystem, computations occur as and when the streams update. 

The join is handled by the ecosystem surrounding the business logic.}






\editme

\subsection{Milan}
\slides{
1.  A general-purpose stream algebra that encodes relationships between
      data streams (the Milan Intermediate Language or Milan IL)

2.  A Scala library for building programs in that algebra.

3.  A compiler that takes programs expressed in Milan IL and produces a
     Flink application that executes the program.
}

\notes{To answer these challenges at Amazon we began the process of constructing software for data oriented architectures. The team built a *data-oriented programming* language which is
[now available through MIT license](https://github.com/amzn/milan). The language is called Milan.}

\newslide{}

\centerdiv{\tomBorchertPicture{15%}}

\notes{The Principle Engineer behind the Milan architecture has been Tom Borchert.Quoting from [Tom Borchert's blog
on Milan](https://tborchertblog.wordpress.com/2020/02/13/28/):}

> Milan has three components:
>
> 1.  A general-purpose stream algebra that encodes relationships between
>      data streams (the Milan Intermediate Language or Milan IL)
>
> 2.  A Scala library for building programs in that algebra.
>
> 3.  A compiler that takes programs expressed in Milan IL and produces a
>     Flink application that executes the program.
>
> Component (2) can be extended to support interfaces in additional
> languages, and component (3) can be extended to support additional
> runtime targets. Considering just the multiple interfaces and the
> multiple runtimes, Milan looks a lot like the much more mature Apache
> Beam. The difference lies in (1), Milan's general-purpose stream
> algebra.
}

\slides{
\newslide{}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan-schematic000}{80%}}{The Milan Software has a general purpose stream algebra at its core, the Milan IL.}{milan-schematic}
}
\slides{
\newslide{}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan-schematic001}{80%}}{The Milan Software has a general purpose stream algebra at its core, the Milan IL.}{milan-schematic}
}
\slides{
\newslide{}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan-schematic002}{80%}}{The Milan Software has a general purpose stream algebra at its core, the Milan IL.}{milan-schematic}
}
\slides{
\newslide{}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan-schematic003}{80%}}{The Milan Software has a general purpose stream algebra at its core, the Milan IL.}{milan-schematic}
}
\slides{
\newslide{}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan-schematic004}{80%}}{The Milan Software has a general purpose stream algebra at its core, the Milan IL.}{milan-schematic}
}

\newslide{}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan-schematic}{80%}}{The Milan Software has a general purpose stream algebra at its core, the Milan IL.}{milan-schematic}

\newslide{}

\figure{\includepng{https://mlatcl.github.com/mlai/./slides/diagrams//software/milan}{80%}}{The Milan Software is designed for building modern AI systems. <https://github.com/amzn/milan/>}{milan-software-page}

\notes{It is through the general-purpose stream algebra that we hope to make
significant inroads on the intellectual debt challenge.

The stream algebra defines the relationship between different machine
learning components in the wider software architecture. Composition of
multiple services cannot occur without a signature existing within the
stream algebra. The Milan IL becomes the key information structure that
is required to reason about the wider software system.}

\notes{\subsection{Context}}

\notes{This deals with the challenges that arise through the intellectual debt  because we can now see the context around each service. This
allows us to design the relevant validation checks to ensure that
accuracy and fairness are maintained. By recompiling the algebra to
focus on a particular decision within the system we can also derive new
statistical tests to validate performance. These are the checks that we
refer to as progression testing. The loss of programmer control means
that we can no longer rely on software tests written at design time, we
must have the capability to deploy new (statistical) tests after
deployment as the uses to which each service is placed extend to
previously un-envisaged domains.}

\notes{\subsection{Stateless Services}}

\notes{Importantly, Milan does not place onerous constraints on the builders of
individual machine learning models (or other components). Standard
modelling frameworks can be used. The main constraint is that any code
that is not visible to the ecosystem does not maintain or store global
state. This condition implies that the parameters of any machine
learning model need to also be declared as an input to the model within
the Milan IL.}

\subsection{Meta Modelling}

\figure{\includepng{https://mlatcl.github.com/mlai/./slides/diagrams//uq/emukit-software-page}{80%}}{The Emukit software is a set of software tools for emulation and surrogate modeling. <https://amzn.github.io/emukit/>}{emukit-software-page}

\notes{Where does machine learning come in? The strategy I propose is that the
Milan IL is integrated with meta-modelling approaches to assist in the
explanation of the decision-making framework. At their simplest these
approaches may be novelty detection algorithms on the data streams that
are emerging from a given service. This is a form of *progression
testing*. But we can go much further. By knowing the training data, the
inputs and outputs of the individual services in the software ecosystem,
we can build meta-models that test for fairness, accuracy not just of
individual system components, but short or long cascades of decision
making. Through the use of the Milan IL algebra all these tests could be
automatically deployed. The focus of machine learning is on the
models-that-model-the-models. The meta-models.

In Amazon, our own focus was on the use of statistical emulators,
sometimes known as surrogate models, for fulfilling this task. The work
we were putting into this route is available through another software
package, [Emukit, a framework for decision making under
uncertainty](https://amzn.github.io/emukit/). With collaborators my
current focus for addressing these issues is a form of fusion of Emukit
and Milan (Milemukit??). But the nature of this fusion requires testing
on real world problem sets. A task we hope to carry out in close
collaboration with colleagues at [Data Science
Africa](http://www.datascienceafrica.org/).}







\editme

\subsection{Trading System}

\slides{* High frequency share trading.
* Stream of prices with millisecond updates.
* Trades required on millisecond time line
}
\notes{As a simple example we'll consider a high frequency trading system. Anne wishes to build a share trading system. She has access to a high frequency trading system which provides prices and allows trades at millisecond intervals. She wishes to build an automated trading system.

Let's assume that price trading data is available as a data stream. But the price now is not the only information that Anne needs, she needs an estimate of the price in the future.}

\setupplotcode{import pandas as pd
import numpy as np
import os}

\plotcode{# Generate an artificial trading stream
days=pd.date_range(start='21/5/2017', end='21/05/2020')
z = np.random.randn(len(days), 1)
x = z.cumsum()+400}

\plotcode{prices = pd.Series(x, index=days)
hypothetical = prices.loc['21/5/2019':]
real = prices.copy()
real['21/5/2019':] = np.NaN}

\setupplotcode{import mlai
import teaching_plots as plot
import matplotlib.pyplot as plt}

\plotcode{fontsize=16}

\plotcode{fig, ax = plt.subplots(figsize=plot.wide_figsize)
real.plot(color='k', fontsize=fontsize)

hypothetical.plot(color='b')
ylim = ax.get_ylim()}
mlai.write_figure('hypothetical-prices.svg', directory='./data-science/')

\plotcode{fig, ax = plt.subplots(figsize=plot.wide_figsize)
real.plot(color='k', fontsize=fontsize)
ax.set_ylim(ylim)

mlai.write_figure('real-prices.svg', directory='./data-science/')}


\newslide{Real Price}

\slides{\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/real-prices}{80%}}{A set of prices from a a trading stream.}{real-prices}}

\newslide{Future Price}


\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/hypothetical-prices}{80%}}{Anne has access to the share prices in the black stream but not in the blue stream. A hypothetical stream is the stream of future prices. Anne can define this hypothetical under constraints (latency, input etc). The need for a model is now exposed in the software infrastructure}{hypothetical-prices}

\subsection{Hypothetical Streams}

\slides{
* Real stream --- share prices
    * derived *hypothetical* stream --- share prices in future.
* Hypothetical constrained by
    * input constraints.
    * decision functional
    * computational requirements (latency)
}


\notes{We'll call the future price a hypothetical stream.}

\notes{A hypothetical stream is a desired stream of information which cannot be directly accessed. The lack of direct access may be because the events happen in the future, or there may be some latency between the event and the availability of the data.}

\notes{Any hypothetical stream will only be provided as a prediction, ideally with an error bar.}

\notes{The nature of the hypothetical Anne needs is dependent on her decision-making process. In Anne's case it will depend over what period she is expecting her returns. In MDOP Anne specifies a hypothetical that is derived from the pricing stream. }

\notes{It is not the price stream directly, but Anne looks for *future* predictions from the price stream, perhaps for price in $T$ days' time.}

\notes{At this stage, this stream is merely typed as a hypothetical.}

\notes{There are constraints on the hypothetical, they include: the *input* information, the upper limit of latency between input and prediction, and the decision Anne needs to make (how far ahead, what her upside, downside risks are). These three constraints mean that we can only recover an approximation to the hypothetical.}

\subsection{Hypothetical Advantage}

\slides{* Modelling is now required.
* But modelling is declared in the ecosystem.
* If it's manual, warnings can be used 
     * calibration, fairness, dataset shift
* Opens door to Auto AI.
}

\notes{What is the advantage to defining things in this way? By defining, clearly, the two streams as real and hypothetical variants of each other, we now enable automation of the deployment and any redeployment process. The hypothetical can be *instantiated* against the real, and design criteria can be constantly evaluated triggering retraining when necessary.}






\editme


\subsection{SafeBoda}

\notes{The complexity of building safe, maintainable systems that are based on interacting components which include machine learning models means that smaller companies can be excluded from access to these technologies due the technical and intellectual debt incurred when maintaining such systems in a real-world environment.}

\figure{\includepng{https://mlatcl.github.com/mlai/./slides/diagrams//ai/safe-boda}{60%}}{SafeBoda is a ride allocation system for Boda Boda drivers. Let's imagine the capabilities we need for such an AI system.}{safe-boda-system}

\notes{[SafeBoda](https://safeboda.com/ug/index.php#whysafeboda) is a Kampala based rider allocation system for Boda Boda drivers. Boda boda are motorcycle taxis which give employment to, often young men, across Kampala. Safe Boda is driven by the knowledge that road accidents are set to match HIV/AIDS as the highest cause of death in low/middle income families by 2030.} 

\newslide{SafeBoda}

> With road accidents set to match HIV/AIDS as the highest cause of death in low/middle income countries by 2030, SafeBoda’s aim is to modernise informal transportation and ensure safe access to mobility.

\notes{A key aim of the AutoAI agenda is to reduce these technical challenges, so that such software can be maintained safely and reliably by a small team of software engineers. Without this capability it is hard to imagine how low resource environments can fully benefit from the 'data revolution' without heavy reliance on technical provision from high-resource environments. Such dependence would inevitably mean a skew towards the challenges that high-resource economies face, rather than the more urgent and important problems that are faced in low-resource environments.}






\editme


\newslide{Ride Allocation Prediction}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//ai/ride-allocation-prediction}{60%}}{Some software components in a ride allocation system. Circled components are hypothetical, rectangles represent actual data.}{ride-allocation-system}





\notes{Let's consider a ride sharing app, for example the SafeBoda system. 

Anne is on her way home now; she wishes to hail a car using a ride sharing app. 

The app is designed in the following way. On opening her app Anne is notified about drivers in the nearby neighborhood. She is given an estimate of the time a ride may take to come.

Given this information about driver availability, Anne may feel encouraged to enter a destination. Given this destination, a price estimate can be given. This price is conditioned on other riders that may wish to go in the same direction, but the price estimate needs to be made before the user agrees to the ride. 

Business customer service constraints dictate that this price may not change after Anne's order is confirmed. 

In this simple system, several decisions are being made, each of them on the basis of a hypothetical.

When Anne calls for a ride, she is provided with an estimate based on the expected time a ride can be with her. But this estimate is made without knowing where Anne wants to go. There are constraints on drivers imposed by regional boundaries, reaching the end of their shift, or their current passengers mean that this estimate can only be a best guess.

This best guess may well be driven by previous data.}




\editme

\notes{\subsection{Ride Sharing: Service Oriented to Data Oriented}}

\newslide{Ride Sharing: Service Oriented}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/ride-share-service-soa}{80%}}{Service oriented architecture. The data access is buried in the cost allocation service. Data dependencies of the service cannot be found without trawling through the underlying code base.}{ride-share-service-soa}

\notes{The modern approach to software systems design is known as a
*service-oriented architectures* (SOA). The idea is that software
engineers are responsible for the availability and reliability of the
API that accesses the service they own. Quality of service is
maintained by rigorous standards around *testing* of software systems.}

\newslide{Ride Sharing: Data Oriented}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/ride-share-service-doa}{80%}}{Data oriented architecture. Now the joins and the updates are exposed within the streaming ecosystem. We can programatically determine the factor graph which gives the thread through the model.}{ride-share-service-doa}

\notes{In data driven decision-making systems, the quality of decision-making
is determined by the quality of the data. We need to extend the notion
of *service*-oriented architecture to *data*-oriented architecture
(DOA).

The focus in SOA is eliminating *hard* failures. Hard failures can
occur due to bugs or systems overload. This notion needs to be
extended in ML systems to capture *soft failures* associated with
declining data quality, incorrect modeling assumptions and
inappropriate re-deployments of models. We need to focus on data
quality assessments. In data-oriented architectures engineering teams
are responsible for the *quality* of their output data streams in
addition to the *availability* of the service they support
[@Lawrence:drl17]. Quality here is not just accuracy, but fairness and
explainability. This important cultural change would be capable of
addressing both the challenge of *technical debt* [@Sculley:debt15]
and the social responsibility of ML systems.}

\notes{Software development proceeds with a *test-oriented*
culture. One where tests are written before software, and software is
not incorporated in the wider system until all tests pass. We must
apply the same standards of care to our ML systems, although for ML we need statistical tests for quality, fairness and consistency within the
environment. Fortunately, the main burden of this testing need not
fall to the engineers themselves: through leveraging *classical
statistics* and *emulation* we will automate the creation and
redeployment of these tests across the software ecosystem, we call
this *ML hypervision* (WP5 \textsection \ref{sec:hypervision}).

Modern AI can be based on ML models with many millions of parameters,
trained on very large data sets. In ML, strong emphasis is placed on
*predictive accuracy* whereas sister-fields such as statistics have a
strong emphasis on *interpretability*. ML models are said to be 'black
boxes' which make decisions that are not explainable.[^dark-secret]

[^dark-secret]: See for example
    ["The Dark Secret at the Heart of AI" in Technology Review](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/).}

\newslide{Ride Sharing: Hypothetical}

\figure{\includediagram{https://mlatcl.github.com/mlai/./slides/diagrams//data-science/ride-share-service-doa-hypothetical}{80%}}{Data-oriented programing. There is a requirement for an estimate of the driver allocation to give a rough cost estimate before the user has confirmed the ride. In data-oriented programming, this is achieved through declaring a hypothetical stream which approximates the true driver allocation, but with restricted input information and constraints on the computational latency.}{ride-share-service-doa-hypothetical}

\notes{For the ride sharing system, we start to see a common issue with a more complex algorithmic decision-making system. Several decisions are being made multilple times. Let's look at the decisions we need along with some design criteria.

1. Driver Availability: Estimate time to arrival for Anne's ride using Anne's location and local available car locations. Latency 50 milliseconds
2. Cost Estimate: Estimate cost for journey using Anne's destination, location and local available car current destinations and availability. Latency 50 milliseconds
3. Driver Allocation: Allocate car to minimize transport cost to destination. Latency 2 seconds.

So we need:

1. a hypothetical to estimate availability. It is constrained by lacking destination information and a low latency requirement.
2. a hypothetical to estimate cost. It is constrained by low latency requirement and 


Simultaneously, drivers in this data ecosystem have an app which notifies them about new jobs and recommends them where to go.

Further advantages. Strategies for data retention (when to snapshot) can be set globally.


A few decisions need to be made in this system. First of all, when the user opens the app, the estimate of the time to the nearest ride may need to be computed quickly, to avoid latency in the service. 

This may require a quick estimate of the ride availability.}





\editme

\subsection{Information Dynamics}

\slides{* Potential for information feedback loops.
* Hypothetical streams are instantiated.
* Nature hypothesis (e.g. price prediction) can effect reality.
* Leads to information dynamics, similar to dynamics of governors.
* See e.g. [Closed Loop Data Science](https://www.gla.ac.uk/schools/computing/research/researchsections/ida-section/closedloop/) at Glasgow.
}

\notes{With all the second guessing within a complex automated decision-making system, there are potential problems with information dynamics, the 'closed loop' problem, where the sub-systems are being approximated (second guessing) and predictions downstream are being affected.

This leads to the need for a closed loop analysis, for example, see the ["Closed Loop Data Science"](https://www.gla.ac.uk/schools/computing/research/researchsections/ida-section/closedloop/) project led by Rod Murray-Smith at Glasgow.}











-->
<ul>
<li>Access</li>
<li>Assess</li>
<li>Process</li>
</ul>
<p>\thanks</p>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
