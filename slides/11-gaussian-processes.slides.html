<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Special Topics: Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="http://inverseprobability.com/talks/assets/js/figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Special Topics: Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
</section>
<section id="review" class="slide level2">
<h2>Review</h2>
<ul>
<li>Last week: Logistic Regression and Generalised Linear Models</li>
<li>Introduced link functions and different transformations.</li>
<li>Showed examples in classification and mentioned possibilities for disease rate models.</li>
<li>This week:
<ul>
<li>Gaussian Processes: non parametric Bayesian modelling }</li>
</ul></li>
</ul>
</section>
<section id="generalized-linear-models" class="slide level2">
<h2>Generalized Linear Models</h2>
<p>Logistic regression is part of a wider class of models known as <em>generalized linear models</em>. In these models we determine that some characteristic of the model is speicified by a function that is liniear in the parameters. So we might suggest that <span class="math display">\[
\log \frac{p(\inputVector)}{1-p(\inputVector)} = \mappingFunction(\inputVector; \mappingVector)
\]</span> where <span class="math inline">\(\mappingFunction(\inputVector; \mappingVector)\)</span> is a linear-in-the-parameters function (here the parameters are <span class="math inline">\(\mappingVector\)</span>, which is generally non-linear in the inputs. So far we have considered basis function models of the form <span class="math display">\[
\mappingFunction(\inputVector) =
\mappingVector^\top \basisVector(\inputVector).
\]</span></p>
</section>
<section id="gaussian-processes" class="slide level2">
<h2>Gaussian Processes</h2>
</section>
<section id="gaussian-processes-1" class="slide level2">
<h2>Gaussian Processes</h2>
<ul>
<li>Basis function models give non-linear predictions.</li>
<li>Need to choose number and location of basis functions.</li>
<li>Gaussian processes is a general framework (basis functions special case)</li>
<li>Within the framework you can consider models with infinite basis functions.</li>
</ul>
</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1" class="slide level2">
<h2>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h2>
<script>
showDivs(9, 'two_point_sample2');
</script>
<p><small></small> <input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')"> <button onclick="plusDivs(-1, 'two_point_sample2')">❮</button> <button onclick="plusDivs(1, 'two_point_sample2')">❯</button></p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample009.svg" width style=" ">
</object>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample010.svg" width style=" ">
</object>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample011.svg" width style=" ">
</object>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" width style=" ">
</object>
</div>
</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1-1" class="slide level2">
<h2>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h2>
<p><small> * The single contour of the Gaussian density represents the <font color="cyan">joint distribution, <span class="math inline">\(p(\mappingFunction_1, \mappingFunction_2)\)</span></font></p>
<div class="fragment">
<ul>
<li>We observe that <font color="magenta"><span class="math inline">\(\mappingFunction_1=?\)</span></font></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conditional density: <font color="cyan"><span class="math inline">\(p(\mappingFunction_2|\mappingFunction_1=?)\)</span></font> </small></li>
</ul>
</div>
</section>
<section id="prediction-with-correlated-gaussians" class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mappingFunction_2\)</span> from <span class="math inline">\(\mappingFunction_1\)</span> requires <em>conditional density</em>.</p></li>
<li><p>Conditional density is <em>also</em> Gaussian. <span class="math display">\[
p(\mappingFunction_2|\mappingFunction_1) = {\mathcal{N}\left(\mappingFunction_2|\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1,\kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}\right)}
\]</span> where covariance of joint density is given by <span class="math display">\[
\kernelMatrix= \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="prediction-of-mappingfunction_8-from-mappingfunction_1" class="slide level2">
<h2>Prediction of <span class="math inline">\(\mappingFunction_{8}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h2>
<script>
showDivs(13, 'two_point_sample3');
</script>
<p><small></small> <input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')"> <button onclick="plusDivs(-1, 'two_point_sample3')">❮</button> <button onclick="plusDivs(1, 'two_point_sample3')">❯</button></p>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample013.svg" width style=" ">
</object>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample014.svg" width style=" ">
</object>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample015.svg" width style=" ">
</object>
</div>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample016.svg" width style=" ">
</object>
</div>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample017.svg" width style=" ">
</object>
</div>
</section>
<section id="details" class="slide level2">
<h2>Details</h2>
<ul>
<li>The single contour of the Gaussian density represents the <font color="yellow">joint distribution, <span class="math inline">\(p(\mappingFunction_1, \mappingFunction_8)\)</span></font></li>
</ul>
<div class="fragment">
<ul>
<li>We observe a value for <font color="magenta"><span class="math inline">\(\mappingFunction_1=-?\)</span></font></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conditional density: <font color="cyan"><span class="math inline">\(p(\mappingFunction_5|\mappingFunction_1=?)\)</span></font>.</li>
</ul>
</div>
</section>
<section id="prediction-with-correlated-gaussians-1" class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mappingFunctionVector_*\)</span> from <span class="math inline">\(\mappingFunctionVector\)</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <span class="math display">\[
p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|\kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector,\kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}\right)}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span class="math display">\[
\kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="prediction-with-correlated-gaussians-2" class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mappingFunctionVector_*\)</span> from <span class="math inline">\(\mappingFunctionVector\)</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <span class="math display">\[
p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|{\boldsymbol{{\mu}}},\boldsymbol{\Sigma}\right)}
\]</span> <span class="math display">\[
{\boldsymbol{{\mu}}}= \kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector
\]</span> <span class="math display">\[\boldsymbol{\Sigma} = \kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span class="math display">\[
\kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level2">
<h2>Marginal Likelihood</h2>
</section>
<section id="sampling-from-the-prior" class="slide level2">
<h2>Sampling from the Prior</h2>
</section>
<section id="weight-space-view" class="slide level2">
<h2>Weight Space View</h2>
<p><span class="math display">\[
\weightVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
\]</span></p>
<p><span class="math display">\[
\mappingFunction(\inputVector) =
\weightVector^\top \basisVector(\inputVector).
\]</span></p>
</section>
<section id="function-space-view" class="slide level2">
<h2>Function Space View</h2>
<p><span class="math display">\[
\weightVector \sim \gaussianSamp{\zerosVector}{\alpha \eye}
\]</span></p>
<p><span class="math display">\[
\basisMatrix = \begin{bmatrix}\basisVector(\inputVector_1) \\ \vdots \\
\basisVector(\inputVector_n)\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mappingFunctionVector = \begin{bmatrix} \mappingFunction_1
\\ \vdots \mappingFunction_n\end{bmatrix}
\]</span> in the form <span class="math display">\[
\mappingFunctionVector = \basisMatrix
\weightVector.
\]</span>}</p>
<p><span class="math display">\[
\mappingFunctionVector \sim \gaussianSamp{\zerosVector}{\alpha \basisMatrix\basisMatrix^\top}.
\]</span></p>
<p><span class="math display">\[
\kernelMatrix = \alpha
\basisMatrix\basisMatrix^\top.
\]</span></p>
<p>{This image is the covariance expressed between different points on the function. In regression we normally also add independent Gaussian noise to obtain our observations <span class="math inline">\(\dataVector\)</span>, <span class="math display">\[
\dataVector = \mappingFunctionVector + \boldsymbol{\epsilon}
\]</span></p>
<p><span class="math display">\[
\epsilon \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}.
\]</span></p>
<p><span class="math display">\[
\dataVector \sim \gaussianSamp{\zerosVector}{\basisMatrix\basisMatrix^\top +\dataStd^2\eye}.
\]</span></p>
</section>
<section id="exercise-2" class="slide level2">
<h2>Exercise 2</h2>
<p><strong>Function Space Reflection</strong> How do you include the noise term when sampling in the weight space point of view?</p>
</section>
<section id="non-degenerate-gaussian-processes" class="slide level2">
<h2>Non-degenerate Gaussian Processes</h2>
<ul>
<li>This process is <em>degenerate</em>.</li>
<li>Covariance function is of rank at most <span class="math inline">\(\numHidden\)</span>.</li>
<li>As <span class="math inline">\(\numData \rightarrow \infty\)</span>, covariance matrix is not full rank.</li>
<li>Leading to <span class="math inline">\(\det{\kernelMatrix} = 0\)</span></li>
</ul>
</section>
<section id="infinite-networks" class="slide level2">
<h2>Infinite Networks</h2>
<ul>
<li>In ML Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would happen if you took <span class="math inline">\(\numHidden \rightarrow \infty\)</span>?”</li>
</ul>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Page 37 of <a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s 1994 thesis</a>
</aside>
</section>
<section id="roughly-speaking" class="slide level2">
<h2>Roughly Speaking</h2>
<ul>
<li><p>Instead of</p>
<p><span class="math display">\[
\begin{align*}
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) &amp; = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)\\
&amp; = \alpha \sum_k \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_j\right)
\end{align*}
\]</span></p></li>
<li><p>Sample infinitely many from a prior density, <span class="math inline">\(p(\mappingVector^{(1)})\)</span>,</p></li>
</ul>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \int \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}, \inputVector_j\right) p(\mappingVector^{(1)}) \text{d}\mappingVector^{(1)}
\]</span></p>
<ul>
<li>Also applies for non-Gaussian <span class="math inline">\(p(\mappingVector^{(1)})\)</span> because of the <em>central limit theorem</em>.</li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level2">
<h2>Simple Probabilistic Program</h2>
<ul>
<li><p>If <span class="math display">\[
\begin{align*} 
\mappingVector^{(1)} &amp; \sim p(\cdot)\\ \phi_i &amp; = \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right), 
\end{align*}
\]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level2">
<h2>Further Reading</h2>
<ul>
<li><p>Chapter 2 of Neal’s thesis <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>Rest of Neal’s thesis. <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
</section>
<section id="gaussian-process" class="slide level2">
<h2>Gaussian Process</h2>
<p><span class="math display">\[
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left( -\frac{\left\Vert \inputVector-\inputVector^\prime\right\Vert^2}{2\ell^2}\right),
\]</span></p>
<p><span class="math display">\[
\left\Vert\inputVector - \inputVector^\prime\right\Vert^2 = (\inputVector - \inputVector^\prime)^\top (\inputVector - \inputVector^\prime) 
\]</span></p>
</section>
<section id="exercise-3" class="slide level2">
<h2>Exercise 3</h2>
<p><strong>Moving Parameters</strong> Have a play with the parameters for this covariance function (the lengthscale and the variance) and see what effects the parameters have on the types of functions you observe.</p>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions defined by our covarariance
</aside>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in Bayesian inference we discard all samples that are distant from the data.
</aside>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed analytically using linear algebra.
</aside>
</section>
<section id="gaussian-process-1" class="slide level2">
<h2>Gaussian Process</h2>
<p><span class="math display">\[
p(\dataVector|\inputMatrix) =
\frac{1}{(2\pi)^{\frac{\numData}{2}}|\kernelMatrix|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\dataVector^\top \left(\kernelMatrix+\dataStd^2
\eye\right)^{-1}\dataVector\right)
\]</span></p>
<p><span class="math display">\[
\errorFunction(\boldsymbol{\theta}) = \frac{1}{2} \log |\kernelMatrix|
+ \frac{1}{2} \dataVector^\top \left(\kernelMatrix +
\dataStd^2\eye\right)^{-1}\dataVector
\]</span></p>
</section>
<section id="making-predictions" class="slide level2">
<h2>Making Predictions</h2>
<p><span class="math display">\[
\begin{bmatrix}\mappingFunctionVector \\ \mappingFunctionVector^*\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\
\kernelMatrix_\ast^\top &amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\ \kernelMatrix_\ast^\top
&amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}
\]</span></p>
</section>
<section id="conditional-density" class="slide level2">
<h2>Conditional Density</h2>
<p><span class="math display">\[
\mappingFunctionVector^* | \mappingFunctionVector \sim \gaussianSamp{\meanVector_\mappingFunction}{\mathbf{C}_\mappingFunction}
\]</span> with a mean given by <span class="math display">\[
\meanVector_\mappingFunction = \kernelMatrix_*^\top \left[\kernelMatrix + \dataStd^2
\eye\right]^{-1} \dataVector
\]</span> and a covariance given by <span class="math display">\[
\mathbf{C}_\mappingFunction
= \kernelMatrix_{*,*} - \kernelMatrix_*^\top \left[\kernelMatrix + \dataStd^2
\eye\right]^{-1} \kernelMatrix_\ast.
\]</span></p>
<p><span class="math display">\[
\mathbf{A} = \left[\kernelMatrix + \dataStd^2\eye\right]^{-1}\kernelMatrix_*.
\]</span></p>
<p><span class="math display">\[
\text{data} + \text{model} \rightarrow \text{prediction}.
\]</span></p>
</section>
<section id="exercise-4" class="slide level2">
<h2>Exercise 4</h2>
<p>Now try changing the parameters of the covariance function (and the noise) to see how the predictions change.</p>
<p>Then try sampling from this conditional density to see what your predictions look like. What happens if you sample from the conditional density in regions a long way into the future or the past? How does this compare with the results from the polynomial model?</p>
</section>
<section id="the-importance-of-the-covariance-function" class="slide level2">
<h2>The Importance of the Covariance Function</h2>
<p><span class="math display">\[
\meanVector_\mappingFunction = \mathbf{A}^\top \dataVector,
\]</span></p>
<iframe width height src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</section>
<section id="improving-the-numerics" class="slide level2">
<h2>Improving the Numerics</h2>
<p>In practice we shouldn’t be using matrix inverse directly to solve the GP system. One more stable way is to compute the <em>Cholesky decomposition</em> of the kernel matrix. The log determinant of the covariance can also be derived from the Cholesky decomposition.</p>
</section>
<section id="capacity-control" class="slide level2">
<h2>Capacity Control</h2>
</section>
<section id="gradients-of-the-likelihood" class="slide level2">
<h2>Gradients of the Likelihood</h2>
</section>
<section id="overall-process-scale" class="slide level2">
<h2>Overall Process Scale</h2>
</section>
<section id="capacity-control-and-data-fit" class="slide level2">
<h2>Capacity Control and Data Fit</h2>
</section>
<section id="learning-covariance-parameters" class="slide level2">
<h2>Learning Covariance Parameters</h2>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\]</span></p>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\color{yellow} \det{\kernelMatrix}^{\frac{1}{2}}}}{\color{cyan}\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;{\color{yellow}-\frac{1}{2}\log\det{\kernelMatrix}}{\color{cyan}-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\errorFunction(\parameterVector) = {\color{yellow} \frac{1}{2}\log\det{\kernelMatrix}} + {\color{cyan} \frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
\]</span></p>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <span class="math display">\[\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<p><span> <span class="math display">\[\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\eigenvalueMatrix\)</span> represents distance on axes. <span class="math inline">\(\rotationMatrix\)</span> gives rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<ul>
<li><span class="math inline">\(\eigenvalueMatrix\)</span> is <em>diagonal</em>, <span class="math inline">\(\rotationMatrix^\top\rotationMatrix = \eye\)</span>.</li>
<li>Useful representation since <span class="math inline">\(\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-coloryellow-log-detkernelmatrix" class="slide level2">
<h2>Capacity control: <span class="math inline">\({\color{yellow} \log \det{\kernelMatrix}}\)</span></h2>
<!--
\only<1>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant1.svg}}\only<2>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant2.svg}}\only<3>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant3.svg}}\only<4>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant4.svg}}\only<5>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant5.svg}}\only<6>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant6.svg}}\only<7>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant7.svg}}\only<8>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant8.svg}}\only<9>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant9.svg}}\only<10>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant10.svg}}-->
</section>
<section id="data-fit-colorcyan-fracdatavectortopkernelmatrix-1datavector2" class="slide level2">
<h2>Data Fit: <span class="math inline">\({\color{cyan} \frac{\dataVector^\top\kernelMatrix^{-1}\dataVector}{2}}\)</span></h2>
<!--

\only<1>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic1.svg}}\only<2>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic2.svg}}\only<3>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic3.svg}}-->
</section>
<section id="errorfunctionparametervector-coloryellowfrac12logdetkernelmatrixcolorcyanfracdatavectortopkernelmatrix-1datavector2" class="slide level2">
<h2><span class="math display">\[\errorFunction(\parameterVector) = {\color{yellow}\frac{1}{2}\log\det{\kernelMatrix}}+{\color{cyan}\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}\]</span></h2>
<script>
showDivs(0, 'gp-optimise');
</script>
<p><small></small> <input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')"> <button onclick="plusDivs(-1, 'gp-optimise')">❮</button> <button onclick="plusDivs(1, 'gp-optimise')">❯</button></p>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise000.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise001.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise002.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise003.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise004.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise005.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise007.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise008.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise009.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise011.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise012.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise013.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise014.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise015.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise017.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise018.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise019.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise020.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level2">
<h2>Exponentiated Quadratic Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The exponentiated quadratic covariance function.
</aside>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1892.
</aside>
</section>
<section id="alan-turing" class="slide level2">
<h2>Alan Turing</h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank" >Alan Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="probability-winning-olympics" class="slide level2">
<h2>Probability Winning Olympics?</h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-marathon-gp.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="della-gatta-gene-data" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gene expression levels over time for a gene from data provided by <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>. We would like to understand whethere there is signal in the data, or we are only observing noise.
</aside>
</section>
<section id="gene-expression-example" class="slide level2">
<h2>Gene Expression Example</h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-29" class="slide level2">
<h2></h2>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The example is taken from the paper “A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression.” <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.
</aside>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
</section>
<section id="tp53-gene-data-gp" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale parameter initialized to 50 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-1" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale parameter initialized to 2000 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-2" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the noise initialized low (standard deviation 0.1) and the time scale parameter initialized to 20 minutes.
</aside>
</section>
<section id="multiple-optima" class="slide level2">
<h2>Multiple Optima</h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<!--
## Multiple Optima  {}



<object class="svgplot " data="../slides/diagrams/gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="example-prediction-of-malaria-incidence-in-uganda" class="slide level2">
<h2>Example: Prediction of Malaria Incidence in Uganda</h2>
<span style="text-align:right"><svg viewBox="2662 600 1110-2662 1780-600" style="width:1.5cm"> <defs> <clipPath id="clip">
<style>
  rect {
      fill: black;
         }
  </style>
<p><rect x="2662" y="600" width="1110-2662" height="1780-600"/> </clipPath> </defs> <image preserveAspectRatio="xMinYMin slice"  xlink:href="../slides/diagrams/people/2013_03_28_180606.JPG" clip-path="url(#clip)" /> </svg></span></p>
<ul>
<li>Work with Ricardo Andrade Pacheco, John Quinn and Martin Mubaganzi (Makerere University, Uganda)</li>
<li>See <a href="http://air.ug/research.html">AI-DEV Group</a>.</li>
</ul>
</section>
<section id="malaria-prediction-in-uganda" class="slide level2">
<h2>Malaria Prediction in Uganda</h2>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Ugandan districs. Data SRTM/NASA from <a href="https://dds.cr.usgs.gov/srtm/version2_1" class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al., 2014; Mubangizi et al., 2014)</span></span></p>
</section>
<section id="kapchorwa-district" class="slide level2">
<h2>Kapchorwa District</h2>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Kapchorwa_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kapchorwa District, home district of Stephen Kiprotich.
</aside>
</section>
<section id="tororo-district" class="slide level2">
<h2>Tororo District</h2>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Tororo district, where the sentinel site, Nagongera, is located.
</aside>
</section>
<section id="malaria-prediction-in-nagongera-sentinel-site" class="slide level2">
<h2>Malaria Prediction in Nagongera (Sentinel Site)</h2>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Sentinel and HMIS data along with rainfall and temperature for the Nagongera sentinel station in the Tororo district.
</aside>
</section>
<section id="mubende-district" class="slide level2">
<h2>Mubende District</h2>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Mubende District.
</aside>
</section>
<section id="malaria-prediction-in-uganda-1" class="slide level2">
<h2>Malaria Prediction in Uganda</h2>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Prediction of malaria incidence in Mubende.
</aside>
</section>
<section id="gp-school-at-makerere" class="slide level2">
<h2>GP School at Makerere</h2>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The project arose out of the Gaussian process summer school held at Makerere in Kampala in 2013. The school led, in turn, to the Data Science Africa initiative.
</aside>
</section>
<section id="kabarole-district" class="slide level2">
<h2>Kabarole District</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kabarole district in Uganda.
</aside>
</section>
<section id="early-warning-system" class="slide level2">
<h2>Early Warning System</h2>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Estimate of the current disease situation in the Kabarole district over time. Estimate is constructed with a Gaussian process with an additive covariance funciton.
</aside>
</section>
<section id="early-warning-systems" class="slide level2">
<h2>Early Warning Systems</h2>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The map of Ugandan districts with an overview of the Malaria situation in each district.
</aside>
</section>
<section id="additive-covariance" class="slide level2">
<h2>Additive Covariance</h2>
<center>
<span class="math display">\[\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/add_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
An additive covariance function formed by combining two exponentiated quadratic covariance functions.
</aside>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
This is a retrospective analysis of US births by Aki Vehtari. The challenges of forecasting. Even with seasonal and weekly effects removed there are significant effects on holidays, weekends, etc.
</aside>
</section>
<section id="gelman-book" class="slide level2">
<h2>Gelman Book</h2>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Two different editions of Bayesian Data Analysis <span class="citation" data-cites="Gelman:bayesian13">(Gelman et al., 2013)</span>.
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Gelman:bayesian13">Gelman et al. (2013)</span></span></p>
</section>
<section id="basis-function-covariance" class="slide level2">
<h2>Basis Function Covariance</h2>
<center>
<span class="math display">\[\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
A covariance function based on a non-linear basis given by <span class="math inline">\(\basisVector(\inputVector)\)</span>.
</aside>
</section>
<section id="brownian-covariance" class="slide level2">
<h2>Brownian Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)\]</span>
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Brownian motion covariance function.
</aside>
</section>
<section id="mlp-covariance" class="slide level2">
<h2>MLP Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The multi-layer perceptron covariance function. This is derived by considering the infinite limit of a neural network with probit activation functions.
</aside>
</section>
<section id="gpss-gaussian-process-summer-school" class="slide level2">
<h2>GPSS: Gaussian Process Summer School</h2>
<table>
<tr>
<td width="60%">
<ul>
<li><a href="http://gpss.cc" class="uri">http://gpss.cc</a></li>
<li>Next one is in Sheffield in <em>September 2019</em>.</li>
<li>Many lectures from past meetings available online</li>
</ul>
</td>
<td width="40%">
<div style="width:1.5cm;text-align:center">

</div>
</td>
</tr>
</table>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2014/07/01/open-data-science">Open Data Science</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Andrade:consistent14">
<p>Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014. Consistent mapping of government malaria records across a changing territory delimitation. Malaria Journal 13. <a href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a></p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a></p>
</div>
<div id="ref-Gelman:bayesian13">
<p>Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through Gaussian process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a></p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis). California Institute of Technology.</p>
</div>
<div id="ref-Mubangizi:malaria14">
<p>Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence, N.D., 2014. Malaria surveillance with multiple data sources using Gaussian process models, in: 1st International Conference on the Use of Mobile ICT in Africa.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis). Dept. of Computer Science, University of Toronto.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
