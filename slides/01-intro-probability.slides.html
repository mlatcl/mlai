<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Probability and an Introduction to Jupyter, Python and Pandas</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="http://inverseprobability.com/talks/assets/js/figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Probability and an Introduction to Jupyter, Python and Pandas</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
</section>
<section id="course-text" class="slide level2">
<h2>Course Text</h2>
<div class="figure">
<div id="a-first-course-in-machine-learning-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/mlai/a-first-course-in-machine-learning.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The main course text is “A First Course in Machine Learning” by <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span>.
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></span></p>
</section>
<section id="additional-course-text" class="slide level2">
<h2>Additional Course Text</h2>
<div class="figure">
<div id="pattern-recognition-and-machine-learning-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/mlai/978-0-387-31073-2.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
For additional reading we will regularly refer to “Pattern Recognition and Machine Learning” by <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span>
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></span></p>
<!-- SECTION Assumed Knowledge -->
</section>
<section id="assumed-knowledge" class="slide level2">
<h2>Assumed Knowledge</h2>
</section>
<section id="choice-of-language" class="slide level2">
<h2>Choice of Language</h2>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise 1</h2>
<p>Who invented python and why? What was the language designed to do? What is the origin of the name “python”? Is the language a compiled language? Is it an object orientated language?</p>
<!-- SECTION Choice of Environment -->
</section>
<section id="choice-of-environment" class="slide level2">
<h2>Choice of Environment</h2>
</section>
<section id="exercise-1-1" class="slide level2">
<h2>Exercise 1</h2>
<p>What is jupyter and why was it invented? Give some examples of functionality it gives over standard python. What is the jupyter project? Name two languages involved in the Jupyter project other than python.</p>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level2">
<h2>What is Machine Learning?</h2>
</section>
<section id="what-is-machine-learning-1" class="slide level2">
<h2>What is Machine Learning?</h2>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-2" class="slide level2">
<h2>What is Machine Learning?</h2>
<p><span class="math display">\[\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction (\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span class="math inline">\(\errorFunction (\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</section>
<section id="overdetermined-system" class="slide level2">
<h2>Overdetermined System</h2>
</section>
<section id="section" class="slide level2">
<h2></h2>
<script>
showDivs(1, 'over_determined_system');
</script>
<p><small></small> <input id="range-over_determined_system" type="range" min="1" max="8" value="1" onchange="setDivs('over_determined_system')" oninput="setDivs('over_determined_system')"> <button onclick="plusDivs(-1, 'over_determined_system')">❮</button> <button onclick="plusDivs(1, 'over_determined_system')">❯</button></p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system001.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system002.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system003.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system004.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system005.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system006.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="over_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/over_determined_system007.svg" width="40%" style=" ">
</object>
</div>
</section>
<section id="datascalar-minputscalar-c" class="slide level2">
<h2><span class="math inline">\(\dataScalar = m\inputScalar + c\)</span></h2>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span></p>
<p><span class="math display">\[2.5 = 2m + c\]</span></p>
</div>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pierre-simon-laplace-image-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/Pierre-Simon_Laplace.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Pierre-Simon Laplace 1749-1827.
</aside>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<p><a href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2"><img data-src="../slides/diagrams/books/1YQPAAAAQAAJ-PR17-IA2.png" /></a></p>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="figure">
<div id="laplaces-determinism-english-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/laplacesDeterminismEnglish.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Laplace’s determinsim in English translation.
</aside>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="figure">
<div id="laplaces-demon-cropped-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/philosophicaless00lapliala_16_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
</aside>
<!--<object data="../slides/diagrams/physics/philosophicaless00lapliala.pdf" type="application/pdf" width="80%" height="">
    <embed src="../slides/diagrams/physics/philosophicaless00lapliala.pdf" type="application/pdf">
        <p>This browser does not support PDF viewing. Please download the PDF to view it: <a href="../slides/diagrams/physics/philosophicaless00lapliala.pdf">Download PDF</a>.</p>
    </embed>
</object>-->
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<p><a href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PR17-IA4"><img data-src="../slides/diagrams/books/1YQPAAAAQAAJ-PR17-IA4.png" /></a></p>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="figure">
<div id="probability-relative-in-part-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/philosophicaless00lapliala.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
To Laplace, determinism is a strawman. Ignorance of mechanism and data leads to uncertainty which should be dealt with through probability.
</aside>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="figure">
<div id="probability-relative-in-part-cropped-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/philosophicaless00lapliala_18_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
</aside>
<!--<object data="../slides/diagrams/physics/philosophicaless00lapliala.pdf" type="application/pdf" width="80%" height="">
    <embed src="../slides/diagrams/physics/philosophicaless00lapliala.pdf" type="application/pdf">
        <p>This browser does not support PDF viewing. Please download the PDF to view it: <a href="../slides/diagrams/physics/philosophicaless00lapliala.pdf">Download PDF</a>.</p>
    </embed>
</object>
-->
</section>
<section id="datascalar-minputscalar-c-noisescalar" class="slide level2">
<h2><span class="math inline">\(\dataScalar = m\inputScalar + c + \noiseScalar\)</span></h2>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c + \noiseScalar_1
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c + \noiseScalar_2
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span> <span class="math display">\[
2.5 = 2m + c + \noiseScalar_3
\]</span></p>
</div>
</section>
<section id="a-probabilistic-process" class="slide level2">
<h2>A Probabilistic Process</h2>
<div class="fragment">
<p>Set the mean of Gaussian to be a function. <span class="math display">\[
p\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp \left(-\frac{\left(\dataScalar_i-\mappingFunction\left(\inputScalar_i\right)\right)^{2}}{2\dataStd^2}\right).
\]</span></p>
</div>
<div class="fragment">
<p>This gives us a ‘noisy function’.</p>
</div>
<div class="fragment">
<p>This is known as a stochastic process.</p>
<!-- SECTION Nigerian NMIS Data -->
</div>
</section>
<section id="nigerian-nmis-data" class="slide level2">
<h2>Nigerian NMIS Data</h2>
</section>
<section id="nigerian-nmis-data-notebook" class="slide level2">
<h2>Nigerian NMIS Data: Notebook</h2>
</section>
<section id="exercise-1-2" class="slide level2">
<h2>Exercise 1</h2>
<p>Read on the internet about the following python libraries: <code>numpy</code>, <code>matplotlib</code>, <code>scipy</code> and <code>pandas</code>. What functionality does each provide python?</p>
<!-- SECTION Probabilities -->
</section>
<section id="probabilities" class="slide level2">
<h2>Probabilities</h2>
</section>
<section id="probability-and-the-nmis-data" class="slide level2">
<h2>Probability and the NMIS Data</h2>
<!-- SECTION Conditioning -->
</section>
<section id="conditioning" class="slide level2">
<h2>Conditioning</h2>
</section>
<section id="exercise-2" class="slide level2">
<h2>Exercise 2</h2>
<p>Write code that prints out the probability of nurses being greater than 2 for different numbers of doctors.</p>
</section>
<section id="probability-review" class="slide level2">
<h2>Probability Review</h2>
<ul>
<li>We are interested in trials which result in two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, each of which has an ‘outcome’ denoted by <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>.</li>
<li>We summarise the notation and terminology for these distributions in the following table.</li>
</ul>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<table>
<thead>
<tr class="header">
<th>Terminology</th>
<th>Mathematical notation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>joint</td>
<td><span class="math inline">\(P(X=x, Y=y)\)</span></td>
<td>prob. that X=x <em>and</em> Y=y</td>
</tr>
<tr class="even">
<td>marginal</td>
<td><span class="math inline">\(P(X=x)\)</span></td>
<td>prob. that X=x <em>regardless of</em> Y</td>
</tr>
<tr class="odd">
<td>conditional</td>
<td><span class="math inline">\(P(X=x\vert Y=y)\)</span></td>
<td>prob. that X=x <em>given that</em> Y=y</td>
</tr>
</tbody>
</table>
<center>
The different basic probability distributions.
</center>
</section>
<section id="a-pictorial-definition-of-probability" class="slide level2">
<h2>A Pictorial Definition of Probability</h2>
<div class="figure">
<div id="prob-diagram-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/mlai/prob_diagram.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Diagram representing the different probabilities, joint, marginal and conditional. This diagram was inspired by lectures given by Christopher Bishop.
</aside>
<p><span style="text-align:right">Inspired by lectures from Christopher Bishop</span></p>
</section>
<section id="definition-of-probability-distributions" class="slide level2">
<h2>Definition of probability distributions</h2>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 46%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Terminology</th>
<th>Definition</th>
<th>Probability Notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Joint Probability</td>
<td><span class="math inline">\(\lim_{N\rightarrow\infty}\frac{n_{X=3,Y=4}}{N}\)</span></td>
<td><span class="math inline">\(P\left(X=3,Y=4\right)\)</span></td>
</tr>
<tr class="even">
<td>Marginal Probability</td>
<td><span class="math inline">\(\lim_{N\rightarrow\infty}\frac{n_{X=5}}{N}\)</span></td>
<td><span class="math inline">\(P\left(X=5\right)\)</span></td>
</tr>
<tr class="odd">
<td>Conditional Probability</td>
<td><span class="math inline">\(\lim_{N\rightarrow\infty}\frac{n_{X=3,Y=4}}{n_{Y=4}}\)</span></td>
<td><span class="math inline">\(P\left(X=3\vert Y=4\right)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="notational-details" class="slide level2">
<h2>Notational Details</h2>
<ul>
<li><p>Typically we should write out <span class="math inline">\(P\left(X=x,Y=y\right)\)</span>.</p></li>
<li><p>In practice, we often use <span class="math inline">\(P\left(x,y\right)\)</span>.</p></li>
<li><p>This looks very much like we might write a multivariate function, <em>e.g.</em> <span class="math inline">\(f\left(x,y\right)=\frac{x}{y}\)</span>.</p>
<ul>
<li>For a multivariate function though, <span class="math inline">\(f\left(x,y\right)\neq f\left(y,x\right)\)</span>.</li>
<li>However <span class="math inline">\(P\left(x,y\right)=P\left(y,x\right)\)</span> because <span class="math inline">\(P\left(X=x,Y=y\right)=P\left(Y=y,X=x\right)\)</span>.</li>
</ul></li>
<li><p>We now quickly review the ‘rules of probability’.</p></li>
</ul>
</section>
<section id="normalization" class="slide level2">
<h2>Normalization</h2>
<p><em>All</em> distributions are normalized. This is clear from the fact that <span class="math inline">\(\sum_{x}n_{x}=N\)</span>, which gives <span class="math display">\[\sum_{x}P\left(x\right)={\lim_{N\rightarrow\infty}}\frac{\sum_{x}n_{x}}{N}={\lim_{N\rightarrow\infty}}\frac{N}{N}=1.\]</span> A similar result can be derived for the marginal and conditional distributions.</p>
</section>
<section id="the-product-rule" class="slide level2">
<h2>The Product Rule</h2>
<ul>
<li><span class="math inline">\(P\left(x|y\right)\)</span> is <span class="math display">\[
{\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{n_{y}}.
\]</span></li>
<li><span class="math inline">\(P\left(x,y\right)\)</span> is <span class="math display">\[
{\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{N}={\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{n_{y}}\frac{n_{y}}{N}
\]</span> or in other words <span class="math display">\[
P\left(x,y\right)=P\left(x|y\right)P\left(y\right).
\]</span> This is known as the product rule of probability.</li>
</ul>
</section>
<section id="the-sum-rule" class="slide level2">
<h2>The Sum Rule</h2>
<p>Ignoring the limit in our definitions:</p>
<ul>
<li><p>The marginal probability <span class="math inline">\(P\left(y\right)\)</span> is <span class="math inline">\({\lim_{N\rightarrow\infty}}\frac{n_{y}}{N}\)</span> .</p></li>
<li><p>The joint distribution <span class="math inline">\(P\left(x,y\right)\)</span> is <span class="math inline">\({\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{N}\)</span>.</p></li>
<li><p><span class="math inline">\(n_{y}=\sum_{x}n_{x,y}\)</span> so <span class="math display">\[
{\lim_{N\rightarrow\infty}}\frac{n_{y}}{N}={\lim_{N\rightarrow\infty}}\sum_{x}\frac{n_{x,y}}{N},
\]</span> in other words <span class="math display">\[
P\left(y\right)=\sum_{x}P\left(x,y\right).
\]</span> This is known as the sum rule of probability.</p></li>
</ul>
</section>
<section id="exercise-3" class="slide level2">
<h2>Exercise 3</h2>
<p>Write code that computes <span class="math inline">\(P(y)\)</span> by adding <span class="math inline">\(P(y, x)\)</span> for all values of <span class="math inline">\(x\)</span>.</p>
</section>
<section id="bayes-rule" class="slide level2">
<h2>Bayes’ Rule</h2>
<ul>
<li>From the product rule, <span class="math display">\[
P\left(y,x\right)=P\left(x,y\right)=P\left(x|y\right)P\left(y\right),\]</span> so <span class="math display">\[
P\left(y|x\right)P\left(x\right)=P\left(x|y\right)P\left(y\right)
\]</span> which leads to Bayes’ rule, <span class="math display">\[
P\left(y|x\right)=\frac{P\left(x|y\right)P\left(y\right)}{P\left(x\right)}.
\]</span></li>
</ul>
</section>
<section id="bayes-theorem-example" class="slide level2">
<h2>Bayes’ Theorem Example</h2>
<ul>
<li>There are two barrels in front of you. Barrel One contains 20 apples and 4 oranges. Barrel Two other contains 4 apples and 8 oranges. You choose a barrel randomly and select a fruit. It is an apple. What is the probability that the barrel was Barrel One?</li>
</ul>
</section>
<section id="bayes-rule-example-answer-i" class="slide level2">
<h2>Bayes’ Rule Example: Answer I</h2>
<ul>
<li>We are given that: <span class="math display">\[\begin{aligned}
  P(\text{F}=\text{A}|\text{B}=1) = &amp; 20/24 \\
  P(\text{F}=\text{A}|\text{B}=2) = &amp; 4/12 \\
  P(\text{B}=1) = &amp; 0.5 \\
  P(\text{B}=2) = &amp; 0.5
\end{aligned}\]</span></li>
</ul>
</section>
<section id="bayes-rule-example-answer-ii" class="slide level2">
<h2>Bayes’ Rule Example: Answer II</h2>
<ul>
<li>We use the sum rule to compute: <span class="math display">\[\begin{aligned}
  P(\text{F}=\text{A}) = &amp; P(\text{F}=\text{A}|\text{B}=1)P(\text{B}=1) \\&amp; + P(\text{F}=\text{A}|\text{B}=2)P(\text{B}=2) \\
        = &amp; 20/24\times 0.5 + 4/12 \times 0.5 = 7/12
 \end{aligned}\]</span></li>
<li>And Bayes’ rule tells us that: <span class="math display">\[\begin{aligned}
  P(\text{B}=1|\text{F}=\text{A}) = &amp; \frac{P(\text{F} = \text{A}|\text{B}=1)P(\text{B}=1)}{P(\text{F}=\text{A})}\\ 
       = &amp; \frac{20/24 \times 0.5}{7/12} = 5/7
\end{aligned}\]</span></li>
</ul>
</section>
<section id="further-reading" class="slide level2 scrollable">
<h2 class="scrollable">Further Reading</h2>
<ul>
<li>Probability distributions: page 12–17 (Section 1.2) of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></li>
</ul>
</section>
<section id="exercises" class="slide level2 scrollable">
<h2 class="scrollable">Exercises</h2>
<ul>
<li>Exercise 1.3 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></li>
</ul>
</section>
<section id="computing-expectations-example" class="slide level2">
<h2>Computing Expectations Example</h2>
<ul>
<li>Consider the following distribution.</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P\left(y\right)\)</span></td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<ul>
<li>What is the mean of the distribution?</li>
<li>What is the standard deviation of the distribution?</li>
<li>Are the mean and standard deviation representative of the distribution form?</li>
<li>What is the expected value of <span class="math inline">\(-\log P(y)\)</span>?</li>
</ul>
</section>
<section id="expectations-example-answer" class="slide level2">
<h2>Expectations Example: Answer</h2>
<ul>
<li>We are given that:</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P\left(y\right)\)</span></td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
</tr>
<tr class="even">
<td><span class="math inline">\(y^2\)</span></td>
<td>1</td>
<td>4</td>
<td>9</td>
<td>16</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(-\log(P(y))\)</span></td>
<td>1.204</td>
<td>1.609</td>
<td>2.302</td>
<td>0.916</td>
</tr>
</tbody>
</table>
<ul>
<li>Mean: <span class="math inline">\(1\times 0.3 + 2\times 0.2 + 3 \times 0.1 + 4 \times 0.4 = 2.6\)</span></li>
<li>Second moment: <span class="math inline">\(1 \times 0.3 + 4 \times 0.2 + 9 \times 0.1 + 16 \times 0.4 = 8.4\)</span></li>
<li>Variance: <span class="math inline">\(8.4 - 2.6\times 2.6 = 1.64\)</span></li>
<li>Standard deviation: <span class="math inline">\(\sqrt{1.64} = 1.2806\)</span></li>
</ul>
</section>
<section id="expectations-example-answer-ii" class="slide level2">
<h2>Expectations Example: Answer II</h2>
<ul>
<li>We are given that:</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P\left(y\right)\)</span></td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
</tr>
<tr class="even">
<td><span class="math inline">\(y^2\)</span></td>
<td>1</td>
<td>4</td>
<td>9</td>
<td>16</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(-\log(P(y))\)</span></td>
<td>1.204</td>
<td>1.609</td>
<td>2.302</td>
<td>0.916</td>
</tr>
</tbody>
</table>
<ul>
<li>Expectation <span class="math inline">\(-\log(P(y))\)</span>: <span class="math inline">\(0.3\times 1.204 + 0.2\times 1.609 + 0.1\times 2.302 +0.4\times 0.916 = 1.280\)</span></li>
</ul>
</section>
<section id="sample-based-approximation-example" class="slide level2">
<h2>Sample Based Approximation Example</h2>
<ul>
<li><p>You are given the following values samples of heights of students,</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y_i\)</span></td>
<td>1.76</td>
<td>1.73</td>
<td>1.79</td>
<td>1.81</td>
<td>1.85</td>
<td>1.80</td>
</tr>
</tbody>
</table></li>
<li><p>What is the sample mean?</p></li>
<li><p>What is the sample variance?</p></li>
<li><p>Can you compute sample approximation expected value of <span class="math inline">\(-\log P(y)\)</span>?</p></li>
</ul>
</section>
<section id="sample-based-approximation-example-answer" class="slide level2">
<h2>Sample Based Approximation Example: Answer</h2>
<ul>
<li>We can compute:</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y_i\)</span></td>
<td>1.76</td>
<td>1.73</td>
<td>1.79</td>
<td>1.81</td>
<td>1.85</td>
<td>1.80</td>
</tr>
<tr class="even">
<td><span class="math inline">\(y^2_i\)</span></td>
<td>3.0976</td>
<td>2.9929</td>
<td>3.2041</td>
<td>3.2761</td>
<td>3.4225</td>
<td>3.2400</td>
</tr>
</tbody>
</table>
<ul>
<li>Mean: <span class="math inline">\(\frac{1.76 + 1.73 + 1.79 + 1.81 + 1.85 + 1.80}{6} = 1.79\)</span></li>
<li>Second moment: $  = 3.2055$</li>
<li>Variance: <span class="math inline">\(3.2055 - 1.79\times1.79 = 1.43\times 10^{-3}\)</span></li>
<li>Standard deviation: <span class="math inline">\(0.0379\)</span></li>
<li>No, you can’t compute it. You don’t have access to <span class="math inline">\(P(y)\)</span> directly.</li>
</ul>
</section>
<section id="sample-based-approximation-example-1" class="slide level2">
<h2>Sample Based Approximation Example</h2>
<ul>
<li><p>You are given the following values samples of heights of students,</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y_i\)</span></td>
<td>1.76</td>
<td>1.73</td>
<td>1.79</td>
<td>1.81</td>
<td>1.85</td>
<td>1.80</td>
</tr>
</tbody>
</table></li>
<li><p>Actually these “data” were sampled from a Gaussian with mean 1.7 and standard deviation 0.15. Are your estimates close to the real values? If not why not?</p></li>
</ul>
</section>
<section id="exercise-1-3" class="slide level2">
<h2>Exercise 1</h2>
<p>Now we see we have several additional features. Let’s assume we want to predict <code>maternal_health_delivery_services</code>. How would we go about doing it?</p>
<p>Using what you’ve learnt about joint, conditional and marginal probabilities, as well as the sum and product rule, how would you formulate the question you want to answer in terms of probabilities? Should you be using a joint or a conditional distribution? If it’s conditional, what should the distribution be over, and what should it be conditioned on?</p>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mlai-lecture-2012-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/GX8VLYUYScM?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
MLAI Lecture 2 from 2012.
</aside>
</section>
<section id="reading" class="slide level2">
<h2>Reading</h2>
<ul>
<li><p>See probability review at end of slides for reminders.</p></li>
<li><p>For other material in Bishop read:</p></li>
<li><p>If you are unfamiliar with probabilities you should complete the following exercises:</p></li>
</ul>
</section>
<section id="further-reading-1" class="slide level2 scrollable">
<h2 class="scrollable">Further Reading</h2>
<ul>
<li><p>Section 2.2 (pg 41–53) of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></p></li>
<li><p>Section 2.4 (pg 55–58) of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></p></li>
<li><p>Section 2.5.1 (pg 58–60) of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></p></li>
<li><p>Section 2.5.3 (pg 61–62) of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span></p></li>
<li><p>Probability densities: Section 1.2.1 (Pages 17–19) of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
<li><p>Expectations and Covariances: Section 1.2.2 (Pages 19–20) of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
<li><p>The Gaussian density: Section 1.2.4 (Pages 24–28) (don’t worry about material on bias) of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
<li><p>For material on information theory and KL divergence try Section 1.6 &amp; 1.6.1 (pg 48 onwards) of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
</ul>
</section>
<section id="exercises-1" class="slide level2 scrollable">
<h2 class="scrollable">Exercises</h2>
<ul>
<li><p>Exercise 1.7 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
<li><p>Exercise 1.8 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
<li><p>Exercise 1.9 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span></p></li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What is Machine Learning?</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Bishop:book06">
<p>Bishop, C.M., 2006. Pattern recognition and machine learning. springer.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
