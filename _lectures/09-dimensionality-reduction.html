---
title: "Dimensionality Reduction: Latent Variable Modelling"
abstract: "In this lecture we turn to <em>unsupervised learning</em>.
Specifically, we introduce the idea of a latent variable model. Latent
variable models are a probabilistic perspective on unsupervised learning
which lead to dimensionality reduction algorithms."
edit_url: https://github.com/mlatcl/mlai/edit/gh-pages/_lamd/dimensionality-reduction.md
week: 9
featured_image: slides/diagrams/dimred/dem_manifold_print002.png
reveal: 09-dimensionality-reduction.slides.html
ipynb: 09-dimensionality-reduction.ipynb
pptx: 09-dimensionality-reduction.pptx
youtube: "0mtK2_rc0IY"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="setup">Setup</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_notebooks/includes/notebook-setup.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_notebooks/includes/notebook-setup.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
<h2 id="notutils">notutils</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This small package is a helper package for various notebook utilities
used below.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install notutils</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/notutils"
class="uri">https://github.com/lawrennd/notutils</a></p>
<p>Once <code>notutils</code> is installed, it can be imported in the
usual manner.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> notutils</span></code></pre></div>
<h2 id="pods">pods</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In Sheffield we created a suite of software tools for ‘Open Data
Science’. Open data science is an approach to sharing code, models and
data that should make it easier for companies, health professionals and
scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/ods"
class="uri">https://github.com/lawrennd/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<h2 id="mlai">mlai</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The <code>mlai</code> software is a suite of helper functions for
teaching and demonstrating machine learning algorithms. It was first
used in the Machine Learning and Adaptive Intelligence course in
Sheffield in 2013.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install mlai</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/mlai"
class="uri">https://github.com/lawrennd/mlai</a></p>
<p>Once <code>mlai</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<h2 id="review">Review</h2>
<p>So far in our classes we have focussed mainly on regression problems,
which are examples of supervised learning. We have considered the
relationship between the likelihood and the objective function and we
have shown how we can find paramters by maximizing the likelihood
(equivalent to minimizing the objective function) and in the last
session we saw how we can <em>marginalize</em> the parameters in a
process known as Bayesian inference.</p>
<h2 id="clustering">Clustering</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/clustering.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/clustering.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li><p>One common approach, not deeply covered in this course.</p></li>
<li><p>Associate each data point, <span class="math inline">\(\mathbf{
y}_{i, :}\)</span> with one of <span class="math inline">\(k\)</span>
different discrete groups.</p></li>
<li><p>For example:</p>
<ul>
<li>Clustering animals into discrete groups. Are animals discrete or
continuous?</li>
<li>Clustering into different different <em>political</em>
affiliations.</li>
</ul></li>
<li><p>Humans do seem to like clusters:</p>
<ul>
<li>Very useful when interacting with biologists.</li>
</ul></li>
<li><p>Subtle difference between clustering and <em>vector
quantisation</em></p></li>
<li><p>Little anecdote.</p></li>
<li><p>To my mind difference is in clustering there should be a
reduction in data density between samples.</p></li>
<li><p>This definition is not universally applied.</p></li>
<li><p>For today’s purposes we merge them:</p>
<ul>
<li>Determine how to allocate each point to a group and <em>harder</em>
total number of groups.</li>
</ul></li>
<li><p>Simple algorithm for allocating points to groups.</p></li>
<li><p><em>Require</em>: Set of <span class="math inline">\(k\)</span>
cluster centres &amp; assignment of each points to a cluster.</p></li>
</ul>
<ol type="1">
<li>Initialize cluster centres as randomly selected data points.
<ol start="2" type="1">
<li>Assign each data point to <em>nearest</em> cluster centre.</li>
<li>Update each cluster centre by setting it to the mean of assigned
data points.</li>
<li>Repeat 2 and 3 until cluster allocations do not change.</li>
</ol></li>
</ol>
<ul>
<li>This minimizes the objective <span class="math display">\[
E=\sum_{j=1}^K \sum_{i\ \text{allocated to}\ j}  \left(\mathbf{ y}_{i,
:} - \boldsymbol{ \mu}_{j, :}\right)^\top\left(\mathbf{ y}_{i, :} -
\boldsymbol{ \mu}_{j, :}\right)
\]</span> <em>i.e.</em> it minimizes thesum of Euclidean squared
distances betwen points and their associated centres.</li>
<li>The minimum is <em>not</em> guaranteed to be <em>global</em> or
<em>unique</em>.</li>
<li>This objective is a non-convex optimization problem.</li>
</ul>
<p>Clustering methods associate each data point with a different label.
Unlike in classification the label is not provided by a human annotator.
It is allocated by the computer. Clustering is quite intuitive for
humans, we do it naturally with our observations of the real world. For
example, we cluster animals into different groups. If we encounter a new
animal, we can immediately assign it to a group: bird, mammal, insect.
These are certainly labels that can be provided by humans, but they were
also originally invented by humans. With clustering we want the computer
to recreate that process of inventing the label.</p>
<p>Unsupervised learning enables computers to form similar
categorizations on data that is too large scale for us to process. When
the Greek philosopher, Plato, was thinking about ideas, he considered
the concept of the Platonic ideal. The Platonic ideal bird is the bird
that is most bird-like or the chair that is most chair-like. In some
sense, the task in clustering is to define different clusters, by
finding their Platonic ideal (known as the cluster center) and allocate
each data point to the relevant cluster center. So, allocate each animal
to the class defined by its nearest cluster center.</p>
<p>To perform clustering on a computer we need to define a notion of
either similarity or distance between the objects and their Platonic
ideal, the cluster center. We normally assume that our objects are
represented by vectors of data, <span class="math inline">\(\mathbf{
x}_i\)</span>. Similarly, we represent our cluster center for category
<span class="math inline">\(j\)</span> by a vector <span
class="math inline">\(\boldsymbol{ \mu}_j\)</span>. This vector contains
the ideal features of a bird, a chair, or whatever category <span
class="math inline">\(j\)</span> is. In clustering we can either think
in terms of similarity of the objects, or distances. We want objects
that are similar to each other to cluster together. We want objects that
are distant from each other to cluster apart.</p>
<p>This requires us to formalize our notion of similarity or distance.
Let’s focus on distances. A definition of distance between an object,
<span class="math inline">\(i\)</span>, and the cluster center of class
<span class="math inline">\(j\)</span> is a function of two vectors, the
data point, <span class="math inline">\(\mathbf{ x}_i\)</span> and the
cluster center, <span class="math inline">\(\boldsymbol{
\mu}_j\)</span>, <span class="math display">\[
d_{ij} = f(\mathbf{ x}_i, \boldsymbol{ \mu}_j).
\]</span> Our objective is then to find cluster centers that are close
to as many data points as possible. For example, we might want to
cluster customers into their different tastes. We could represent each
customer by the products they’ve purchased in the past. This could be a
binary vector <span class="math inline">\(\mathbf{ x}_i\)</span>. We can
then define a distance between the cluster center and the customer.</p>
<h3 id="squared-distance">Squared Distance</h3>
<p>A commonly used distance is the squared distance, <span
class="math display">\[
d_{ij} = (\mathbf{ x}_i - \boldsymbol{ \mu}_j)^2.
\]</span> The squared distance comes up a lot in machine learning. In
unsupervised learning it was used to measure dissimilarity between
predictions and observed data. Here its being used to measure the
dissimilarity between a cluster center and the data.</p>
<p>Once we have decided on the distance or similarity function, we can
decide a number of cluster centers, <span
class="math inline">\(K\)</span>. We find their location by allocating
each center to a sub-set of the points and minimizing the sum of the
squared errors, <span class="math display">\[
E(\mathbf{M}) = \sum_{i \in \mathbf{i}_j} (\mathbf{ x}_i - \boldsymbol{
\mu}_j)^2
\]</span> where the notation <span
class="math inline">\(\mathbf{i}_j\)</span> represents all the indices
of each data point which has been allocated to the <span
class="math inline">\(j\)</span>th cluster represented by the center
<span class="math inline">\(\boldsymbol{ \mu}_j\)</span>.</p>
<h3 id="k-means-clustering"><span class="math inline">\(k\)</span>-Means
Clustering</h3>
<p>One approach to minimizing this objective function is known as
<em><span class="math inline">\(k\)</span>-means clustering</em>. It is
simple and relatively quick to implement, but it is an initialization
sensitive algorithm. Initialization is the process of choosing an
initial set of parameters before optimization. For <span
class="math inline">\(k\)</span>-means clustering you need to choose an
initial set of centers. In <span class="math inline">\(k\)</span>-means
clustering your final set of clusters is very sensitive to the initial
choice of centers. For more technical details on <span
class="math inline">\(k\)</span>-means clustering you can watch a video
of Alex Ihler introducing the algorithm here.</p>
<h3 id="k-means-clustering-1"><span
class="math inline">\(k\)</span>-Means Clustering</h3>
<div class="figure">
<div id="kmeans-clustering-13-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ml/kmeans_clustering_013.svg" width="\width" style=" ">
</object>
</div>
<div id="kmeans-clustering-13-magnify" class="magnify"
onclick="magnifyFigure(&#39;kmeans-clustering-13&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kmeans-clustering-13-caption" class="caption-frame">
<p>Figure: Clustering with the <span
class="math inline">\(k\)</span>-means clustering algorithm.</p>
</div>
</div>
<div class="figure">
<div id="k-means-clustering-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/mfqmoUN-Cuw?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="k-means-clustering-magnify" class="magnify"
onclick="magnifyFigure(&#39;k-means-clustering&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="k-means-clustering-caption" class="caption-frame">
<p>Figure: <span class="math inline">\(k\)</span>-means clustering by
Alex Ihler.</p>
</div>
</div>
<h3 id="hierarchical-clustering">Hierarchical Clustering</h3>
<p>Other approaches to clustering involve forming taxonomies of the
cluster centers, like humans apply to animals, to form trees. You can
learn more about agglomerative clustering in this video from Alex
Ihler.</p>
<div class="figure">
<div id="alex-ihler-hierarchical-clustering-figure"
class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/OcoE7JlbXvY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="alex-ihler-hierarchical-clustering-magnify" class="magnify"
onclick="magnifyFigure(&#39;alex-ihler-hierarchical-clustering&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="alex-ihler-hierarchical-clustering-caption"
class="caption-frame">
<p>Figure: Hierarchical Clustering by Alex Ihler.</p>
</div>
</div>
<h3 id="phylogenetic-trees">Phylogenetic Trees</h3>
<p>Indeed, one application of machine learning techniques is performing
a hierarchical clustering based on genetic data, i.e. the actual
contents of the genome. If we do this across a number of species then we
can produce a <em>phylogeny</em>. The phylogeny aims to represent the
actual evolution of the species and some phylogenies even estimate the
timing of the common ancestor between two species<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
Similar methods are used to estimate the origin of viruses like AIDS or
Bird flu which mutate very quickly. Determining the origin of viruses
can be important in containing or treating outbreaks.</p>
<h3 id="product-clustering">Product Clustering</h3>
<p>An e-commerce company could apply hierarchical clustering to all its
products. That would give a phylogeny of products. Each cluster of
products would be split into sub-clusters of products until we got down
to individual products. For example, we might expect a high level split
to be Electronics/Clothing. Of course, a challenge with these tree-like
structures is that many products belong in more than one parent cluster:
for example running shoes should be in more than one group, they are
‘sporting goods’ and they are ‘apparel’. A tree structure doesn’t allow
this allocation.</p>
<h3 id="hierarchical-clustering-challenge">Hierarchical Clustering
Challenge</h3>
<p>Our own psychological grouping capabilities are studied as a domain
of cognitive science. Researchers like Josh Tenenbaum have developed
algorithms that decompose data in more complex ways, but they can
normally only be applied to smaller data sets.</p>
<h2 id="other-clustering-approaches">Other Clustering Approaches</h2>
<ul>
<li>Spectral clustering (<span class="citation"
data-cites="Shi:normalized00">Shi and Malik (2000)</span>,<span
class="citation" data-cites="Ng:spectral02">Ng et al. (n.d.)</span>)
<ul>
<li>Allows clusters which aren’t convex hulls.</li>
</ul></li>
<li>Dirichlet process
<ul>
<li>A probabilistic formulation for a clustering algorithm that is
<em>non-parametric</em>.</li>
<li>Loosely speaking it allows infinite clusters</li>
<li>In practice useful for dealing with previously unknown species
(e.g. a “Black Swan Event”).</li>
</ul></li>
</ul>
<h2 id="high-dimensional-data">High Dimensional Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>To introduce high dimensional data, we will first of all introduce a
hand written digit from the U.S. Postal Service handwritten digit data
set (originally collected from scanning enveolopes) and used in the
first convolutional neural network paper <span class="citation"
data-cites="LeCun:zip89">(Le Cun et al., 1989)</span>.</p>
<p><span class="citation" data-cites="LeCun:zip89">Le Cun et al.
(1989)</span> downscaled the images to <span class="math inline">\(16
\times 16\)</span>, here we use an image at the original scale,
containing 64 rows and 57 columns. Since the pixels are binary, and the
number of dimensions is 3,648, this space contains <span
class="math inline">\(2^{3,648}\)</span> possible images. So this space
contains a lot more than just one digit.</p>
<h2 id="usps-samples">USPS Samples</h2>
<p>If we sample from this space, taking each pixel independently from a
probability which is given by the number of pixels which are ‘on’ in the
original image, over the total number of pixels, we see images that look
nothing like the original digit.</p>
<div class="figure">
<div id="dem-six-sample-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_six000.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_six001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_six002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="dem-six-sample-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-six-sample&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-six-sample-caption" class="caption-frame">
<p>Figure: Even if we sample every nano second from now until the end of
the universe we won’t see the original six again.</p>
</div>
</div>
<p>Even if we sample every nanosecond from now until the end of the
universe you won’t see the original six again.</p>
<h2 id="simple-model-of-digit">Simple Model of Digit</h2>
<p>So, an independent pixel model for this digit doesn’t seem sensible.
The total space is enormous, and yet the space occupied by the type of
data we’re interested in is relatively small.</p>
<p>Consider a different type of model. One where we take a prototype six
and we rotate it left and right to create new data.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install scikit<span class="op">-</span>image</span></code></pre></div>
<div class="figure">
<div id="dem-six-rotate-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_six_rotate001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_six_rotate003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_six_rotate005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="dem-six-rotate-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-six-rotate&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-six-rotate-caption" class="caption-frame">
<p>Figure: Rotate a prototype six to form a set of plausible sixes.</p>
</div>
</div>
<div class="figure">
<div id="dem-six-mainfold-print-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_manifold_print001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_manifold_print002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="dem-six-mainfold-print-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-six-mainfold-print&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-six-mainfold-print-caption" class="caption-frame">
<p>Figure: The rotated sixes projected onto the first two principal
components of the ‘rotated data set’. The data lives on a one
dimensional manifold in the 3,648 dimensional space.</p>
</div>
</div>
<h2 id="low-dimensional-manifolds">Low Dimensional Manifolds</h2>
<p>Of course, in practice pure rotation of the image is too simple a
model. Digits can undergo several distortions and retain their nature.
For example, they can be scaled, they can go through translation, they
can udnergo ‘thinning’. But, for data with ‘structure’ we expect fewer
of these distortions than the dimension of the data. This means we
expect data to live on a lower dimensonal manifold. This implies that we
should deal with high dimensional data by looking for a lower
dimensional (non-linear) embedding.</p>
<h1 id="latent-variables">Latent Variables</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/latent-variables.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/latent-variables.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Latent means hidden, and hidden variables are simply
<em>unobservable</em> variables. The idea of a latent variable is
crucial to the concept of artificial intelligence, machine learning and
experimental design. A latent variable could take many forms. We might
observe a man walking along a road with a large bag of clothes and we
might <em>infer</em> that the man is walking to the laundrette. Our
observations are a highly complex data space, the response in our eyes
is processed through our visual cortex, the combination of the
individual’s limb movememnts and the direction they are walking in all
conflate in our heads to cause us to infer that (perhaps) the individual
is going to the laundrette. We don’t <em>know</em> that the man is
walking to the laundrette, but we have a model of the world that
suggests that it’s a likely outcome for the very complex data. In some
ways the latent variable can be seen as a <em>compression</em> of this
very complex scene. If I were writing a book, I might write that “A man
tripped over whilst walking to the laundrette”. In the reader’s mind an
image of a man, perhaps laden with dirty clothes, may occur. All these
ideas come from our expectations of the world around us. We can make
further inference about the man, some of it perhaps plausible others
less so. The man may be going to the laundrette because his washing
machine is broken, or because he doesn’t have a large enough flat to
have a washing machine, or because he’s carrying a duvet, or because he
doesn’t like ironing. All of these may <em>increase</em> in probability
given our observation, but they are still <em>latent</em> variables.
Unless we follow the man back to his appartment, or start making other
enquirires about the man, we don’t know the true answer.</p>
<p>It’s clear that to do inference about any complex system we
<em>must</em> include latent variables. Latent variables are extremely
powerful. In robotics, they are used to represent the <em>state</em> of
the robot. The state of the robot may include its position (in x, y
coordinates) its speed, its direction of facing. How are <em>these</em>
variables unknown to the robot? Well the robot only posesses
<em>sensors</em>, it can make observations of the nearest object in a
certain direction, and it may have a map of its environment. If we
represent the state of the robot as its position on a map, it may be
uncertain of that position. If you go walking or running in the hills
around Sheffield, you can take a very high quality ordnance survey map
with you. However, unless you are a really excellent orienteer, when you
are far from any given landmark, you will probably be <em>uncertain</em>
about your true position on the map. These states are also latent
variables.</p>
<p>In statistical analysis of experiments you try to control for each
aspect of the experiment, in particular by <em>randomization</em>. So if
I’m interested in the ability of a particular fertilizer to improve the
yield of a particular plant I may design an experiment where I apply the
fertilizer to some plants (the treatment group) and withold the
fertilizer from others (the control group). I then test to see whether
the yield from the treatment group is better (or worse) than the control
group. I may find that I have an excellent yield for the treatment
group. However, what if I’d (unknowlingly) planted all my treatment
plants in a sunny part of the field, and all the control plants in a
shady part of the field. That would also be a latent variable, in this
case known as a <em>confounder</em>. In statistical experimental design
<em>randomization</em> is used to attempt to eliminate the correlated
effects of these confounders: you aim to ensure that if these
confounders <em>do</em> exist their effects are not correlated with
treatment and contorl. This is known as a <a
href="http://en.wikipedia.org/wiki/Randomized_controlled_trial">randomized
control trial</a>.</p>
<p>Greek philosophers worried a great deal about what was knowable and
what was unknowable. Adherents of <a
href="http://en.wikipedia.org/wiki/Skepticism">philosophical
Skeptisism</a> were inspired by the idea that since your senses
sometimes give you contradictory information, they cannot be trusted,
and in extreme cases they chose to <em>ignore</em> their senses. This is
an acknowledgement that very often the true state of the world cannot be
known with precision. Unfortunately, these philosophers didn’t have a
good understanding of probability, so they were unable to encapsulate
their ideas through a <em>degree</em> of belief.</p>
<p>We often use language to express the compression of a complex
behavior or patterns in a simpler way, for example we talk about motives
as a useful distallation for a perhaps very complex patter of behavior.
In physics we use principles of causation and simple laws to describe
the world around us. Such motives or underlying principles are difficult
to observe directly, our conclusions about them emerge over a period of
time by observing indirect consequences of the latent variables.</p>
<p>Epistemic uncertainty allows us to deal with these worries by
associating our degree of belief about the state of the world with a
probaiblity distribution. This core idea underpins state space
modelling, probabilistic graphical models and the wider field of latent
variable modelling. In this session we are going to explore the idea in
a simple linear system and see how it relates to <em>factor
analysis</em> and <em>principal component analysis</em>.</p>
<h1 id="your-personality">Your Personality</h1>
<p>At the beginning of the 20th century there was a great deal of
interest amoungst psychologists in formalizing patterns of thought. The
approach they used became known as factor analysis. The principle is
that we observe a potentially high dimensional vector of characteristics
about an individual. To formalize this, social scientists designed
questionaires. We can envisage many questions that we may ask, but the
assumption is that underlying these questions there are only a few
traits that dictate the behavior. These models are known as latent trait
models and the analysis is sometimes known as factor analysis. The idea
is that there are a few characteristic traits that we are looking to
discern. These traits or factors can be extracted by assimilating the
high dimensional characteristics of the individual into a few latent
factors.</p>
<h2 id="factor-analysis-model">Factor Analysis Model</h2>
<p>This causes us to consider a model as follows, if we are given a high
dimensional vector of features (perhaps questionaire answers) associated
with an individual, <span class="math inline">\(\mathbf{ y}\)</span>, we
assume that these factors are actually generated from a low dimensional
vector latent traits, or latent variables, which determine the
personality. <span class="math display">\[
\mathbf{ y}= \mathbf{f}(\mathbf{ z}) + \boldsymbol{ \epsilon},
\]</span> where <span class="math inline">\(\mathbf{f}(\mathbf{
z})\)</span> is a <em>vector valued</em> function that is dependent on
the latent traits and <span class="math inline">\(\boldsymbol{
\epsilon}\)</span> is some corrupting noise. For simplicity, we assume
that the function is given by a <em>linear</em> relationship, <span
class="math display">\[
\mathbf{f}(\mathbf{ z}) = \mathbf{W}\mathbf{ z}
\]</span> where we have introduced a matrix <span
class="math inline">\(\mathbf{W}\)</span> that is sometimes referred to
as the <em>factor loadings</em> but we also immediately see is related
to our <em>multivariate linear regression</em> models from the . That is
because our vector valued function is of the form</p>
<p><span class="math display">\[
\mathbf{f}(\mathbf{ z}) =
\begin{bmatrix} f_1(\mathbf{ z}) \\ f_2(\mathbf{ z}) \\ \vdots \\
f_p(\mathbf{ z})\end{bmatrix}
\]</span> where there are <span class="math inline">\(p\)</span>
features associated with the individual. If we consider any of these
functions individually we have a prediction function that looks like a
regression model, <span class="math display">\[
f_j(\mathbf{ z}) =
\mathbf{ w}_{j, :}^\top \mathbf{ z},
\]</span> for each element of the vector valued function, where <span
class="math inline">\(\mathbf{ w}_{:, j}\)</span> is the <span
class="math inline">\(j\)</span>th column of the matrix <span
class="math inline">\(\mathbf{W}\)</span>. In that context each column
of <span class="math inline">\(\mathbf{W}\)</span> is a vector of
<em>regression weights</em>. This is a multiple input and multiple
output regression. Our inputs (or covariates) have dimensionality
greater than 1 and our outputs (or response variables) also have
dimensionality greater than one. Just as in a standard regression, we
are assuming that we don’t observe the function directly (note that this
<em>also</em> makes the function a <em>type</em> of latent variable),
but we observe some corrupted variant of the function, where the
corruption is given by <span class="math inline">\(\boldsymbol{
\epsilon}\)</span>. Just as in linear regression we can assume that this
corruption is given by Gaussian noise, where the noise for the <span
class="math inline">\(j\)</span>th element of <span
class="math inline">\(\mathbf{ y}\)</span> is by, <span
class="math display">\[
\epsilon_j \sim \mathcal{N}\left(0,\sigma^2_j\right).
\]</span> Of course, just as in a regression problem we also need to
make an assumption across the individual data points to form our full
likelihood. Our data set now consists of many observations of <span
class="math inline">\(\mathbf{ y}\)</span> for diffetent individuals. We
store these observations in a <em>design matrix</em>, <span
class="math inline">\(\mathbf{Y}\)</span>, where each <em>row</em> of
<span class="math inline">\(\mathbf{Y}\)</span> contains the observation
for one individual. To emphasize that <span
class="math inline">\(\mathbf{ y}\)</span> is a vector derived from a
row of <span class="math inline">\(\mathbf{Y}\)</span> we represent the
observation of the features associated with the <span
class="math inline">\(i\)</span>th individual by <span
class="math inline">\(\mathbf{ y}_{i, :}\)</span>, and place each
individual in our data matrix,</p>
<p><span class="math display">\[
\mathbf{Y}
= \begin{bmatrix} \mathbf{ y}_{1, :}^\top \\ \mathbf{ y}_{2, :}^\top \\
\vdots \\
\mathbf{ y}_{n, :}^\top\end{bmatrix},
\]</span> where we have <span class="math inline">\(n\)</span> data
points. Our data matrix therefore has <span
class="math inline">\(n\)</span> rows and <span
class="math inline">\(p\)</span> columns. The point to notice here is
that each data obsesrvation appears as a row vector in the design matrix
(thus the transpose operation inside the brackets). Our prediction
functions are now actually a <em>matrix value</em> function, <span
class="math display">\[
\mathbf{F} = \mathbf{Z}\mathbf{W}^\top,
\]</span> where for each matrix the data points are in the rows and the
data features are in the columns. This implies that if we have <span
class="math inline">\(q\)</span> inputs to the function we have <span
class="math inline">\(\mathbf{F}\in \Re^{n\times p}\)</span>, <span
class="math inline">\(\mathbf{W}\in \Re^{p \times q}\)</span> and <span
class="math inline">\(\mathbf{Z}\in \Re^{n\times q}\)</span>.</p>
<h3 id="exercise-1">Exercise 1</h3>
<p>Show that, given all the definitions above, if, <span
class="math display">\[
\mathbf{F} = \mathbf{Z}\mathbf{W}^\top
\]</span> and the elements of the vector valued function <span
class="math inline">\(\mathbf{F}\)</span> are given by <span
class="math display">\[
f_{i, j} = f_j(\mathbf{ z}_{i, :}),
\]</span> where <span class="math inline">\(\mathbf{ z}_{i, :}\)</span>
is the <span class="math inline">\(i\)</span>th row of the latent
variables, <span class="math inline">\(\mathbf{Z}\)</span>, then show
that <span class="math display">\[
f_j(\mathbf{ z}_{i, :}) = \mathbf{ w}_{j, :}^\top
\mathbf{ z}_{i, :}
\]</span></p>
<h2 id="latent-variables-vs-linear-regression">Latent Variables vs
Linear Regression</h2>
<p>The difference between this model and a multiple output regression is
that in the regression case we are provided with the covariates <span
class="math inline">\(\mathbf{Z}\)</span>, here they are <em>latent
variables</em>. These variables are unknown. Just as we have done in the
past for unknowns, we now treat them with a probability distribution. In
<em>factor analysis</em> we assume that the latent variables have a
Gaussian density which is independent across both across the latent
variables associated with the different data points, and across those
associated with different data features, so we have, <span
class="math display">\[
x_{i,j} \sim
\mathcal{N}\left(0,1\right),
\]</span> and we can write the density governing the latent variable
associated with a single point as, <span class="math display">\[
\mathbf{ z}_{i, :} \sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right).
\]</span> If we consider the values of the function for the <span
class="math inline">\(i\)</span>th data point as <span
class="math display">\[
\mathbf{f}_{i, :} =
\mathbf{f}(\mathbf{ z}_{i, :}) = \mathbf{W}\mathbf{ z}_{i, :}
\]</span> then we can use the rules for multivariate Gaussian
relationships to write that <span class="math display">\[
\mathbf{f}_{i, :} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top\right)
\]</span> which implies that the distribution for <span
class="math inline">\(\mathbf{ y}_{i, :}\)</span> is given by <span
class="math display">\[
\mathbf{ y}_{i, :} = \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top +
\boldsymbol{\Sigma}\right)
\]</span> where <span class="math inline">\(\boldsymbol{\Sigma}\)</span>
the covariance of the noise variable, <span
class="math inline">\(\epsilon_{i, :}\)</span> which for factor analysis
is a diagonal matrix (because we have assumed that the noise was
<em>independent</em> across the features), <span class="math display">\[
\boldsymbol{\Sigma} = \begin{bmatrix}\sigma^2_{1} &amp; 0 &amp; 0 &amp;
0\\
0 &amp; \sigma^2_{2} &amp; 0 &amp; 0\\
                                     0 &amp; 0 &amp; \ddots &amp;
0\\
                                     0 &amp; 0 &amp; 0 &amp;
\sigma^2_p\end{bmatrix}.
\]</span> For completeness, we could also add in a <em>mean</em> for the
data vector <span class="math inline">\(\boldsymbol{ \mu}\)</span>,
<span class="math display">\[
\mathbf{ y}_{i, :} = \mathbf{W}\mathbf{ z}_{i, :} +
\boldsymbol{ \mu}+ \boldsymbol{ \epsilon}_{i, :}
\]</span> which would give our marginal distribution for <span
class="math inline">\(\mathbf{ y}_{i, :}\)</span> a mean <span
class="math inline">\(\boldsymbol{ \mu}\)</span>. However, the maximum
likelihood solution for <span class="math inline">\(\boldsymbol{
\mu}\)</span> turns out to equal the empirical mean of the data, <span
class="math display">\[
\boldsymbol{ \mu}= \frac{1}{n} \sum_{i=1}^n
\mathbf{ y}_{i, :},
\]</span> <em>regardless</em> of the form of the covariance, <span
class="math inline">\(\mathbf{C}= \mathbf{W}\mathbf{W}^\top +
\boldsymbol{\Sigma}\)</span>. As a result it is very common to simply
preprocess the data and ensure it is zero mean. We will follow that
convention for this session.</p>
<p>The prior density over latent variables is independent, and the
likelihood is independent, that means that the marginal likelihood here
is also independent over the data points. Factor analysis was developed
mainly in psychology and the social sciences for understanding
personality and intelligence. <a
href="http://en.wikipedia.org/wiki/Charles_Spearman">Charles
Spearman</a> was concerned with the measurements of “the abilities of
man” and is credited with the earliest version of factor analysis.</p>
<h1 id="principal-component-analysis">Principal Component Analysis</h1>
<p>In 1933 <a
href="http://en.wikipedia.org/wiki/Harold_Hotelling">Harold
Hotelling</a> published on <em>principal component analysis</em> the
first mention of this approach <span class="citation"
data-cites="Hotelling:analysis33">(Hotelling, 1933)</span>. Hotelling’s
inspiration was to provide mathematical foundation for factor analysis
methods that were by then widely used within psychology and the social
sciences. His model was a factor analysis model, but he considered the
noiseless ‘limit’ of the model. In other words he took <span
class="math inline">\(\sigma^2_i \rightarrow 0\)</span> so that he
had</p>
<p><span class="math display">\[
\mathbf{ y}_{i, :} \sim \lim_{\sigma^2 \rightarrow 0}
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right).
\]</span> The paper had two unfortunate effects. Firstly, the resulting
model is no longer valid probablistically, because the covariance of
this Gaussian is ‘degenerate’. Because <span
class="math inline">\(\mathbf{W}\mathbf{W}^\top\)</span> has rank of at
most <span class="math inline">\(q\)</span> where <span
class="math inline">\(q&lt;p\)</span> (due to the dimensionality
reduction) the determinant of the covariance is zero, meaning the
inverse doesn’t exist so the density, <span class="math display">\[
p(\mathbf{ y}_{i, :}|\mathbf{W}) =
\lim_{\sigma^2 \rightarrow 0} \frac{1}{(2\pi)^\frac{p}{2}
|\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\mathbf{ y}_{i, :}\left[\mathbf{W}\mathbf{W}^\top+
\sigma^2
\mathbf{I}\right]^{-1}\mathbf{ y}_{i, :}\right),
\]</span> is <em>not</em> valid for <span
class="math inline">\(q&lt;p\)</span> (where <span
class="math inline">\(\mathbf{W}\in \Re^{p\times q}\)</span>). This
mathematical consequence is a probability density which has no ‘support’
in large regions of the space for <span class="math inline">\(\mathbf{
y}_{i, :}\)</span>. There are regions for which the probability of <span
class="math inline">\(\mathbf{ y}_{i, :}\)</span> is zero. These are any
regions that lie off the hyperplane defined by mapping from <span
class="math inline">\(\mathbf{ z}\)</span> to <span
class="math inline">\(\mathbf{ y}\)</span> with the matrix <span
class="math inline">\(\mathbf{W}\)</span>. In factor analysis the noise
corruption, <span class="math inline">\(\boldsymbol{ \epsilon}\)</span>,
allows for points to be found away from the hyperplane. In Hotelling’s
PCA the noise variance is zero, so there is only support for points that
fall precisely on the hyperplane. Secondly, Hotelling explicity chose to
rename factor analysis as principal component analysis, arguing that the
factors social scientist sought were different in nature to the concept
of a mathematical factor. This was unfortunate because the factor
loadings, <span class="math inline">\(\mathbf{W}\)</span> can also be
seen as factors in the mathematical sense because the model Hotelling
defined is a Gaussian model with covariance given by <span
class="math inline">\(\mathbf{C}= \mathbf{W}\mathbf{W}^\top\)</span> so
<span class="math inline">\(\mathbf{W}\)</span> is a <em>factor</em> of
the covariance in the mathematical sense, as well as a factor
loading.</p>
<p>However, the paper had one great advantage over standard approaches
to factor analysis. Despite the fact that the model was a special case
that is subsumed by the more general approach of factor analysis it is
this special case that leads to a particular algorithm, namely that the
factor loadings (or principal components as Hotelling referred to them)
are given by an <em>eigenvalue decomposition</em> of the empirical
covariance matrix.</p>
<h2 id="computation-of-the-marginal-likelihood">Computation of the
Marginal Likelihood</h2>
<p><span class="math display">\[
\mathbf{ y}_{i,:}=\mathbf{W}\mathbf{ z}_{i,:}+\boldsymbol{
\epsilon}_{i,:},\quad \mathbf{ z}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{I}\right), \quad \boldsymbol{
\epsilon}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\sigma^{2}\mathbf{I}\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}\mathbf{ z}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{W}\mathbf{ z}_{i, :} + \boldsymbol{ \epsilon}_{i, :} \sim
\mathcal{N}\left(\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right)
\]</span></p>
<p><strong>Probabilistic PCA Max. Likelihood Soln</strong> (<span
class="citation" data-cites="Tipping:probpca99">Tipping and Bishop
(1999a)</span>)</p>
<div class="figure">
<div id="ppca-graph-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/ppca_graph.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="ppca-graph-magnify" class="magnify"
onclick="magnifyFigure(&#39;ppca-graph&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ppca-graph-caption" class="caption-frame">
<p>Figure: Graphical model representing probabilistic PCA.</p>
</div>
</div>
<p><span
class="math display">\[p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{
y}_{i,
:}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)\]</span></p>
<h2 id="eigenvalue-decomposition">Eigenvalue Decomposition</h2>
<p>Eigenvalue problems are widespreads in physics and mathematics, they
are often written as a matrix/vector equation but we prefer to write
them as a full matrix equation. In an eigenvalue problem you are looking
to find a matrix of eigenvectors, <span
class="math inline">\(\mathbf{U}\)</span> and a <em>diagonal</em> matrix
of eigenvalues, <span
class="math inline">\(\boldsymbol{\Lambda}\)</span> that satisfy the
<em>matrix</em> equation <span class="math display">\[
\mathbf{A}\mathbf{U} = \mathbf{U}\boldsymbol{\Lambda}.
\]</span> where <span class="math inline">\(\mathbf{A}\)</span> is your
matrix of interest. This equation is not trivially solvable through
matrix inverse because matrix multiplication is not <a
href="http://en.wikipedia.org/wiki/Commutative_property">commutative</a>,
so premultiplying by <span
class="math inline">\(\mathbf{U}^{-1}\)</span> gives <span
class="math display">\[
\mathbf{U}^{-1}\mathbf{A}\mathbf{U}
= \boldsymbol{\Lambda},
\]</span> where we remember that <span
class="math inline">\(\boldsymbol{\Lambda}\)</span> is a
<em>diagonal</em> matrix, so the eigenvectors can be used to
<em>diagonalise</em> the matrix. When performing the eigendecomposition
on a Gaussian covariances, diagonalisation is very important because it
returns the covariance to a form where there is no correlation between
points.</p>
<h2 id="positive-definite">Positive Definite</h2>
<p>We are interested in the case where <span
class="math inline">\(\mathbf{A}\)</span> is a covariance matrix, which
implies it is <em>positive definite</em>. A positive definite matrix is
one for which the inner product, <span class="math display">\[
\mathbf{ w}^\top \mathbf{C}\mathbf{ w}
\]</span> is positive for <em>all</em> values of the vector <span
class="math inline">\(\mathbf{ w}\)</span> other than the zero vector.
One way of creating a positive definite matrix is to assume that the
symmetric and positive definite matrix <span
class="math inline">\(\mathbf{C}\in \Re^{p\times p}\)</span> is
factorised into, <span class="math inline">\(\mathbf{A}in \Re^{p\times
p}\)</span>, a <em>full rank</em> matrix, so that <span
class="math display">\[
\mathbf{C}= \mathbf{A}^\top
\mathbf{A}.
\]</span> This ensures that <span
class="math inline">\(\mathbf{C}\)</span> must be positive definite
because <span class="math display">\[
\mathbf{ w}^\top \mathbf{C}\mathbf{ w}=\mathbf{ w}^\top
\mathbf{A}^\top\mathbf{A}\mathbf{ w}
\]</span> and if we now define a new <em>vector</em> <span
class="math inline">\(\mathbf{b}\)</span> as <span
class="math display">\[
\mathbf{b} = \mathbf{A}\mathbf{ w}
\]</span> we can now rewrite as <span class="math display">\[
\mathbf{ w}^\top \mathbf{C}\mathbf{ w}= \mathbf{b}^\top\mathbf{b} =
\sum_{i}
b_i^2
\]</span> which, since it is a sum of squares, is positive or zero. The
constraint that <span class="math inline">\(\mathbf{A}\)</span> must be
<em>full rank</em> ensures that there is no vector <span
class="math inline">\(\mathbf{ w}\)</span>, other than the zero vector,
which causes the vector <span class="math inline">\(\mathbf{b}\)</span>
to be all zeros.</p>
<h3 id="exercise-2">Exercise 2</h3>
<p>If <span class="math inline">\(\mathbf{C}=\mathbf{A}^\top
\mathbf{A}\)</span> then express <span
class="math inline">\(c_{i,j}\)</span>, the value of the element at the
<span class="math inline">\(i\)</span>th row and the <span
class="math inline">\(j\)</span>th column of <span
class="math inline">\(\mathbf{C}\)</span>, in terms of the columns of
<span class="math inline">\(\mathbf{A}\)</span>. Use this to show that
(i) the matrix is symmetric and (ii) the matrix has positive elements
along its diagonal.</p>
<h2 id="eigenvectors-of-a-symmetric-matric">Eigenvectors of a Symmetric
Matric</h2>
<p>Symmetric matrices have <em>orthonormal</em> eigenvectors. This means
that <span class="math inline">\(\mathbf{U}\)</span> is an <a
href="http://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal
matrix</a>, <span class="math inline">\(\mathbf{U}^\top\mathbf{U} =
\mathbf{I}\)</span>. This implies that <span
class="math inline">\(\mathbf{u}_{:, i} ^\top \mathbf{u}_{:, j}\)</span>
is equal to 0 if <span class="math inline">\(i\neq j\)</span> and 1 if
<span class="math inline">\(i=j\)</span>.</p>
<h2 id="principal-component-analysis-1">Principal Component
Analysis</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/principal-component-analysis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/principal-component-analysis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li><p>PCA (<span class="citation"
data-cites="Hotelling:analysis33">Hotelling (1933)</span>) is a linear
embedding.</p></li>
<li><p>Today its presented as:</p>
<ul>
<li>Rotate to find ‘directions’ in data with maximal variance.</li>
<li>How do we find these directions?</li>
</ul></li>
<li><p>Algorithmically we do this by diagonalizing the sample covariance
matrix <span class="math display">\[
\mathbf{S}=\frac{1}{n}\sum_{i=1}^n\left(\mathbf{ y}_{i, :}-\boldsymbol{
\mu}\right)\left(\mathbf{ y}_{i, :} - \boldsymbol{ \mu}\right)^\top
\]</span></p></li>
<li><p>Find directions in the data, <span class="math inline">\(\mathbf{
z}= \mathbf{U}\mathbf{ y}\)</span>, for which variance is
maximized.</p></li>
<li><p>Solution is found via constrained optimisation (which uses <a
href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange
multipliers</a>): <span class="math display">\[
L\left(\mathbf{u}_{1},\lambda_{1}\right)=\mathbf{u}_{1}^{\top}\mathbf{S}\mathbf{u}_{1}+\lambda_{1}\left(1-\mathbf{u}_{1}^{\top}\mathbf{u}_{1}\right)
\]</span></p></li>
<li><p>Gradient with respect to <span
class="math inline">\(\mathbf{u}_{1}\)</span> <span
class="math display">\[\frac{\text{d}L\left(\mathbf{u}_{1},\lambda_{1}\right)}{\text{d}\mathbf{u}_{1}}=2\mathbf{S}\mathbf{u}_{1}-2\lambda_{1}\mathbf{u}_{1}\]</span>
rearrange to form <span
class="math display">\[\mathbf{S}\mathbf{u}_{1}=\lambda_{1}\mathbf{u}_{1}.\]</span>
Which is known as an <a
href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><em>eigenvalue
problem</em></a>.</p></li>
<li><p>Further directions that are <em>orthogonal</em> to the first can
also be shown to be eigenvectors of the covariance.</p></li>
<li><p>Represent data, <span class="math inline">\(\mathbf{Y}\)</span>,
with a lower dimensional set of latent variables <span
class="math inline">\(\mathbf{Z}\)</span>.</p></li>
<li><p>Assume a linear relationship of the form <span
class="math display">\[
\mathbf{ y}_{i,:}=\mathbf{W}\mathbf{ z}_{i,:}+\boldsymbol{
\epsilon}_{i,:},
\]</span> where <span class="math display">\[
\boldsymbol{ \epsilon}_{i,:} \sim
\mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)
\]</span></p></li>
</ul>
<p><strong>Probabilistic PCA</strong></p>
<table>
<tr>
<td width>
<ul>
<li>Define <em>linear-Gaussian relationship</em> between latent
variables and data.</li>
<li><strong>Standard</strong> Latent variable approach:
<ul>
<li>Define Gaussian prior over <em>latent space</em>, <span
class="math inline">\(\mathbf{Z}\)</span>.</li>
</ul></li>
<li>Integrate out <em>latent variables</em>.
</td>
<td width="">
<div class="figure">
<div id="ppca-graph-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/ppca_graph.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="ppca-graph-magnify" class="magnify"
onclick="magnifyFigure(&#39;ppca-graph&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ppca-graph-caption" class="caption-frame">
<p>Figure: Graphical model representing probabilistic PCA.</p>
</div>
</div></li>
</ul>
<p><span class="math display">\[
p\left(\mathbf{Y}|\mathbf{Z},\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{
y}_{i,:}|\mathbf{W}\mathbf{ z}_{i,:},\sigma^2\mathbf{I}\right)
\]</span></p>
<p><span class="math display">\[
p\left(\mathbf{Z}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{
z}_{i,:}|\mathbf{0},\mathbf{I}\right)
\]</span></p>
<span class="math display">\[
p\left(\mathbf{Y}|\mathbf{W}\right)=\prod_{i=1}^{n}\mathcal{N}\left(\mathbf{
y}_{i,:}|\mathbf{0},\mathbf{W}\mathbf{W}^{\top}+\sigma^{2}\mathbf{I}\right)
\]</span>
</td>
</tr>
</table>
<h1 id="probabilistic-pca">Probabilistic PCA</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/probabilistic-pca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/probabilistic-pca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In 1997 <a
href="http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf">Tipping
and Bishop</a> <span class="citation"
data-cites="Tipping:pca97">(Tipping and Bishop, 1999b)</span> and <a
href="https://www.cs.nyu.edu/~roweis/papers/empca.pdf">Roweis</a> <span
class="citation" data-cites="Roweis:SPCA97">(Roweis, n.d.)</span>
independently revisited Hotelling’s model and considered the case where
the noise variance was finite, but <em>shared</em> across all output
dimensons. Their model can be thought of as a factor analysis where
<span class="math display">\[
\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}.
\]</span> This leads to a marginal likelihood of the form <span
class="math display">\[
p(\mathbf{Y}|\mathbf{W}, \sigma^2)
= \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i,
:}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right)
\]</span> where the limit of <span
class="math inline">\(\sigma^2\rightarrow 0\)</span> is <em>not</em>
taken. This defines a proper probabilistic model. Tippping and Bishop
then went on to prove that the <em>maximum likelihood</em> solution of
this model with respect to <span
class="math inline">\(\mathbf{W}\)</span> is given by an eigenvalue
problem. In the probabilistic PCA case the eigenvalues and eigenvectors
are given as follows. <span class="math display">\[
\mathbf{W}= \mathbf{U}\mathbf{L} \mathbf{R}^\top
\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is the
eigenvectors of the empirical covariance matrix <span
class="math display">\[
\mathbf{S} = \sum_{i=1}^n(\mathbf{ y}_{i, :} - \boldsymbol{
\mu})(\mathbf{ y}_{i,
:} - \boldsymbol{ \mu})^\top,
\]</span> which can be written <span class="math inline">\(\mathbf{S} =
\frac{1}{n} \mathbf{Y}^\top\mathbf{Y}\)</span> if the data is zero mean.
The matrix <span class="math inline">\(\mathbf{L}\)</span> is diagonal
and is dependent on the <em>eigenvalues</em> of <span
class="math inline">\(\mathbf{S}\)</span>, <span
class="math inline">\(\boldsymbol{\Lambda}\)</span>. If the <span
class="math inline">\(i\)</span>th diagonal element of this matrix is
given by <span class="math inline">\(\lambda_i\)</span> then the
corresponding element of <span class="math inline">\(\mathbf{L}\)</span>
is <span class="math display">\[
\ell_i = \sqrt{\lambda_i - \sigma^2}
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the
noise variance. Note that if <span
class="math inline">\(\sigma^2\)</span> is larger than any particular
eigenvalue, then that eigenvalue (along with its corresponding
eigenvector) is <em>discarded</em> from the solution.</p>
<h2 id="python-implementation-of-probabilistic-pca">Python
Implementation of Probabilistic PCA</h2>
<p>We will now implement this algorithm in python.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># probabilistic PCA algorithm</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppca(Y, q):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove mean</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Comute covariance</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> np.dot(Y_cent.T, Y_cent)<span class="op">/</span>Y.shape[<span class="dv">0</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    lambd, U <span class="op">=</span> np.linalg.eig(S)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Choose number of eigenvectors</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(lambd[q:])<span class="op">/</span>(Y.shape[<span class="dv">1</span>]<span class="op">-</span>q)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> np.sqrt(lambd[:q]<span class="op">-</span>sigma2)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> U[:, :q]<span class="op">*</span>l[<span class="va">None</span>, :]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W, sigma2</span></code></pre></div>
<p>In practice we may not wish to compute the eigenvectors of the
covariance matrix directly. This is because it requires us to estimate
the covariance, which involves a sum of squares term, before estimating
the eigenvectors. We can estimate the eigenvectors directly either
through <a href="http://en.wikipedia.org/wiki/QR_decomposition">QR
decomposition</a> or <a
href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular
value decomposition</a>. We saw a similar issue arise when , where we
also wished to avoid computation of <span
class="math inline">\(\mathbf{Z}^\top\mathbf{Z}\)</span> (or in the case
of <span
class="math inline">\(\boldsymbol{\Phi}^\top\boldsymbol{\Phi}\)</span>).</p>
<h1 id="posterior-for-principal-component-analysis">Posterior for
Principal Component Analysis</h1>
<p>Under the latent variable model justification for principal component
analysis, we are normally interested in inferring something about the
latent variables given the data. This is the distribution, <span
class="math display">\[
p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :})
\]</span> for any given data point. Determining this density turns out
to be very similar to the approach for determining the Bayesian
posterior of <span class="math inline">\(\mathbf{ w}\)</span> in
Bayesian linear regression, only this time we place the prior density
over <span class="math inline">\(\mathbf{ z}_{i, :}\)</span> instead of
<span class="math inline">\(\mathbf{ w}\)</span>. The posterior is
proportional to the joint density as follows, <span
class="math display">\[
p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) \propto p(\mathbf{ y}_{i,
:}|\mathbf{W}, \mathbf{ z}_{i, :}, \sigma^2) p(\mathbf{ z}_{i, :})
\]</span> And as in the Bayesian linear regression case we first
consider the log posterior, <span class="math display">\[
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) = \log p(\mathbf{ y}_{i,
:}|\mathbf{W},
\mathbf{ z}_{i, :}, \sigma^2) + \log p(\mathbf{ z}_{i, :}) +
\text{const}
\]</span> where the constant is not dependent on <span
class="math inline">\(\mathbf{ z}\)</span>. As before we collect the
quadratic terms in <span class="math inline">\(\mathbf{ z}_{i,
:}\)</span> and we assemble them into a Gaussian density over <span
class="math inline">\(\mathbf{ z}\)</span>. <span
class="math display">\[
\log p(\mathbf{ z}_{i, :} | \mathbf{ y}_{i, :}) =
-\frac{1}{2\sigma^2} (\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i,
:})^\top(\mathbf{ y}_{i, :} - \mathbf{W}\mathbf{ z}_{i, :}) -
\frac{1}{2}
\mathbf{ z}_{i, :}^\top \mathbf{ z}_{i, :} + \text{const}
\]</span></p>
<h3 id="exercise-3">Exercise 3</h3>
<p>Multiply out the terms in the brackets. Then collect the quadratic
term and the linear terms together. Show that the posterior has the form
<span class="math display">\[
\mathbf{ z}_{i, :} | \mathbf{W}\sim \mathcal{N}\left(\boldsymbol{
\mu}_x,\mathbf{C}_x\right)
\]</span> where <span class="math display">\[
\mathbf{C}_x = \left(\sigma^{-2}
\mathbf{W}^\top\mathbf{W}+ \mathbf{I}\right)^{-1}
\]</span> and <span class="math display">\[
\boldsymbol{ \mu}_x
= \mathbf{C}_x \sigma^{-2}\mathbf{W}^\top \mathbf{ y}_{i, :}
\]</span> Compare this to the posterior for the Bayesian linear
regression from last week, do they have similar forms? What matches and
what differs?</p>
<h2 id="python-implementation-of-the-posterior">Python Implementation of
the Posterior</h2>
<p>Now let’s implement the system in code.</p>
<h3 id="exercise-4">Exercise 4</h3>
<p>Use the values for <span class="math inline">\(\mathbf{W}\)</span>
and <span class="math inline">\(\sigma^2\)</span> you have computed,
along with the data set <span class="math inline">\(\mathbf{Y}\)</span>
to compute the posterior density over <span
class="math inline">\(\mathbf{Z}\)</span>. Write a function of the
form</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mu_x, C_x <span class="op">=</span> posterior(Y, W, sigma2)</span></code></pre></div>
<p>where <code>mu_x</code> and <code>C_x</code> are the posterior mean
and posterior covariance for the given <span
class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>Don’t forget to subtract the mean of the data <code>Y</code> inside
your function before computing the posterior: remember we assumed at the
beginning of our analysis that the data had been centred (i.e. the mean
was removed).</p>
<h2 id="numerically-stable-and-efficient-version">Numerically Stable and
Efficient Version</h2>
<p>Just as we saw for and computation of a matrix such as <span
class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> (or its centred
version) can be a bad idea in terms of loss of numerical accuracy.
Fortunately, we can find the eigenvalues and eigenvectors of the matrix
<span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span> without
direct computation of the matrix. This can be done with the <a
href="http://en.wikipedia.org/wiki/Singular_value_decomposition"><em>singular
value decomposition</em></a>. The singular value decompsition takes a
matrix, <span class="math inline">\(\mathbf{Z}\)</span> and represents
it in the form, <span class="math display">\[
\mathbf{Z} = \mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top
\]</span> where <span class="math inline">\(\mathbf{U}\)</span> is a
matrix of orthogonal vectors in the columns, meaning <span
class="math inline">\(\mathbf{U}^\top\mathbf{U} = \mathbf{I}\)</span>.
It has the same number of rows and columns as <span
class="math inline">\(\mathbf{Z}\)</span>. The matrices <span
class="math inline">\(\mathbf{\Lambda}\)</span> and <span
class="math inline">\(\mathbf{V}\)</span> are both square with
dimensionality given by the number of columns of <span
class="math inline">\(\mathbf{Z}\)</span>. The matrix <span
class="math inline">\(\mathbf{\Lambda}\)</span> is <em>diagonal</em> and
<span class="math inline">\(\mathbf{V}\)</span> is an orthogonal matrix
so <span class="math inline">\(\mathbf{V}^\top\mathbf{V} =
\mathbf{V}\mathbf{V}^\top = \mathbf{I}\)</span>. The eigenvalues of the
matrix <span class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span>
are then given by the singular values of the matrix <span
class="math inline">\(\mathbf{Y}^\top\)</span> squared and the
eigenvectors are given by <span
class="math inline">\(\mathbf{U}\)</span>.</p>
<h2 id="solution-for-mathbfw">Solution for <span
class="math inline">\(\mathbf{W}\)</span></h2>
<p>Given the singular value decomposition of <span
class="math inline">\(\mathbf{Y}\)</span> then we have <span
class="math display">\[
\mathbf{W}=
\mathbf{U}\mathbf{L}\mathbf{R}^\top
\]</span> where <span class="math inline">\(\mathbf{R}\)</span> is an
arbitrary rotation matrix. This implies that the posterior is given by
<span class="math display">\[
\mathbf{C}_x =
\left[\sigma^{-2}\mathbf{R}\mathbf{L}^2\mathbf{R}^\top +
\mathbf{I}\right]^{-1}
\]</span> because <span class="math inline">\(\mathbf{U}^\top \mathbf{U}
= \mathbf{I}\)</span>. Since, by convention, we normally take <span
class="math inline">\(\mathbf{R} = \mathbf{I}\)</span> to ensure that
the principal components are orthonormal we can write <span
class="math display">\[
\mathbf{C}_x = \left[\sigma^{-2}\mathbf{L}^2 +
\mathbf{I}\right]^{-1}
\]</span> which implies that <span
class="math inline">\(\mathbf{C}_x\)</span> is actually diagonal with
elements given by <span class="math display">\[
c_i = \frac{\sigma^2}{\sigma^2 + \ell^2_i}
\]</span> and allows us to write <span class="math display">\[
\boldsymbol{ \mu}_x = [\mathbf{L}^2 + \sigma^2
\mathbf{I}]^{-1} \mathbf{L} \mathbf{U}^\top \mathbf{ y}_{i, :}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}_x = \mathbf{D}\mathbf{U}^\top \mathbf{ y}_{i, :}
\]</span> where <span class="math inline">\(\mathbf{D}\)</span> is a
diagonal matrix with diagonal elements given by <span
class="math inline">\(d_{i} = \frac{\ell_i}{\sigma^2 +
\ell_i^2}\)</span>.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># probabilistic PCA algorithm using SVD</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppca(Y, q, center<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Probabilistic PCA through singular value decomposition&quot;&quot;&quot;</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove mean</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> center:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        Y_cent <span class="op">=</span> Y</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Comute singluar values, discard &#39;R&#39; as we will assume orthogonal</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    U, sqlambd, _ <span class="op">=</span> sp.linalg.svd(Y_cent.T,full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    lambd <span class="op">=</span> (sqlambd<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>Y.shape[<span class="dv">0</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute residual and extract eigenvectors</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(lambd[q:])<span class="op">/</span>(Y.shape[<span class="dv">1</span>]<span class="op">-</span>q)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    ell <span class="op">=</span> np.sqrt(lambd[:q]<span class="op">-</span>sigma2)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> U[:, :q], ell, sigma2</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior(Y, U, ell, sigma2, center<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Posterior computation for the latent variables given the eigendecomposition.&quot;&quot;&quot;</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> center:</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        Y_cent <span class="op">=</span> Y <span class="op">-</span> Y.mean(<span class="dv">0</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        Y_cent <span class="op">=</span> Y</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    C_x <span class="op">=</span> np.diag(sigma2<span class="op">/</span>(sigma2<span class="op">+</span>ell<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> ell<span class="op">/</span>(sigma2<span class="op">+</span>ell<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    mu_x <span class="op">=</span> np.dot(Y_cent, U)<span class="op">*</span>d[<span class="va">None</span>, :]</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu_x, C_x</span></code></pre></div>
<h2 id="examples-motion-capture-data">Examples: Motion Capture Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/mocap-ppca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/mocap-ppca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>For our first example we’ll consider some motion capture data of a
man breaking into a run. <a
href="http://en.wikipedia.org/wiki/Motion_capture">Motion capture
data</a> involves capturing a 3-d point cloud to represent a character,
often by an underlying skeleton. For this data set, from Ohio State
University, we have 54 frame of motion capture, each frame containing
102 values, which are the 3-d locations of 34 different points from the
subject’s skeleton.</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.osu_run1()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span></code></pre></div>
<p>Once the data is loaded in we can examine the first two principal
components as follows,</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>U, ell, sigma2 <span class="op">=</span> ppca(Y, q)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>mu_x, C_x <span class="op">=</span> posterior(Y, U, ell, sigma2)</span></code></pre></div>
<div class="figure">
<div id="dem-osu-run1-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_osu_run1.svg" width="70%" style=" ">
</object>
</div>
<div id="dem-osu-run1-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-osu-run1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-osu-run1-caption" class="caption-frame">
<p>Figure: First two principle components of motion capture data of an
individual running.</p>
</div>
</div>
<p>Here because the data is a time course, we have connected points that
are neighbouring in time. This highlights the form of the run, which
involves 3 paces. This projects in our low dimensional space to 3 loops.
We can examin how much residual variance there is in the system by
looking at <code>sigma2</code>.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sigma2)</span></code></pre></div>
<h2 id="robot-navigation-example">Robot Navigation Example</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/robot-wireless-ppca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/robot-wireless-ppca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In the next example we will load in data from a robot navigation
problem. The data consists of wireless access point strengths as
recorded by a robot performing a loop around the University of
Washington’s Computer Science department in Seattle. The robot records
all the wireless access points it can cache and stores their signal
strength.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.robot_wireless()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>Y.shape</span></code></pre></div>
<p>There are 215 observations of 30 different access points. In this
case the model is suggesting that the access point signal strength
should be linearly dependent on the location in the map. In other words
we are expecting the access point strength for the <span
class="math inline">\(j\)</span>th access point at robot position <span
class="math inline">\(x_{i, :}\)</span> to be represented by <span
class="math inline">\(y_{i, j} = \mathbf{ w}_{j, :}^\top \mathbf{ z}_{i,
:} + \epsilon_{i,j}\)</span>.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>U, ell, sigma2 <span class="op">=</span> ppca(Y.T, q)</span></code></pre></div>
<h1 id="interpretations-of-principal-component-analysis">Interpretations
of Principal Component Analysis</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-interpretations.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-interpretations.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="relationship-to-matrix-factorization">Relationship to Matrix
Factorization</h2>
<p>We can use the robot naviation example to realise that PCA (and
factor analysis) are very reminiscient of the that we used for
introducing objective functions. In that system we used slightly
different notation, <span class="math inline">\(\mathbf{u}_{i,
:}\)</span> for <em>user</em> location in our metaphorical library and
<span class="math inline">\(\mathbf{v}_{j, :}\)</span> for <em>item</em>
location in our metaphorical library. To see how these systems are
somewhat analagous, now let us think about the user as the robot and the
items as the wifi access points. We can plot the relative location of
both. This process is known as “SLAM”: simultaneous
<em>localisation</em> and <em>mapping</em>. A latent variable model of
the type we have developed is one way of performing SLAM. We have an
estimate of the <em>landmarks</em> in the system (in this case WIFI
access points) and we have an estimate of the robot position. These are
analagous to the estimate of the user’s position and the estimate of the
items positions in the library. In the matrix factorisation example
users are informing us what items they are ‘close’ to by expressing
their preferences, in the robot localization example the robot is
informing us what access point it is close to by measuring signal
strength.</p>
<p>From a personal perspective, I find this analogy quite comforting. I
think it is very arguable that one of the mechanisms through which we
(as humans) may have developed higher reasoning is through the need to
navigate around our environment, identifying landmarks and associating
them with our search for food. If such a system were to exist, the idea
that it could be readily adapted to other domains such as categorising
the nature of the different foodstuffs we were able to forage is
intriguing.</p>
<p>From an algorithmic perspective, we also can now realise that matrix
factorization and latent variable modelling are effectively the same
thing. The only difference is the objective function and our
probabilistic (or lack of probabilistic) treatment of the variables. But
the prediction function for both systems, <span class="math display">\[
f_{i, j} =
\mathbf{u}_{i, :}^\top \mathbf{v}_{j, :}
\]</span> for matrix factorization or <span class="math display">\[
f_{i, j} = \mathbf{ z}_{i, :}^\top \mathbf{ w}_{j, :}
\]</span> for probabilistic PCA and factor analysis are the same.</p>
<h2
id="other-interpretations-of-pca-separating-model-and-algorithm">Other
Interpretations of PCA: Separating Model and Algorithm</h2>
<p>Since Hotelling first introduced his perspective on factor analysis
as PCA there has been somewhat of a conflation of the idea of the model
underlying PCA (for which it was very clear that Hotelling was inspired
by Factor Analysis) and the algorithm that is used to fit that model:
the eigenvalues and eigenvectors of the covariance matrix. The
eigenvectors of an ellipsoid have been known since the middle of the
19th century as the principal axes of the elipsoid, and they arise
through the following additional ideas: seeking the orthogonal
directions of <em>maximum variance</em> in a dataset. Pearson in 1901
arrived at the same algorithm driven by a desire to seek a <em>symmetric
regression</em> between two covariate/response variables <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> <span class="citation"
data-cites="Pearson:01">(Pearson, 1901)</span>. He is, therefore, often
credited with the invention of principal component analysis, but to me
this seems disengenous. His aim was very different from Hotellings, it
was just happened that the optimal solution for his model was coincident
with that of Hotelling. The approach is also known as the <a
href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem">Karhunen
Loeve Transform</a> in stochastic process theory and in classical
multidimensional scaling the same operation can be shown to be
minimising a particular objective function based on interpoint distances
in the data and the latent space (see the section on Classical
Multidimensional Scaling in <a
href="http://store.elsevier.com/Multivariate-Analysis/Kanti-%20Mardia/isbn-9780124712522/">Mardia,
Kent and Bibby</a>) <span class="citation"
data-cites="Mardia:multivariate79">(Mardia et al., 1979)</span>. One of
my own contributions to machine learning was deriving yet another model
whose linear variant was solved by finding the principal subspace of the
covariance matrix (an approach I termed dual probabilistic PCA or
probabilistic principal coordinate analysis). Finally, the approach is
sometimes referred to simply as singular value decomposition (SVD). The
singular value decomposition of a data set has the following form, <span
class="math display">\[
\mathbf{Y}= \mathbf{V} \boldsymbol{\Lambda} \mathbf{U}^\top
\]</span> where <span class="math inline">\(\mathbf{V}\in\Re^{n\times
n}\)</span> and <span class="math inline">\(\mathbf{U}^\in \Re^{p\times
p}\)</span> are square orthogonal matrices and <span
class="math inline">\(\mathbf{\Lambda}^{n \times p}\)</span> is zero
apart from its first <span class="math inline">\(p\)</span> diagonal
entries. Singularvalue decomposition gives a diagonalisation of the
covariance matrix, because under the SVD we have <span
class="math display">\[
\mathbf{Y}^\top\mathbf{Y}=
\mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top\mathbf{V}
\boldsymbol{\Lambda}
\mathbf{U}^\top = \mathbf{U}\boldsymbol{\Lambda}^2 \mathbf{U}^\top
\]</span> where <span
class="math inline">\(\boldsymbol{\Lambda}^2\)</span> is now the
eigenvalues of the covariane matrix and <span
class="math inline">\(\mathbf{U}\)</span> are the eigenvectors. So
performing the SVD can simply be seen as another approach to determining
the principal components.</p>
<h2 id="separating-model-and-algorithm">Separating Model and
Algorithm</h2>
<p>I’ve given a fair amount of personal thought to this situation and my
own opinion that this confusion about method arises because of a
conflation of model and algorithm. The model of Hotelling, that which he
termed principal component analysis, was really a variant of factor
analysis, and it was unfortunate that he chose to rename it. However,
the algorithm he derived was a very convenient way of optimising a
(simplified) factor analysis, and it’s therefore become very popular.
The algorithm is also the optimal solution for many other models of the
data, even some which might seem initally to be unrelated (e.g. seeking
directions of maximum variance). It is only through the mathematics of
this linear system (which also contains some intersting symmetries) that
all these ides become related. However, as soon as we choose to
non-linearise the system (e.g. through basis functions) we find that
each of the non-linear intepretations we can derive for the different
models each leads to a very different algorithm (if such an algorithm is
possible). For example <a
href="http://web.stanford.edu/~hastie/Papers/Principal_Curves.pdf">principal
curves</a> of <span class="citation"
data-cites="Hastie:pcurves89">Hastie and Stuetzle (1989)</span> attempt
to non-linearise the maximum variance interpretation, <a
href="http://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel
PCA</a> of <span class="citation"
data-cites="Scholkopf:nonlinear98">Schölkopf et al. (1998)</span> uses
basis functions to form the eigenvalue problem in a nonlinear space, and
my own work in this area <a
href="http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf">non-linearises
the dual probabilistic PCA</a> <span class="citation"
data-cites="Lawrence:pnpca05">(Lawrence, 2005)</span>.</p>
<p>My conclusion is that when you are doing machine learning you should
always have it clear in your mind what your <em>model</em> is and what
your <em>algorithm</em> is. You can recognise your model because it
normally contains a prediction function and an objective function. The
algorithm on the other hand is the sequence of steps you implement on
the computer to solve for the parameters of this model. For efficient
implementation, we often modify our model to allow for faster
algorithms, and this is a perfectly valid pragmatist’s approach, so
conflation of model and algorithm is not always a bad thing. But for
clarity of thinking and understanding it is necessary to maintain the
separation and to maintain a handle on when and why we perform the
conflation.</p>
<h1 id="pca-in-practice">PCA in Practice</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/pca-in-practice.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/pca-in-practice.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Principal component analysis is so effective in practice that there
has almost developed a mini-industry in renaming the method itself
(which is ironic, given its origin). In particular <a
href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent
Semantic Indexing</a> in text processing is simply PCA on a particular
representation of the term frequencies of the document. There is a
particular fad to rename the eigenvectors after the nature of the data
you are examining, perhaps initially triggered by <a
href="http://www.face-rec.org/algorithms/PCA/jcn.pdf">Turk and
Pentland’s</a> paper on eigenfaces, but also with <a
href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1_2007_8/kuhn-%20junqua-eigenvoice-icslp1998.pdf">eigenvoices</a>
and <a
href="http://www.biomedcentral.com/1752-0509/1/54">eigengenes</a>. This
seems to be an instantiation of a wider, and hopefully subconcious,
tendency in academia to attempt to differentiate one idea from the same
idea in related fields in order to emphasise the novelty. The
unfortunate result is somewhat of a confusing literature for relatively
simple model. My recommendations would be as follows. If you have
multivariate data, applying some form of principal component would seem
to be a very good idea as a first step. Even if you intend to later
perform classification or regression on your data, it can give you
understanding of the structure of the underlying data and help you to
develop your intuitions about the nature of your data. Intelligent
plotting and interaction with your data is always a good think, and for
high dimensional data that means that you need some way of
visualisation, PCA is typically a good starting point.</p>
<h1 id="ppca-marginal-likelihood">PPCA Marginal Likelihood</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-marginal-likelihood.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-marginal-likelihood.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We have developed the posterior density over the latent variables
given the data and the parameters, and due to symmetries in the
underlying prediction function, it has a very similar form to its sister
density, the posterior of the weights given the data from Bayesian
regression. Two key differences are as follows. If we were to do a
Bayesian multiple output regression we would find that the marginal
likelihood of the data is independent across the features and correlated
across the data, <span class="math display">\[
p(\mathbf{Y}|\mathbf{Z})
= \prod_{j=1}^p \mathcal{N}\left(\mathbf{ y}_{:, j}|\mathbf{0},
\alpha\mathbf{Z}\mathbf{Z}^\top + \sigma^2 \mathbf{I}\right)
\]</span> where <span class="math inline">\(\mathbf{ y}_{:, j}\)</span>
is a column of the data matrix and the independence is across the
<em>features</em>, in probabilistic PCA the marginal likelihood has the
form, <span class="math display">\[
p(\mathbf{Y}|\mathbf{W}) = \prod_{i=1}^n\mathcal{N}\left(\mathbf{ y}_{i,
:}|\mathbf{0},\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right)
\]</span> where <span class="math inline">\(\mathbf{ y}_{i, :}\)</span>
is a row of the data matrix <span
class="math inline">\(\mathbf{Y}\)</span> and the independence is across
the data points.</p>
<h1 id="computation-of-the-log-likelihood">Computation of the Log
Likelihood</h1>
<p>The quality of the model can be assessed using the log likelihood of
this Gaussian form. <span class="math display">\[
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2} \log \left|
\mathbf{W}\mathbf{W}^\top + \sigma^2 \mathbf{I}\right| -\frac{1}{2}
\sum_{i=1}^n\mathbf{ y}_{i, :}^\top \left(\mathbf{W}\mathbf{W}^\top +
\sigma^2
\mathbf{I}\right)^{-1} \mathbf{ y}_{i, :} +\text{const}
\]</span> but this can be computed more rapidly by exploiting the low
rank form of the covariance covariance, <span
class="math inline">\(\mathbf{C}= \mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\)</span> and the fact that <span
class="math inline">\(\mathbf{W}=
\mathbf{U}\mathbf{L}\mathbf{R}^\top\)</span>. Specifically, we first use
the decomposition of <span class="math inline">\(\mathbf{W}\)</span> to
write: <span class="math display">\[
-\frac{n}{2} \log \left| \mathbf{W}\mathbf{W}^\top + \sigma^2
\mathbf{I}\right|
= -\frac{n}{2} \sum_{i=1}^q \log (\ell_i^2 + \sigma^2) -
\frac{n(p-q)}{2}\log
\sigma^2,
\]</span> where <span class="math inline">\(\ell_i\)</span> is the <span
class="math inline">\(i\)</span>th diagonal element of <span
class="math inline">\(\mathbf{L}\)</span>. Next, we use the <a
href="http://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury
matrix identity</a> which allows us to write the inverse as a quantity
which contains another inverse in a smaller matrix: <span
class="math display">\[
(\sigma^2 \mathbf{I}+ \mathbf{W}\mathbf{W}^\top)^{-1} =
\sigma^{-2}\mathbf{I}-\sigma^{-4}\mathbf{W}{\underbrace{(\mathbf{I}+\sigma^{-2}\mathbf{W}^\top\mathbf{W})}_{\mathbf{C}_x}}^{-1}\mathbf{W}^\top
\]</span> So, it turns out that the original inversion of the <span
class="math inline">\(p \times p\)</span> matrix can be done by forming
a quantity which contains the inversion of a <span
class="math inline">\(q \times q\)</span> matrix which, moreover, turns
out to be the quantity <span class="math inline">\(\mathbf{C}_x\)</span>
of the posterior.</p>
<p>Now, we put everything together to obtain: <span
class="math display">\[
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2} \sum_{i=1}^q
\log (\ell_i^2 + \sigma^2)
- \frac{n(p-q)}{2}\log \sigma^2 - \frac{1}{2}
\text{tr}\left(\mathbf{Y}^\top \left(
\sigma^{-2}\mathbf{I}-\sigma^{-4}\mathbf{W}\mathbf{C}_x
\mathbf{W}^\top \right) \mathbf{Y}\right) + \text{const},
\]</span> where we used the fact that a scalar sum can be written as
<span class="math inline">\(\sum_{i=1}^n\mathbf{ y}_{i,:}^\top
\mathbf{K}\mathbf{ y}_{i,:} = \text{tr}\left(\mathbf{Y}^\top
\mathbf{K}\mathbf{Y}\right)\)</span>, for any matrix <span
class="math inline">\(\mathbf{K}\)</span> of appropriate dimensions. We
now use the properties of the trace <span
class="math inline">\(\text{tr}\left(\mathbf{A}+\mathbf{B}\right)=\text{tr}\left(\mathbf{A}\right)+\text{tr}\left(\mathbf{B}\right)\)</span>
and <span class="math inline">\(\text{tr}\left(c \mathbf{A}\right) = c
\text{tr}\left(\mathbf{A}\right)\)</span>, where <span
class="math inline">\(c\)</span> is a scalar and <span
class="math inline">\(\mathbf{A},\mathbf{B}\)</span> matrices of
compatible sizes. Therefore, the final log likelihood takes the form:
<span class="math display">\[
\log p(\mathbf{Y}|\mathbf{W}) = -\frac{n}{2}
\sum_{i=1}^q \log (\ell_i^2 + \sigma^2) - \frac{n(p-q)}{2}\log \sigma^2
-
\frac{\sigma^{-2}}{2} \text{tr}\left(\mathbf{Y}^\top \mathbf{Y}\right)
+\frac{\sigma^{-4}}{2}
\text{tr}\left(\mathbf{B}\mathbf{C}_x\mathbf{B}^\top\right) +
\text{const}
\]</span> where we also defined <span
class="math inline">\(\mathbf{B}=\mathbf{Y}^\top\mathbf{W}\)</span>.
Finally, notice that <span
class="math inline">\(\text{tr}\left(\mathbf{Y}\mathbf{Y}^\top\right)=\text{tr}\left(\mathbf{Y}^\top\mathbf{Y}\right)\)</span>
can be computed faster as the sum of all the elements of <span
class="math inline">\(\mathbf{Y}\circ\mathbf{Y}\)</span>, where <span
class="math inline">\(\circ\)</span> denotes the element-wise (or <a
href="http://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard</a>)
product.</p>
<h2 id="reconstruction-of-the-data">Reconstruction of the Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-reconstruction.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-reconstruction.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Given any posterior projection of a data point, we can replot the
original data as a function of the input space.</p>
<p>We will now try to reconstruct the motion capture figure form some
different places in the latent plot.</p>
<h3 id="exercise-5">Exercise 5</h3>
<p>Project the motion capture data onto its principal components, and
then use the <em>mean posterior estimate</em> to reconstruct the data
from the latent variables at the data points. Use two latent dimensions.
What is the sum of squares error for the reconstruction?</p>
<h2 id="other-data-sets-to-explore">Other Data Sets to Explore</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/pca-other-data-sets.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/pca-other-data-sets.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Below there are a few other data sets from <code>pods</code> you
might want to explore with PCA. Both of them have <span
class="math inline">\(p\)</span>&gt;<span
class="math inline">\(n\)</span> so you need to consider how to do the
larger eigenvalue probleme efficiently without large demands on computer
memory.</p>
<p>The data is actually quite high dimensional, and solving the
eigenvalue problem in the high dimensional space can take some time. At
this point we turn to a neat trick, you don’t have to solve the full
eigenvalue problem in the <span class="math inline">\(p\times p\)</span>
covariance, you can choose instead to solve the related eigenvalue
problem in the <span class="math inline">\(n\times n\)</span> space, and
in this case <span class="math inline">\(n=200\)</span> which is much
smaller than <span class="math inline">\(p\)</span>.</p>
<p>The original eigenvalue problem has the form <span
class="math display">\[
\mathbf{Y}^\top\mathbf{Y}\mathbf{U} = \mathbf{U}\boldsymbol{\Lambda}
\]</span> But if we premultiply by <span
class="math inline">\(\mathbf{Y}\)</span> then we can solve, <span
class="math display">\[
\mathbf{Y}\mathbf{Y}^\top\mathbf{Y}\mathbf{U} =
\mathbf{Y}\mathbf{U}\boldsymbol{\Lambda}
\]</span> but it turns out that we can write <span
class="math display">\[
\mathbf{U}^\prime = \mathbf{Y}\mathbf{U} \Lambda^{\frac{1}{2}}
\]</span> where <span class="math inline">\(\mathbf{U}^\prime\)</span>
is an orthorormal matrix because <span class="math display">\[
\left.\mathbf{U}^\prime\right.^\top\mathbf{U}^\prime =
\Lambda^{-\frac{1}{2}}\mathbf{U}\mathbf{Y}^\top\mathbf{Y}\mathbf{U}
\Lambda^{-\frac{1}{2}}
\]</span> and since <span class="math inline">\(\mathbf{U}\)</span>
diagonalises <span
class="math inline">\(\mathbf{Y}^\top\mathbf{Y}\)</span>, <span
class="math display">\[
\mathbf{U}\mathbf{Y}^\top\mathbf{Y}\mathbf{U} = \Lambda
\]</span> then <span class="math display">\[
\left.\mathbf{U}^\prime\right.^\top\mathbf{U}^\prime = \mathbf{I}
\]</span></p>
<h2 id="olivetti-faces">Olivetti Faces</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/olivetti-eigenfaces.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/olivetti-eigenfaces.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>You too can create your own eigenfaces. In this example we load in
the ‘Olivetti Face’ data set, a small data set of 200 faces from the <a
href="http://en.wikipedia.org/wiki/Olivetti_Research_Laboratory">Olivetti
Research Laboratory</a>. Below we load in the data and display an image
of the second face in the data set (i.e., indexed by 1).</p>
<div class="figure">
<div id="olivetti-faces-data-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/olivetti_faces_data.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="olivetti-faces-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;olivetti-faces-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olivetti-faces-data-caption" class="caption-frame">
<p>Figure: Data set from Olivetti Research Laboratory of faces.</p>
</div>
</div>
<p>Note that to display the face we had to reshape the appropriate row
of the data matrix. This is because the images are turned into vectors
by stacking columns of the image on top of each other to form a vector.
The operation</p>
<p><code>im = np.reshape(Y[1, :].flatten(), (64, 64)).T}</code></p>
<p>recovers the original image into a matrix <code>im</code> by using
the <code>np.reshape</code> function to return the vector to a
matrix.</p>
<h2 id="visualizing-the-eigenvectors">Visualizing the Eigenvectors</h2>
<p>Each retained eigenvector is stored in the <span
class="math inline">\(j\)</span>th column of <span
class="math inline">\(\mathbf{U}\)</span>. Each of these eigenvectors is
associated with particular directions of variation in the original data.
Principal component analysis implies that we can reconstruct any face by
using a weighted sum of these eigenvectors where the weights for each
face are given by the relevant vector of the latent variables, <span
class="math inline">\(\mathbf{ z}_{i, :}\)</span> and the diagonal
elements of the matrix <span class="math inline">\(\mathbf{L}\)</span>.
We can visualize the eigenvectors <span
class="math inline">\(\mathbf{U}\)</span> as images by performing the
same reshape operation we used to recover the image associated with a
data point above. Below we do this for the first nine eigenvectors of
the Olivetti Faces data.</p>
<div class="figure">
<div id="dem-olivetti-faces-eigenvectors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_olivetti_faces_eigenvectors.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="dem-olivetti-faces-eigenvectors-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-olivetti-faces-eigenvectors&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-olivetti-faces-eigenvectors-caption" class="caption-frame">
<p>Figure: First 9 eigenvectors of the Olivetti faces data.</p>
</div>
</div>
<h2 id="reconstruction">Reconstruction</h2>
<p>We can now attempt to reconstruct a given training point from these
eigenvectors. As we mentioned above, the reconstruction is dependent on
the value of the latent variable and the weights from the matrix <span
class="math inline">\(\mathbf{L}\)</span>. First let’s compute the value
of the latent variables for the point we want to construct. Then we’ll
use them to compute the weightings of each of the eigenvectors.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>mu_x, C_x <span class="op">=</span> posterior(Y, U, ell, sigma2)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>reconstruction_weights <span class="op">=</span> mu_x[display_index, :]<span class="op">*</span>ell</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(reconstruction_weights)</span></code></pre></div>
<p>This vector of reconstruction weights is applied to the ‘template
images’ given by the eigenvectors to give us our reconstruction. Below
we weight these templates and combine to form the reconstructed image,
and show the comparison to the original image.</p>
<div class="figure">
<div id="dem-olivetti-faces-reconstruction-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//dimred/dem_olivetti_faces_reconstruction.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="dem-olivetti-faces-reconstruction-magnify" class="magnify"
onclick="magnifyFigure(&#39;dem-olivetti-faces-reconstruction&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dem-olivetti-faces-reconstruction-caption"
class="caption-frame">
<p>Figure: Reconstruction from the latent space of the faces data.</p>
</div>
</div>
<p>The quality of the reconstruction is a bit blurry, it can be improved
by increasing the number of template images used (i.e. increasing the
<em>latent dimensionality</em>).</p>
<h2 id="gene-expression">Gene Expression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/spellman-eigengenes.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/spellman-eigengenes.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Each of the cells in your body stores your entire genetic code in
your DNA, but at any one moment it is only ‘expressing’ a small portion
of that code. Your cells are mainly constructed of protein, and these
proteins are manufactured by first transcribing the DNA to RNA and then
translating the RNA to protein. If the DNA is the cells hard drive, then
one role of the RNA is to act like a cache of data that is being read
from the hard drive at any one time. Gene expression arrays allow us to
measure the quantity of different types of RNA in the cell, effectively
analyzing what’s in the cache (although we have to destroy the cell or
the tissue to access it). A gene expression experiment often consists of
a time course or a series of experiments that characterise the gene
expression of cells at any given time.</p>
<p>We will now load in one of the earliest gene expression data sets
from a <a href="http://www.ncbi.nlm.nih.gov/pubmed/9843569">1998 paper
by Spellman et al.</a>, it consists of gene expression measurements of
over six thousand genes in a range of conditions for brewer’s yeast. The
experiment was designed for understanding the cell cycle of the genes.
The expectation is that there should be oscillating signals inside the
cell.</p>
<p>First we extract the principal components of the gene expression.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in data and replace missing values with zero</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>pods.datasets.spellman_yeast_cdc15()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>].fillna(<span class="dv">0</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>U, ell, sigma2 <span class="op">=</span> ppca(Y, q)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>mu_x, C_x <span class="op">=</span> posterior(Y, U, ell, sigma2)</span></code></pre></div>
<p>Now, looking through, we find that there is indeed a latent variable
that appears to oscilate at approximately the right frequency. The 4th
latent dimension (<code>index=3</code>) can be plotted across the time
course as follows.</p>
<p>To reveal an oscillating shape. We can see which genes correspond to
this shape by examining the associated column of <span
class="math inline">\(\mathbf{U}\)</span>. Let’s augment our data matrix
with this score.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>gene_list <span class="op">=</span> Y.T</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>gene_list[<span class="st">&#39;oscilation&#39;</span>] <span class="op">=</span> np.sqrt(U[:, <span class="dv">3</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>gene_list.sort(columns<span class="op">=</span><span class="st">&#39;oscilation&#39;</span>, ascending<span class="op">=</span><span class="va">False</span>).index[:<span class="dv">4</span>]</span></code></pre></div>
<p>We can look up the first three genes in this list which now ranks the
genes according to how strongly they respond to the fourth latent
dimension. <a href="http://www.ncbi.nlm.nih.gov/gene/">The NCBI gene
database</a> allows us to search for the function of these genes.
Looking at the function of the four genes that respond most strongly to
the third latent variable they are all genes that encode <a
href="http://en.wikipedia.org/wiki/Histone">histone</a> proteins. The
histone is thesupport scaffold for the DNA that ensures it folds
correctly within the cell creating the nucleosomes. It seems to make
sense that production of histone proteins should be strongly correlated
with the cell cycle, as when the cell divides it needs to create a large
number of histone proteins for creating the duplicated nucleosomes. The
relevant links giving the descriptions of each gene given here: <a
href="http://www.ncbi.nlm.nih.gov/gene/851810">YDR224C</a>, <a
href="http://www.ncbi.nlm.nih.gov/gene/851811">YDR225W</a>, <a
href="http://www.ncbi.nlm.nih.gov/gene/852283">YBL003C</a> and <a
href="http://www.ncbi.nlm.nih.gov/gene/855701">YNL030W</a>.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li>Chapter 7 up to pg 249 of <span class="citation"
data-cites="Rogers:book11">Rogers and Girolami (2011)</span></li>
</ul>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Hastie:pcurves89" class="csl-entry" role="listitem">
Hastie, T., Stuetzle, W., 1989. Principal curves. Journal of the
American Statistical Association 84, 502–516.
</div>
<div id="ref-Hotelling:analysis33" class="csl-entry" role="listitem">
Hotelling, H., 1933. Analysis of a complex of statistical variables into
principal components. Journal of Educational Psychology 24, 417–441.
</div>
<div id="ref-Lawrence:pnpca05" class="csl-entry" role="listitem">
Lawrence, N.D., 2005. Probabilistic non-linear principal component
analysis with <span>G</span>aussian process latent variable models.
Journal of Machine Learning Research 6, 1783–1816.
</div>
<div id="ref-LeCun:zip89" class="csl-entry" role="listitem">
Le Cun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E.,
Hubbard, W., Jackel, L.D., 1989. Backpropagation applied to handwritten
zip code recognition. Neural Computation 1, 541–551. <a
href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a>
</div>
<div id="ref-Mardia:multivariate79" class="csl-entry" role="listitem">
Mardia, K.V., Kent, J.T., Bibby, J.M., 1979. Multivariate analysis.
Academic Press, London.
</div>
<div id="ref-Ng:spectral02" class="csl-entry" role="listitem">
Ng, A.Y., Jordan, M.I., Weiss, Y., n.d. On spectral clustering: Analysis
and an algorithm.
</div>
<div id="ref-Pearson:01" class="csl-entry" role="listitem">
Pearson, K., 1901. On lines and planes of closest fit to systems of
points in space. The London, Edinburgh and Dublin Philosophical Magazine
and Journal of Science, Sixth Series 2, 559–572.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="listitem">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC
Press.
</div>
<div id="ref-Roweis:SPCA97" class="csl-entry" role="listitem">
Roweis, S.T., n.d. <span>EM</span> algorithms for <span>PCA</span> and
<span>SPCA</span>. pp. 626–632.
</div>
<div id="ref-Scholkopf:nonlinear98" class="csl-entry" role="listitem">
Schölkopf, B., Smola, A., Müller, K.-R., 1998. Nonlinear component
analysis as a kernel eigenvalue problem. Neural Computation 10,
1299–1319. <a
href="https://doi.org/10.1162/089976698300017467">https://doi.org/10.1162/089976698300017467</a>
</div>
<div id="ref-Shi:normalized00" class="csl-entry" role="listitem">
Shi, J., Malik, J., 2000. Normalized cuts and image segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelligence 22, 888–905.
</div>
<div id="ref-Tipping:pca97" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999b. Mixtures of probabilistic principal
component analysers. Neural Computation 11, 443–482.
</div>
<div id="ref-Tipping:probpca99" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999a. Probabilistic principal component
analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a
href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>These models are quite a lot more complex than the
simple clustering we describe here. They represent a common ancestor
through a cluster center that is then allowed to evolve over time
through a mutation rate. The time of separation between different
species is estimated via these mutation rates.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

