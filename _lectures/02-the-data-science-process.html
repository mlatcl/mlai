---
title: "The Data Science Process"
abstract: "<p>In this session we look at some of what makes the data
science process different from classical computer science.</p>"
edit_url: https://github.com/mlatcl/mlai/edit/gh-pages/_lamd/the-data-science-process.md
week: 2
featured_image: slides/diagrams/data-science/new-flow-of-information003.svg
reveal: 02-the-data-science-process.slides.html
ipynb: 02-the-data-science-process.ipynb
pptx: 02-the-data-science-process.pptx
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="evolved-relationship-with-information">Evolved Relationship with
Information</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/evolved-relationship.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/evolved-relationship.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The high bandwidth of computers has resulted in a close relationship
between the computer and data. Large amounts of information can flow
between the two. The degree to which the computer is mediating our
relationship with data means that we should consider it an
intermediary.</p>
<p>Originally our low bandwidth relationship with data was affected by
two characteristics. Firstly, our tendency to over-interpret driven by
our need to extract as much knowledge from our low bandwidth information
channel as possible. Secondly, by our improved understanding of the
domain of <em>mathematical</em> statistics and how our cognitive biases
can mislead us.</p>
<p>With this new set up there is a potential for assimilating far more
information via the computer, but the computer can present this to us in
various ways. If its motives are not aligned with ours then it can
misrepresent the information. This needn’t be nefarious it can be simply
because of the computer pursuing a different objective from us. For
example, if the computer is aiming to maximize our interaction time that
may be a different objective from ours which may be to summarize
information in a representative manner in the <em>shortest</em> possible
length of time.</p>
<p>For example, for me, it was a common experience to pick up my
telephone with the intention of checking when my next appointment was,
but to soon find myself distracted by another application on the phone
and end up reading something on the internet. By the time I’d finished
reading, I would often have forgotten the reason I picked up my phone in
the first place.</p>
<p>There are great benefits to be had from the huge amount of
information we can unlock from this evolved relationship between us and
data. In biology, large scale data sharing has been driven by a
revolution in genomic, transcriptomic and epigenomic measurement. The
improved inferences that can be drawn through summarizing data by
computer have fundamentally changed the nature of biological science,
now this phenomenon is also influencing us in our daily lives as data
measured by <em>happenstance</em> is increasingly used to characterize
us.</p>
<p>Better mediation of this flow requires a better understanding of
human-computer interaction. This in turn involves understanding our own
intelligence better, what its cognitive biases are and how these might
mislead us.</p>
<p>For further thoughts see Guardian article on <a
href="https://www.theguardian.com/media-network/2015/jul/23/data-driven-economy-marketing">marketing
in the internet era</a> from 2015.</p>
<p>You can also check my blog post on <a
href="http://inverseprobability.com/2015/12/04/what-kind-of-ai">System
Zero</a>. This was also written in 2015.</p>
<h2 id="new-flow-of-information">New Flow of Information</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classically the field of statistics focused on mediating the
relationship between the machine and the human. Our limited bandwidth of
communication means we tend to over-interpret the limited information
that we are given, in the extreme we assign motives and desires to
inanimate objects (a process known as anthropomorphizing). Much of
mathematical statistics was developed to help temper this tendency and
understand when we are valid in drawing conclusions from data.</p>
<div class="figure">
<div id="new-flow-of-information-3-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/new-flow-of-information003.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-3-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-3-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<p>Data science brings new challenges. In particular, there is a very
large bandwidth connection between the machine and data. This means that
our relationship with data is now commonly being mediated by the
machine. Whether this is in the acquisition of new data, which now
happens by happenstance rather than with purpose, or the interpretation
of that data where we are increasingly relying on machines to summarize
what the data contains. This is leading to the emerging field of data
science, which must not only deal with the same challenges that
mathematical statistics faced in tempering our tendency to over
interpret data but must also deal with the possibility that the machine
has either inadvertently or maliciously misrepresented the underlying
data.</p>
<h2 id="embodiment-factors">Embodiment Factors</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
<div id="embodiment-factors-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;embodiment-factors-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="embodiment-factors-table-caption" class="caption-frame">
<p>Figure: Embodiment factors are the ratio between our ability to
compute and our ability to communicate. Relative to the machine we are
also locked in. In the table we represent embodiment as the length of
time it would take to communicate one second’s worth of computation. For
computers it is a matter of minutes, but for a human, it is a matter of
thousands of millions of years. See also “Living Together: Mind and
Machine Intelligence” <span class="citation"
data-cites="Lawrence:embodiment17">Lawrence (2017a)</span></p>
</div>
</div>
<p>There is a fundamental limit placed on our intelligence based on our
ability to communicate. Claude Shannon founded the field of information
theory. The clever part of this theory is it allows us to separate our
measurement of information from what the information pertains to.<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></p>
<p>Shannon measured information in bits. One bit of information is the
amount of information I pass to you when I give you the result of a coin
toss. Shannon was also interested in the amount of information in the
English language. He estimated that on average a word in the English
language contains 12 bits of information.</p>
<p>Given typical speaking rates, that gives us an estimate of our
ability to communicate of around 100 bits per second <span
class="citation" data-cites="Reed-information98">(Reed and Durlach,
1998)</span>. Computers on the other hand can communicate much more
rapidly. Current wired network speeds are around a billion bits per
second, ten million times faster.</p>
<p>When it comes to compute though, our best estimates indicate our
computers are slower. A typical modern computer can process make around
100 billion floating-point operations per second, each floating-point
operation involves a 64 bit number. So the computer is processing around
6,400 billion bits per second.</p>
<p>It’s difficult to get similar estimates for humans, but by some
estimates the amount of compute we would require to <em>simulate</em> a
human brain is equivalent to that in the UK’s fastest computer <span
class="citation" data-cites="Ananthanarayanan-cat09">(Ananthanarayanan
et al., 2009)</span>, the MET office machine in Exeter, which in 2018
ranked as the 11th fastest computer in the world. That machine simulates
the world’s weather each morning, and then simulates the world’s climate
in the afternoon. It is a 16-petaflop machine, processing around 1,000
<em>trillion</em> bits per second.</p>
<div class="figure">
<div id="lotus-49-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//Lotus_49-2.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="lotus-49-magnify" class="magnify"
onclick="magnifyFigure(&#39;lotus-49&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lotus-49-caption" class="caption-frame">
<p>Figure: The Lotus 49, view from the rear. The Lotus 49 was one of the
last Formula One cars before the introduction of aerodynamic aids.</p>
</div>
</div>
<p>So, when it comes to our ability to compute we are extraordinary, not
compute in our conscious mind, but the underlying neuron firings that
underpin both our consciousness, our subconsciousness as well as our
motor control etc.</p>
<p>If we think of ourselves as vehicles, then we are massively
overpowered. Our ability to generate derived information from raw fuel
is extraordinary. Intellectually we have formula one engines.</p>
<p>But in terms of our ability to deploy that computation in actual use,
to share the results of what we have inferred, we are very limited. So,
when you imagine the F1 car that represents a psyche, think of an F1 car
with bicycle wheels.</p>
<div class="figure">
<div id="marcel-renault-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//640px-Marcel_Renault_1903.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="marcel-renault-magnify" class="magnify"
onclick="magnifyFigure(&#39;marcel-renault&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="marcel-renault-caption" class="caption-frame">
<p>Figure: Marcel Renault races a Renault 40 cv during the Paris-Madrid
race, an early Grand Prix, in 1903. Marcel died later in the race after
missing a warning flag for a sharp corner at Couhé Vérac, likely due to
dust reducing visibility.</p>
</div>
</div>
<p>Just think of the control a driver would have to have to deploy such
power through such a narrow channel of traction. That is the beauty and
the skill of the human mind.</p>
<p>In contrast, our computers are more like go-karts. Underpowered, but
with well-matched tires. They can communicate far more fluidly. They are
more efficient, but somehow less extraordinary, less beautiful.</p>
<div class="figure">
<div id="caleb-mcduff-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//Caleb_McDuff_WIX_Silence_Racing_livery.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="caleb-mcduff-magnify" class="magnify"
onclick="magnifyFigure(&#39;caleb-mcduff&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="caleb-mcduff-caption" class="caption-frame">
<p>Figure: Caleb McDuff driving for WIX Silence Racing.</p>
</div>
</div>
<p>For humans, that means much of our computation should be dedicated to
considering <em>what</em> we should compute. To do that efficiently we
need to model the world around us. The most complex thing in the world
around us is other humans. So, it is no surprise that we model them. We
second guess what their intentions are, and our communication is only
necessary when they are departing from how we model them. Naturally, for
this to work well, we need to understand those we work closely with. It
is no surprise that social communication, social bonding, forms so much
of a part of our use of our limited bandwidth.</p>
<p>There is a second effect here, our need to anthropomorphize objects
around us. Our tendency to model our fellow humans extends to when we
interact with other entities in our environment. To our pets as well as
inanimate objects around us, such as computers or even our cars. This
tendency to over interpret could be a consequence of our limited ability
to communicate.<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a></p>
<p>For more details see this paper <a
href="https://arxiv.org/abs/1705.07996">“Living Together: Mind and
Machine Intelligence”</a>, and this <a
href="http://inverseprobability.com/talks/lawrence-tedx17/living-together.html">TEDx
talk</a> and Chapter 1 in <span class="citation"
data-cites="Lawrence-atomic24">Lawrence (2024)</span>.</p>
<!--




## Societal Effects

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/societal-effects.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/societal-effects.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>




We have already seen the effects of this changed dynamic in biology and computational biology. Improved sensorics have led to the new domains of transcriptomics, epigenomics, and 'rich phenomics' as well as considerably augmenting our capabilities in genomics. 

Biologists have had to become data-savvy, they require a rich understanding of the available data resources and need to assimilate existing data sets in their hypothesis generation as well as their experimental design. Modern biology has become a far more quantitative science, but the quantitativeness has required new methods developed in the domains of *computational biology* and *bioinformatics*.

There is also great promise for personalized health, but in health the wide data-sharing that has underpinned success in the computational biology community is much harder to carry out. 





We can expect to see these phenomena reflected in wider society. Particularly as we make use of more automated decision making based only on data. This is leading to a requirement to better understand our own subjective biases to ensure that the human to computer interface allows domain experts to assimilate data driven conclusions in a well calibrated manner. This is particularly important where medical treatments are being prescribed. It also offers potential for different kinds of medical intervention. More subtle interventions are possible when the digital environment is able to respond to users in an bespoke manner. This has particular implications for treatment of mental health conditions.




The main phenomenon we see across the board is the shift in dynamic from the direct pathway between human and data, as traditionally mediated by classical statistics, to a new flow of information via the computer. This change of dynamics gives us the modern and emerging domain of *data science*, where the interactions between human and data are mediated by the machine.


-->
<p>We can characterize the challenges for integrating machine learning
within our systems as the three Ds. Decomposition, Data and
Deployment.</p>
<p>You can also check my blog post on blog post on <a
href="http://inverseprobability.com/2018/11/05/the-3ds-of-machine-learning-systems-design">The
3Ds of Machine Learning Systems Design</a>..</p>
<p>The first two components <em>decomposition</em> and <em>data</em> are
interlinked, but we will first outline the decomposition challenge.
Below we will mainly focus on <em>supervised learning</em> because this
is arguably the technology that is best understood within machine
learning.</p>
<!--



## Decomposition

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-decomposition-challenge.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-decomposition-challenge.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>






Machine learning is not magical pixie dust, we cannot simply automate all decisions through data. We are constrained by our data (see below) and the models we use.[^tribal]  Machine learning models are relatively simple function mappings that include characteristics such as smoothness. With some famous exceptions, e.g. speech and image data, inputs are constrained in the form of vectors and the model consists of a mathematically well-behaved function. This means that some careful thought has to be put in to the right sub-process to automate with machine learning. This is the challenge of *decomposition* of the machine learning system.

[^tribal]: We can also become constrained by our tribal thinking, just as each of the other groups can.



Any repetitive task is a candidate for automation, but many of the repetitive tasks we perform as humans are more complex than any individual algorithm can replace. The selection of which task to automate becomes critical and has downstream effects on our overall system design.

Our approach to complex system design is separation of concerns, decompose the large scale system into smaller parts, each of which can be managed by a separate team.

### Pigeonholing





<div class="figure">
<div class="figure-frame" id="too-many-pigeons2-figure">
<div class="centered centered" style=""><img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//TooManyPigeons.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></div>
</div>
<div class="magnify" id="too-many-pigeons2-magnify" onclick="magnifyFigure('too-many-pigeons2')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="too-many-pigeons2-caption">
Figure: The machine learning systems decomposition process calls for separating a complex task into decomposable separate entities. A process we can think of as <a href="https://en.wikipedia.org/wiki/Pigeonholing" target="_blank">pigeonholing</a>.
</div>
</div>




Some aspects to take into account are

1.  Can we refine the decision we need to a set of repetitive tasks
    where input information and output decision/value is well defined?
2.  Can we represent each sub-task we’ve defined with a mathematical
    mapping?

The representation necessary for the second aspect may involve massaging
of the problem: feature selection or adaptation. It may also involve
filtering out exception cases (perhaps through a pre-classification).

All else being equal, we’d like to keep our models simple and
interpretable. If we can convert a complex mapping to a linear mapping
through clever selection of sub-tasks and features this is a big win.

For example, Facebook have *feature engineers*, individuals whose main
role is to design features they think might be useful for one of their
tasks (e.g. newsfeed ranking, or ad matching). Facebook have a
training/testing pipeline called
[FBLearner](https://www.facebook.com/Engineering/posts/fblearner-flow-is-a-machine-learning-platform-capable-of-easily-reusing-algorith/10154077833317200/).
Facebook have predefined the sub-tasks they are interested in, and they
are tightly connected to their business model.

It is easier for Facebook to do this because their business model is
heavily focused on user interaction. A challenge for companies that have
a more diversified portfolio of activities driving their business is the
identification of the most appropriate sub-task. A potential solution to
feature and model selection is known as *AutoML* [@Feurer:automl15]. Or we
can think of it as using Machine Learning to assist Machine Learning.
It’s also called meta-learning. Learning about learning. The input to
the ML algorithm is a machine learning task, the output is a proposed
model to solve the task.



One trap that is easy to fall in is too much emphasis on the type of model we have deployed rather than the appropriateness of the task decomposition we
have chosen.

**Recommendation**: Conditioned on task decomposition, we should
automate the process of model improvement. Model updates should not be
discussed in management meetings, they should be deployed and updated as
a matter of course. Further details below on model deployment, but model
updating needs to be considered at design time. This is the domain of
AutoML.



<div class="figure">
<div class="figure-frame" id="chicken-and-egg2-figure">
<div class="centered centered" style=""><img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//ai/chicken-and-egg.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></div>
</div>
<div class="magnify" id="chicken-and-egg2-magnify" onclick="magnifyFigure('chicken-and-egg2')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="chicken-and-egg2-caption">
Figure: The answer to the question which comes first, the chicken or the egg is simple, they co-evolve [@Popper:conjectures63]. Similarly, when we place components together in a complex machine learning system, they will tend to co-evolve and compensate for one another.
</div>
</div>




To form modern decision-making systems, many components are interlinked.  We decompose our complex decision making into individual tasks, but the performance of each component is dependent on those upstream of it.

This naturally leads to co-evolution of systems; upstream errors can be
compensated by downstream corrections.

To embrace this characteristic, end-to-end training could be considered. Why produce the best forecast by metrics when we can just produce the best forecast for our systems? End-to-end training can lead to improvements in performance, but it would also damage our systems decomposability and its interpretability, and perhaps its adaptability.

Poor systems decomposition, and a lack of interpretability can compound challenges around *intellectual debt*.

The less human interpretable our systems are, the harder they are to adapt to different circumstances or diagnose when there's a challenge.  The trade-off between interpretability and performance is a constant tension which we should always retain in our minds when performing our system design.


-->
<h2 id="data">Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-data-challenge.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-data-challenge.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>It is difficult to overstate the importance of data. It is half of
the equation for machine learning but is often utterly neglected. We can
speculate that there are two reasons for this. Firstly, data cleaning is
perceived as tedious. It doesn’t seem to consist of the same
intellectual challenges that are inherent in constructing complex
mathematical models and implementing them in code. Secondly, data
cleaning is highly complex, it requires a deep understanding of how
machine learning systems operate and good intuitions about the data
itself, the domain from which data is drawn (e.g. Supply Chain) and what
downstream problems might be caused by poor data quality.</p>
<p>A consequence of these two reasons, data cleaning seems difficult to
formulate into a readily teachable set of principles. As a result, it is
heavily neglected in courses on machine learning and data science.
Despite data being half the equation, most University courses spend
little to no time on its challenges.</p>
<h2 id="the-data-crisis">The Data Crisis</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/the-data-crisis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/the-data-crisis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Anecdotally, talking to data modelling scientists. Most say they
spend 80% of their time acquiring and cleaning data. This is
precipitating what I refer to as the “data crisis”. This is an analogy
with software. The “software crisis” was the phenomenon of inability to
deliver software solutions due to increasing complexity of
implementation. There was no single shot solution for the software
crisis, it involved better practice (scrum, test orientated development,
sprints, code review), improved programming paradigms (object
orientated, functional) and better tools (CVS, then SVN, then git).</p>
<p>However, these challenges aren’t new, they are merely taking a
different form. From the computer’s perspective software <em>is</em>
data. The first wave of the data crisis was known as the <em>software
crisis</em>.</p>
<h3 id="the-software-crisis">The Software Crisis</h3>
<p>In the late sixties early software programmers made note of the
increasing costs of software development and termed the challenges
associated with it as the “<a
href="https://en.wikipedia.org/wiki/Software_crisis">Software
Crisis</a>”. Edsger Dijkstra referred to the crisis in his 1972 Turing
Award winner’s address.</p>
<blockquote>
<p>The major cause of the software crisis is that the machines have
become several orders of magnitude more powerful! To put it quite
bluntly: as long as there were no machines, programming was no problem
at all; when we had a few weak computers, programming became a mild
problem, and now we have gigantic computers, programming has become an
equally gigantic problem.</p>
<p>Edsger Dijkstra (1930-2002), The Humble Programmer</p>
</blockquote>
<blockquote>
<p>The major cause of the data crisis is that machines have become more
interconnected than ever before. Data access is therefore cheap, but
data quality is often poor. What we need is cheap high-quality data.
That implies that we develop processes for improving and verifying data
quality that are efficient.</p>
<p>There would seem to be two ways for improving efficiency. Firstly, we
should not duplicate work. Secondly, where possible we should automate
work.</p>
</blockquote>
<p>What I term “The Data Crisis” is the modern equivalent of this
problem. The quantity of modern data, and the lack of attention paid to
data as it is initially “laid down” and the costs of data cleaning are
bringing about a crisis in data-driven decision making. This crisis is
at the core of the challenge of <em>technical debt</em> in machine
learning <span class="citation" data-cites="Sculley:debt15">(Sculley et
al., 2015)</span>.</p>
<p>Just as with software, the crisis is most correctly addressed by
‘scaling’ the manner in which we process our data. Duplication of work
occurs because the value of data cleaning is not correctly recognised in
management decision making processes. Automation of work is increasingly
possible through techniques in “artificial intelligence”, but this will
also require better management of the data science pipeline so that data
about data science (meta-data science) can be correctly assimilated and
processed. The Alan Turing institute has a program focussed on this
area, <a
href="https://www.turing.ac.uk/research_projects/artificial-intelligence-data-analytics/">AI
for Data Analytics</a>.</p>
<p>Data is the new software, and the data crisis is already upon us. It
is driven by the cost of cleaning data, the paucity of tools for
monitoring and maintaining our deployments, the provenance of our models
(e.g. with respect to the data they’re trained on).</p>
<p>Three principal changes need to occur in response. They are cultural
and infrastructural.</p>
<h3 id="the-data-first-paradigm">The Data First Paradigm</h3>
<p>First of all, to excel in data driven decision making we need to move
from a <em>software first</em> paradigm to a <em>data first</em>
paradigm. That means refocusing on data as the product. Software is the
intermediary to producing the data, and its quality standards must be
maintained, but not at the expense of the data we are producing. Data
cleaning and maintenance need to be prized as highly as software
debugging and maintenance. Instead of <em>software</em> as a service, we
should refocus around <em>data</em> as a service. This first change is a
cultural change in which our teams think about their outputs in terms of
data. Instead of decomposing our systems around the software components,
we need to decompose them around the data generating and consuming
components.<a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a> Software first is only an
intermediate step on the way to becoming <em>data first</em>. It is a
necessary, but not a sufficient condition for efficient machine learning
systems design and deployment. We must move from <em>software orientated
architecture</em> to a <em>data orientated architecture</em>.</p>
<h3 id="data-quality">Data Quality</h3>
<p>Secondly, we need to improve our language around data quality. We
cannot assess the costs of improving data quality unless we generate a
language around what data quality means.
<!--Data Readiness Levels[^data-readiness-levels] are an assessment of data quality that is based on the usage to which data is
put.

[^data-readiness-levels]: [Data Readiness Levels](http://inverseprobability.com/b2017/01/12/data-readiness-levels) [@Lawrence-drl17] are an attempt to develop a language around data quality that can bridge the gap between technical solutions and decision makers such as managers and project planners. They are inspired by Technology Readiness Levels which attempt to quantify the readiness of technologies for deployment.--></p>
<h3 id="data-readiness-levels">Data Readiness Levels</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-readiness-levels-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-readiness-levels-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a
href="http://inverseprobability.com/2017/01/12/data-readiness-levels">Data
Readiness Levels</a> <span class="citation"
data-cites="Lawrence-drl17">(Lawrence, 2017b)</span> are an attempt to
develop a language around data quality that can bridge the gap between
technical solutions and decision makers such as managers and project
planners. They are inspired by Technology Readiness Levels which attempt
to quantify the readiness of technologies for deployment.</p>
<p>See this blog post on <a
href="http://inverseprobability.com/2017/01/12/data-readiness-levels">Data
Readiness Levels</a>.</p>
<h3 id="three-grades-of-data-readiness">Three Grades of Data
Readiness</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/three-grades-of-data-readiness.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/three-grades-of-data-readiness.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Data-readiness describes, at its coarsest level, three separate
stages of data graduation.</p>
<ul>
<li>Grade C - accessibility
<ul>
<li>Transition: data becomes electronically available</li>
</ul></li>
<li>Grade B - validity
<ul>
<li>Transition: pose a question to the data.</li>
</ul></li>
<li>Grade A - usability</li>
</ul>
<p>The important definitions are at the transition. The move from Grade
C data to Grade B data is delimited by the <em>electronic
availability</em> of the data. The move from Grade B to Grade A data is
delimited by posing a question or task to the data <span
class="citation" data-cites="Lawrence-drl17">(Lawrence,
2017b)</span>.</p>
<p><strong>Recommendation</strong>: Build a shared understanding of the
language of data readiness levels for use in planning documents and
costing of data cleaning and the benefits of reusing cleaned data.</p>
<h3 id="move-beyond-software-engineering-to-data-engineering">Move
Beyond Software Engineering to Data Engineering</h3>
<p>Thirdly, we need to improve our mental model of the separation of
data science from applied science. A common trap in our thinking around
data is to see data science (and data engineering, data preparation) as
a sub-set of the software engineer’s or applied scientist’s skill set.
As a result, we recruit and deploy the wrong type of resource. Data
preparation and question formulation is superficially similar to both
because of the need for programming skills, but the day to day problems
faced are very different.</p>
<h2 id="combining-data-and-systems-design">Combining Data and Systems
Design</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-combining-data-and-systems-design-challenge.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-combining-data-and-systems-design-challenge.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="data-science-as-debugging">Data Science as Debugging</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-science-as-debugging.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-science-as-debugging.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One challenge for existing information technology professionals is
realizing the extent to which a software ecosystem based on data differs
from a classical ecosystem. In particular, by ingesting data we bring
unknowns/uncontrollables into our decision-making system. This presents
opportunity for adversarial exploitation and unforeseen operation.</p>
<p>blog post on <a
href="http://inverseprobability.com/2017/03/14/data-science-as-debugging">Data
Science as Debugging</a>.</p>
<p>Starting with the analysis of a data set, the nature of data science
is somewhat difference from classical software engineering.</p>
<p>One analogy I find helpful for understanding the depth of change we
need is the following. Imagine as a software engineer, you find a USB
stick on the ground. And for some reason you <em>know</em> that on that
USB stick is a particular API call that will enable you to make a
significant positive difference on a business problem. You don’t know
which of the many library functions on the USB stick are the ones that
will help. And it could be that some of those library functions will
hinder, perhaps because they are just inappropriate or perhaps because
they have been placed there maliciously. The most secure thing to do
would be to <em>not</em> introduce this code into your production system
at all. But what if your manager told you to do so, how would you go
about incorporating this code base?</p>
<p>The answer is <em>very</em> carefully. You would have to engage in a
process more akin to debugging than regular software engineering. As you
understood the code base, for your work to be reproducible, you should
be documenting it, not just what you discovered, but how you discovered
it. In the end, you typically find a single API call that is the one
that most benefits your system. But more thought has been placed into
this line of code than any line of code you have written before.</p>
<p>An enormous amount of debugging would be required. As the nature of
the code base is understood, software tests to verify it also need to be
constructed. At the end of all your work, the lines of software you
write to interact with the software on the USB stick are likely to be
minimal. But more thought would be put into those lines than perhaps any
other lines of code in the system.</p>
<p>Even then, when your API code is introduced into your production
system, it needs to be deployed in an environment that monitors it. We
cannot rely on an individual’s decision making to ensure the quality of
all our systems. We need to create an environment that includes quality
controls, checks, and bounds, tests, all designed to ensure that
assumptions made about this foreign code base are remaining valid.</p>
<p>This situation is akin to what we are doing when we incorporate data
in our production systems. When we are consuming data from others, we
cannot assume that it has been produced in alignment with our goals for
our own systems. Worst case, it may have been adversarially produced. A
further challenge is that data is dynamic. So, in effect, the code on
the USB stick is evolving over time.</p>
<p>It might see that this process is easy to formalize now, we simply
need to check what the formal software engineering process is for
debugging, because that is the current software engineering activity
that data science is closest to. But when we look for a formalization of
debugging, we find that there is none. Indeed, modern software
engineering mainly focusses on ensuring that code is written without
bugs in the first place.</p>
<h3 id="lessons">Lessons</h3>
<ol type="1">
<li>When you begin an analysis, behave as a debugger.</li>
</ol>
<ul>
<li>Write test code as you go. Document those tests and ensure they are
accessible by others.</li>
<li>Understand the landscape of your data. Be prepared to try several
different approaches to the data set.</li>
<li>Be constantly skeptical.</li>
<li>Use the best tools available, develop a deep understand how they
work.</li>
<li>Share your experience of what challenges you’re facing. Have others
(software engineers, fellow data analysts, your manager) review your
work.</li>
<li>Never go straight for the goal: you’d never try and write the API
call straight away on the discarded hard drive, so why are you launching
your classification algorithm before visualizing the data?</li>
<li>Ensure your analysis is documented and accessible. If your code does
go wrong in production, you’ll need to be able to retrace to where the
error crept in.</li>
</ul>
<ol start="2" type="1">
<li>When managing the data science process, don’t treat it as standard
code development.</li>
</ol>
<ul>
<li>Don’t deploy a traditional agile development pipeline and expect it
to work the same way it does for standard code development. Think about
how you handle bugs, think about how you would handle very many
bugs.</li>
<li>Don’t leave the data scientist alone to wade through the mess.</li>
<li>Integrate the data analysis with your other team activities. Have
the software engineers and domain experts work closely with the data
scientists. This is vital for providing the data scientists with the
technical support they need, but also managing the expectations of the
engineers in terms of when and how the data will be able to
deliver.</li>
</ul>
<p><strong>Recommendation</strong>: Anecdotally, resolving a machine
learning challenge requires 80% of the resource to be focused on the
data and perhaps 20% to be focused on the model. But many companies are
too keen to employ machine learning engineers who focus on the models,
not the data. We should change our hiring priorities and training.
Universities cannot provide the understanding of how to data-wrangle.
Companies must fill this gap.</p>
<div class="figure">
<div id="derwent-valley-resevoir-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/water-bridge-hill-transport-arch-calm-544448-pxhere.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="derwent-valley-resevoir-magnify" class="magnify"
onclick="magnifyFigure(&#39;derwent-valley-resevoir&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="derwent-valley-resevoir-caption" class="caption-frame">
<p>Figure: A reservoir of data has more value if the data is consumable.
The data crisis can only be addressed if we focus on outputs rather than
inputs.</p>
</div>
</div>
<div class="figure">
<div id="lake-district-stream-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/1024px-Lake_District_picture.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="lake-district-stream-magnify" class="magnify"
onclick="magnifyFigure(&#39;lake-district-stream&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lake-district-stream-caption" class="caption-frame">
<p>Figure: For a data first architecture we need to clean our data at
source, rather than individually cleaning data for each task. This
involves a shift of focus from our inputs to our outputs. We should
provide data streams that are consumable by many teams without
purification.</p>
</div>
</div>
<p><strong>Recommendation</strong>: We need to share best practice
around data deployment across our teams. We should make best use of our
processes where applicable, but we need to develop them to become
<em>data first</em> organizations. Data needs to be cleaned at
<em>output</em> not at <em>input</em>.</p>
<h2 id="gdpr-origins">GDPR Origins</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_governance/includes/gdpr-origins.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_governance/includes/gdpr-origins.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>There’s been much recent talk about GDPR, much of it implying that
the recent incarnation is radically different from previous
incarnations. While the most recent iteration began to be developed in
2012, but in reality, its origins are much older. <a
href="https://en.wikipedia.org/wiki/Convention_for_the_protection_of_individuals_with_regard_to_automatic_processing_of_personal_data">It
dates back to 1981</a>, and <a
href="https://en.wikipedia.org/wiki/Data_Privacy_Day">28th January is
“Data Potection day”</a>. The essence of the law didn’t change much in
the previous iterations. The critical chance was the size of the fines
that the EU stipulated may be imposed for infringements. Paul Nemitz,
who was closely involved with the drafting, told me that they were
initially inspired by competition law, which levies fines of 10% of
international revenue. The final implementation is restricted to 5%, but
it’s worth pointing out that Facebook’s fine (imposed in the US by the
FTC) was $5 billion dollars. Or approximately 7% of their international
revenue at the time.</p>
<p>So the big change is the seriousness with which regulators are taking
breaches of the intent of GDPR. And indeed, this newfound will on behalf
of the EU led to an amount of panic around companies who rushed to see
if they were complying with this strengthened legislation.</p>
<p>But is it really the big bad regulator coming down hard on the poor
scientist or company, just trying to do an honest day’s work? I would
argue not. The stipulations of the GDPR include fairly simple things
like the ‘right to an explanation’ for consequential decision-making. Or
the right to deletion, to remove personal private data from a corporate
data ecosystem.</p>
<p>Guardian article on <a
href="https://www.theguardian.com/media-network/2015/mar/05/digital-oligarchy-algorithms-personal-data">Digital
Oligarchies</a></p>
<p>While these are new stipulations, if you reverse the argument and ask
a company “would it not be a good thing if you could explain why your
automated decision making system is making decision X about customer Y”
seems perfectly reasonable. Or “Would it not be a good thing if we knew
that we were capable of deleting customer Z’s data from our systems,
rather than being concerned that it may be lying unregistered in an S3
bucket somewhere?”.</p>
<p>Phrased in this way, you can see that GDPR perhaps would better stand
for “Good Data Practice Rules”, and should really be being adopted by
the scientist, the company or whoever in an effort to respect the rights
of the people they aim to serve.</p>
<p>So how do Data Trusts fit into this landscape? Well it’s appropriate
that we’ve mentioned the commons, because a current challenge is how we
manage data rights within our community. And the situation is rather
akin to that which one might have found in a feudal village (in the days
before Houndkirk Moor was enclosed).</p>
<h1 id="how-the-gdpr-may-help">How the GDPR May Help</h1>
<div class="figure">
<div id="convention-108-coe-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/convention-108-coe.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="convention-108-coe-magnify" class="magnify"
onclick="magnifyFigure(&#39;convention-108-coe&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="convention-108-coe-caption" class="caption-frame">
<p>Figure: The convention for the protection of individuals with regard
to the processing of personal data was opened for signature on 28th
January 1981. It was the first legally binding international instrument
in the field of data protection.</p>
</div>
</div>
<p>Early reactions to the General Data Protection Regulation by
companies seem to have been fairly wary, but if we view the principles
outlined in the GDPR as good practice, rather than regulation, it feels
like companies can only improve their internal data ecosystems by
conforming to the GDPR. For this reason, I like to think of the initials
as standing for “Good Data Practice Rules” rather than General Data
Protection Regulation. In particular, the term “data protection” is a
misnomer, and indeed the earliest <a
href="https://en.wikipedia.org/wiki/Convention_for_the_protection_of_individuals_with_regard_to_automatic_processing_of_personal_data">data
protection directive from the EU</a> (from 1981) refers to the
protection of <em>individuals</em> with regard to the automatic
processing of personal data, which is a much better sense of the
term.</p>
<p>If we think of the legislation as protecting individuals, and we note
that it seeks, and instead of viewing it as regulation, we view it as
“Wouldn’t it be good if …”, e.g. in respect to the <a
href="https://en.wikipedia.org/wiki/Right_to_explanation">“right to an
explanation”</a>, we might suggest: “Wouldn’t it be good if we could
explain why our automated decision making system made a particular
decison”. That seems like good practice for an organization’s automated
decision making systems.</p>
<p>Similarly, with regard to data minimization principles. Retaining the
minimum amount of personal data needed to drive decisions could well
lead to <em>better</em> decision making as it causes us to become
intentional about which data is used rather than the sloppier thinking
that “more is better” encourages. Particularly when we consider that to
be truly useful data has to be cleaned and maintained.</p>
<p>If GDPR is truly reflecting the interests of individuals, then it is
also reflecting the interests of consumers, patients, users etc, each of
whom make use of these systems. For any company that is customer facing,
or any service that prides itself on the quality of its delivery to
those individuals, “good data practice” should become part of the DNA of
the organization.</p>
<h2 id="gdpr-in-practice">GDPR in Practice</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_governance/includes/gdpr-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_governance/includes/gdpr-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Need to understand why you are processing personal data, for example
see the ICO’s <a
href="https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/lawful-basis-for-processing/">Lawful
Basis Guidance</a> and their <a
href="https://ico.org.uk/for-organisations/gdpr-resources/lawful-basis-interactive-guidance-tool/">Lawful
Basis Guidance Tool</a>.</p>
<p>For websites, if you are processing personal data you will need a
privacy policy to be in place. See the ICO’s <a
href="https://ico.org.uk/for-organisations/make-your-own-privacy-notice/">Make
your own privacy notice</a> site which also provides a template.</p>
<!--



## Deployment

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-deployment-challenge.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/ml-deployment-challenge.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>





Much of the academic machine learning systems point of view is based on a software systems point of view that is around 20 years out of date. In particular we build machine learning models on fixed training data sets, and we test them on stationary test data sets. 

In practice modern software systems involve continuous deployment of models into an ever-evolving world of data. These changes are indicated in the software world by greater availability of technologies like *streaming* technologies.

### Continuous Deployment






Once the decomposition is understood, the data is sourced and the models
are created, the model code needs to be deployed.



To extend the USB stick analogy further, how would as software engineer deploy the code if they thought that the code might evolve in production? This is what
data does. We cannot assume that the conditions under which we trained
our model will be retained as we move forward, indeed the only constant
we have is change.

This means that when any data dependent model is deployed into
production, it requires *continuous monitoring* to ensure the
assumptions of design have not been invalidated. Software changes are
qualified through testing, in particular a regression test ensures that
existing functionality is not broken by change. Since data is
continually evolving, machine learning systems require 'continual
regression testing': oversight by systems that ensure their existing
functionality has not been broken as the world evolves around them. An
approach we refer to as *progression testing*. Unfortunately, standards
around ML model deployment yet been developed. The modern world of
continuous deployment does rely on testing, but it does not recognize
the continuous evolution of the world around us.

Progression tests are likely to be *statistical* tests in contrast to classical software tests. The tests should be monitoring model performance and quality measures. They could also monitor conformance to standardized *fairness* measures.




If the world has changed around our decision-making ecosystem, how are we alerted to those changes?

**Recommendation**: We establish best practice around model deployment.
We need to shift our culture from standing up a software service, to
standing up a *data as a service*. Data as a Service would involve
continual monitoring of our deployed models in production. This would be
regulated by 'hypervisor' systems[^emulation] that understand the context in
which models are deployed and recognize when circumstances have changed,
and models need retraining or restructuring.

[^emulation]: Emulation, or surrogate modelling, is one very promising approach to forming such a hypervisor. Emulators are models we fit to other models, often simulations, but the could also be other machine learning models. These models operate at the meta-level, not on the systems directly. This means they can be used to model how the sub-systems interact. As well as emulators we should consider real time dash boards, anomaly detection, mutlivariate analysis, data visualization and classical statistical approaches for hypervision of our deployed systems.













## Data Oriented Architectures

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-oriented-architectures-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-oriented-architectures-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>



In a streaming architecture we shift from management of services, to management of data streams. Instead of worrying about availability of the services we shift to worrying about the quality of the data those services are producing.




Historically we've been *software first*, this is a necessary but insufficient condition for *data first*. We need to move from software-as-a-service to data-as-a-service, from service oriented architectures to *data oriented architectures*.

## Streaming System





Characteristics of a streaming system include a move from *pull* updates to *push* updates, i.e. the computation is driven by a change in the input data rather than the service calling for input data when it decides to run a computation. Streaming systems operate on 'rows' of the data rather than 'columns'. This is because the full column isn't normally available as it changes over time. As an important design principle, the services themselves are stateless, they take their state from the streaming ecosystem. This ensures the inputs and outputs of given computations are easy to declare. As a result, persistence of the data is also handled by the streaming ecosystem and decisions around data retention or recomputation can be taken at the systems level rather than the component level.





**Recommendation**: We should consider a major re-architecting of systems around our services. In particular we should scope the use of a *streaming architecture* (such as Apache Kafka) that ensures data persistence and enables asynchronous operation of our systems.[^data-orientated-architecture] This would enable the provision of QC streams, and real time dash boards as well as hypervisors.

[^data-orientated-architecture]: These approaches are one area of focus for my own team's research. A data first architecture is a prerequisite for efficient deployment of machine learning systems.

Importantly a streaming architecture implies the services we build are
*stateless*, internal state is deployed on streams alongside external
state. This allows for rapid assessment of other services' data.



The philosphy of DOA is also possible with more standard data infrastructures, such as SQL data bases, but more work has to be put into place to ensure that book-keeping around data provenance and origin is stored, as well as approaches for snapshotting the data ecosystem.








## Apache Flink

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/apache-flink.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/apache-flink.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>




[Apache Flink](https://en.wikipedia.org/wiki/Apache_Flink) is a stream processing framework. Flink is a foundation for event driven processing. This gives a high throughput and low latency framework that operates on dataflows.

Data storage is handled by other systems such as Apache Kafka or AWS Kinesis.



```
stream.join(otherStream)
    .where(<KeySelector>)
    .equalTo(<KeySelector>)
    .window(<WindowAssigner>)
    .apply(<JoinFunction>)
```

Apache Flink allows operations on streams. For example, the join operation above. In a traditional data base management system, this join operation may be written in SQL and called on demand. In a streaming ecosystem, computations occur as and when the streams update.

The join is handled by the ecosystem surrounding the business logic.








## Milan

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/milan.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/milan.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>




To answer these challenges at Amazon we began the process of constructing software for data oriented architectures. The team built a *data-oriented programming* language which is
[now available through MIT license](https://github.com/amzn/milan). The language is called Milan.



<div class="centered " style="">

<svg viewBox="0 0 200 200" style="width:15%">
<defs>
<clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/>
</clipPath>
</defs><title>Tom Borchert</title><image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/mlai/./slides/diagrams//people/tom-borchert.jpg" clip-path="url(#clip0)"/></svg></div>

The Principle Engineer behind the Milan architecture has been Tom Borchert.Quoting from [Tom Borchert's blog
on Milan](https://tborchertblog.wordpress.com/2020/02/13/28/):

> Milan has three components:
>
> 1.  A general-purpose stream algebra that encodes relationships between
>      data streams (the Milan Intermediate Language or Milan IL)
>
> 2.  A Scala library for building programs in that algebra.
>
> 3.  A compiler that takes programs expressed in Milan IL and produces a
>     Flink application that executes the program.
>
> Component (2) can be extended to support interfaces in additional
> languages, and component (3) can be extended to support additional
> runtime targets. Considering just the multiple interfaces and the
> multiple runtimes, Milan looks a lot like the much more mature Apache
> Beam. The difference lies in (1), Milan's general-purpose stream
> algebra.
}









<div class="figure">
<div class="figure-frame" id="milan-schematic-figure">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//software/milan-schematic.svg" width="80%" style=" "></object>
</div>
<div class="magnify" id="milan-schematic-magnify" onclick="magnifyFigure('milan-schematic')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="milan-schematic-caption">
Figure: The Milan Software has a general purpose stream algebra at its core, the Milan IL.
</div>
</div>



<div class="figure">
<div class="figure-frame" id="milan-software-page-figure">
<div class="centered " style=""><img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//software/milan.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></div>
</div>
<div class="magnify" id="milan-software-page-magnify" onclick="magnifyFigure('milan-software-page')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="milan-software-page-caption">
Figure: The Milan Software is designed for building modern AI systems. <https://github.com/amzn/milan/>
</div>
</div>

It is through the general-purpose stream algebra that we hope to make
significant inroads on the intellectual debt challenge.

The stream algebra defines the relationship between different machine
learning components in the wider software architecture. Composition of
multiple services cannot occur without a signature existing within the
stream algebra. The Milan IL becomes the key information structure that
is required to reason about the wider software system.

## Context





This deals with the challenges that arise through the intellectual debt  because we can now see the context around each service. This
allows us to design the relevant validation checks to ensure that
accuracy and fairness are maintained. By recompiling the algebra to
focus on a particular decision within the system we can also derive new
statistical tests to validate performance. These are the checks that we
refer to as progression testing. The loss of programmer control means
that we can no longer rely on software tests written at design time, we
must have the capability to deploy new (statistical) tests after
deployment as the uses to which each service is placed extend to
previously un-envisaged domains.

## Stateless Services





Importantly, Milan does not place onerous constraints on the builders of
individual machine learning models (or other components). Standard
modelling frameworks can be used. The main constraint is that any code
that is not visible to the ecosystem does not maintain or store global
state. This condition implies that the parameters of any machine
learning model need to also be declared as an input to the model within
the Milan IL.

## Meta Modelling





<div class="figure">
<div class="figure-frame" id="emukit-software-page-figure">
<div class="centered " style=""><img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//uq/emukit-software-page.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></div>
</div>
<div class="magnify" id="emukit-software-page-magnify" onclick="magnifyFigure('emukit-software-page')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="emukit-software-page-caption">
Figure: The Emukit software is a set of software tools for emulation and surrogate modeling. <https://amzn.github.io/emukit/>
</div>
</div>

Where does machine learning come in? The strategy I propose is that the
Milan IL is integrated with meta-modelling approaches to assist in the
explanation of the decision-making framework. At their simplest these
approaches may be novelty detection algorithms on the data streams that
are emerging from a given service. This is a form of *progression
testing*. But we can go much further. By knowing the training data, the
inputs and outputs of the individual services in the software ecosystem,
we can build meta-models that test for fairness, accuracy not just of
individual system components, but short or long cascades of decision
making. Through the use of the Milan IL algebra all these tests could be
automatically deployed. The focus of machine learning is on the
models-that-model-the-models. The meta-models.

In Amazon, our own focus was on the use of statistical emulators,
sometimes known as surrogate models, for fulfilling this task. The work
we were putting into this route is available through another software
package, [Emukit, a framework for decision making under
uncertainty](https://amzn.github.io/emukit/). With collaborators my
current focus for addressing these issues is a form of fusion of Emukit
and Milan (Milemukit??). But the nature of this fusion requires testing
on real world problem sets. A task we hope to carry out in close
collaboration with colleagues at [Data Science
Africa](http://www.datascienceafrica.org/).









## Trading System

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/hypothetical-trading-system.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/hypothetical-trading-system.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>




As a simple example we'll consider a high frequency trading system. Anne wishes to build a share trading system. She has access to a high frequency trading system which provides prices and allows trades at millisecond intervals. She wishes to build an automated trading system.

Let's assume that price trading data is available as a data stream. But the price now is not the only information that Anne needs, she needs an estimate of the price in the future.












mlai.write_figure('hypothetical-prices.svg', directory='./data-science/')











<div class="figure">
<div class="figure-frame" id="hypothetical-prices-figure">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/hypothetical-prices.svg" width="80%" style=" "></object>
</div>
<div class="magnify" id="hypothetical-prices-magnify" onclick="magnifyFigure('hypothetical-prices')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="hypothetical-prices-caption">
Figure: Anne has access to the share prices in the black stream but not in the blue stream. A hypothetical stream is the stream of future prices. Anne can define this hypothetical under constraints (latency, input etc). The need for a model is now exposed in the software infrastructure
</div>
</div>

## Hypothetical Streams








We'll call the future price a hypothetical stream.

A hypothetical stream is a desired stream of information which cannot be directly accessed. The lack of direct access may be because the events happen in the future, or there may be some latency between the event and the availability of the data.

Any hypothetical stream will only be provided as a prediction, ideally with an error bar.

The nature of the hypothetical Anne needs is dependent on her decision-making process. In Anne's case it will depend over what period she is expecting her returns. In MDOP Anne specifies a hypothetical that is derived from the pricing stream. 

It is not the price stream directly, but Anne looks for *future* predictions from the price stream, perhaps for price in $T$ days' time.

At this stage, this stream is merely typed as a hypothetical.

There are constraints on the hypothetical, they include: the *input* information, the upper limit of latency between input and prediction, and the decision Anne needs to make (how far ahead, what her upside, downside risks are). These three constraints mean that we can only recover an approximation to the hypothetical.

## Hypothetical Advantage







What is the advantage to defining things in this way? By defining, clearly, the two streams as real and hypothetical variants of each other, we now enable automation of the deployment and any redeployment process. The hypothetical can be *instantiated* against the real, and design criteria can be constantly evaluated triggering retraining when necessary.









## SafeBoda

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/safe-boda.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/safe-boda.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>



The complexity of building safe, maintainable systems that are based on interacting components which include machine learning models means that smaller companies can be excluded from access to these technologies due the technical and intellectual debt incurred when maintaining such systems in a real-world environment.

<div class="figure">
<div class="figure-frame" id="safe-boda-system-figure">
<div class="centered " style=""><img class="" src="https://mlatcl.github.io/mlai/./slides/diagrams//ai/safe-boda.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></div>
</div>
<div class="magnify" id="safe-boda-system-magnify" onclick="magnifyFigure('safe-boda-system')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="safe-boda-system-caption">
Figure: SafeBoda is a ride allocation system for Boda Boda drivers. Let's imagine the capabilities we need for such an AI system.
</div>
</div>

[SafeBoda](https://safeboda.com/ug/index.php#whysafeboda) is a Kampala based rider allocation system for Boda Boda drivers. Boda boda are motorcycle taxis which give employment to, often young men, across Kampala. Safe Boda is driven by the knowledge that road accidents are set to match HIV/AIDS as the highest cause of death in low/middle income families by 2030. 



> With road accidents set to match HIV/AIDS as the highest cause of death in low/middle income countries by 2030, SafeBoda’s aim is to modernise informal transportation and ensure safe access to mobility.

A key aim of the AutoAI agenda is to reduce these technical challenges, so that such software can be maintained safely and reliably by a small team of software engineers. Without this capability it is hard to imagine how low resource environments can fully benefit from the 'data revolution' without heavy reliance on technical provision from high-resource environments. Such dependence would inevitably mean a skew towards the challenges that high-resource economies face, rather than the more urgent and important problems that are faced in low-resource environments.











<div class="figure">
<div class="figure-frame" id="ride-allocation-system-figure">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//ai/ride-allocation-prediction.svg" width="60%" style=" "></object>
</div>
<div class="magnify" id="ride-allocation-system-magnify" onclick="magnifyFigure('ride-allocation-system')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="ride-allocation-system-caption">
Figure: Some software components in a ride allocation system. Circled components are hypothetical, rectangles represent actual data.
</div>
</div>





Let's consider a ride sharing app, for example the SafeBoda system. 

Anne is on her way home now; she wishes to hail a car using a ride sharing app. 

The app is designed in the following way. On opening her app Anne is notified about drivers in the nearby neighborhood. She is given an estimate of the time a ride may take to come.

Given this information about driver availability, Anne may feel encouraged to enter a destination. Given this destination, a price estimate can be given. This price is conditioned on other riders that may wish to go in the same direction, but the price estimate needs to be made before the user agrees to the ride. 

Business customer service constraints dictate that this price may not change after Anne's order is confirmed. 

In this simple system, several decisions are being made, each of them on the basis of a hypothetical.

When Anne calls for a ride, she is provided with an estimate based on the expected time a ride can be with her. But this estimate is made without knowing where Anne wants to go. There are constraints on drivers imposed by regional boundaries, reaching the end of their shift, or their current passengers mean that this estimate can only be a best guess.

This best guess may well be driven by previous data.






## Ride Sharing: Service Oriented to Data Oriented

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/ride-sharing-soa-doa.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/ride-sharing-soa-doa.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>





<div class="figure">
<div class="figure-frame" id="ride-share-service-soa-figure">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/ride-share-service-soa.svg" width="80%" style=" "></object>
</div>
<div class="magnify" id="ride-share-service-soa-magnify" onclick="magnifyFigure('ride-share-service-soa')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="ride-share-service-soa-caption">
Figure: Service oriented architecture. The data access is buried in the cost allocation service. Data dependencies of the service cannot be found without trawling through the underlying code base.
</div>
</div>

The modern approach to software systems design is known as a
*service-oriented architectures* (SOA). The idea is that software
engineers are responsible for the availability and reliability of the
API that accesses the service they own. Quality of service is
maintained by rigorous standards around *testing* of software systems.



<div class="figure">
<div class="figure-frame" id="ride-share-service-doa-figure">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/ride-share-service-doa.svg" width="80%" style=" "></object>
</div>
<div class="magnify" id="ride-share-service-doa-magnify" onclick="magnifyFigure('ride-share-service-doa')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="ride-share-service-doa-caption">
Figure: Data oriented architecture. Now the joins and the updates are exposed within the streaming ecosystem. We can programmatically determine the factor graph which gives the thread through the model.
</div>
</div>

In data driven decision-making systems, the quality of decision-making
is determined by the quality of the data. We need to extend the notion
of *service*-oriented architecture to *data*-oriented architecture
(DOA).

The focus in SOA is eliminating *hard* failures. Hard failures can
occur due to bugs or systems overload. This notion needs to be
extended in ML systems to capture *soft failures* associated with
declining data quality, incorrect modeling assumptions and
inappropriate re-deployments of models. We need to focus on data
quality assessments. In data-oriented architectures engineering teams
are responsible for the *quality* of their output data streams in
addition to the *availability* of the service they support
[@Lawrence-drl17]. Quality here is not just accuracy, but fairness and
explainability. This important cultural change would be capable of
addressing both the challenge of *technical debt* [@Sculley:debt15]
and the social responsibility of ML systems.

Software development proceeds with a *test-oriented*
culture. One where tests are written before software, and software is
not incorporated in the wider system until all tests pass. We must
apply the same standards of care to our ML systems, although for ML we need statistical tests for quality, fairness, and consistency within the
environment. Fortunately, the main burden of this testing need not
fall to the engineers themselves: through leveraging *classical
statistics* and *emulation* we will automate the creation and
redeployment of these tests across the software ecosystem, we call
this *ML hypervision*.

Modern AI can be based on ML models with many millions of parameters,
trained on very large data sets. In ML, strong emphasis is placed on
*predictive accuracy* whereas sister-fields such as statistics have a
strong emphasis on *interpretability*. ML models are said to be 'black
boxes' which make decisions that are not explainable.[^dark-secret]

[^dark-secret]: See for example
    ["The Dark Secret at the Heart of AI" in Technology Review](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/).



<div class="figure">
<div class="figure-frame" id="ride-share-service-doa-hypothetical-figure">
<object class="svgplot " data="https://mlatcl.github.io/mlai/./slides/diagrams//data-science/ride-share-service-doa-hypothetical.svg" width="80%" style=" "></object>
</div>
<div class="magnify" id="ride-share-service-doa-hypothetical-magnify" onclick="magnifyFigure('ride-share-service-doa-hypothetical')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="ride-share-service-doa-hypothetical-caption">
Figure: Data-oriented programing. There is a requirement for an estimate of the driver allocation to give a rough cost estimate before the user has confirmed the ride. In data-oriented programming, this is achieved through declaring a hypothetical stream which approximates the true driver allocation, but with restricted input information and constraints on the computational latency.
</div>
</div>

For the ride sharing system, we start to see a common issue with a more complex algorithmic decision-making system. Several decisions are being made multiple times. Let's look at the decisions we need along with some design criteria.

1. Driver Availability: Estimate time to arrival for Anne's ride using Anne's location and local available car locations. Latency 50 milliseconds
2. Cost Estimate: Estimate cost for journey using Anne's destination, location and local available car current destinations and availability. Latency 50 milliseconds
3. Driver Allocation: Allocate car to minimize transport cost to destination. Latency 2 seconds.

So we need:

1. a hypothetical to estimate availability. It is constrained by lacking destination information and a low latency requirement.
2. a hypothetical to estimate cost. It is constrained by low latency requirement and 


Simultaneously, drivers in this data ecosystem have an app which notifies them about new jobs and recommends them where to go.

Further advantages. Strategies for data retention (when to snapshot) can be set globally.


A few decisions need to be made in this system. First of all, when the user opens the app, the estimate of the time to the nearest ride may need to be computed quickly, to avoid latency in the service. 

This may require a quick estimate of the ride availability.







## Information Dynamics

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/information-dynamics.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/information-dynamics.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>





With all the second guessing within a complex automated decision-making system, there are potential problems with information dynamics, the 'closed loop' problem, where the sub-systems are being approximated (second guessing) and predictions downstream are being affected.

This leads to the need for a closed loop analysis, for example, see the ["Closed Loop Data Science"](https://www.gla.ac.uk/schools/computing/research/researchsections/ida-section/closedloop/) project led by Rod Murray-Smith at Glasgow.











-->
<h2 id="putting-in-practice">Putting in Practice</h2>
<p>For our software design we can decompose the problem into three
component parts</p>
<ul>
<li>Access</li>
<li>Assess</li>
<li>Process</li>
</ul>
<h2 id="github-template">Github Template</h2>
<p>See the template repository here: <a
href="https://github.com/lawrennd/analysis_template/"
class="uri">https://github.com/lawrennd/analysis_template/</a></p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><p>Chapter 8 of <span class="citation"
data-cites="Lawrence-atomic24">Lawrence (2024)</span></p></li>
<li><p>Chapter 1 of <span class="citation"
data-cites="Lawrence-atomic24">Lawrence (2024)</span></p></li>
</ul>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Ananthanarayanan-cat09" class="csl-entry" role="listitem">
Ananthanarayanan, R., Esser, S.K., Simon, H.D., Modha, D.S., 2009. The
cat is out of the bag: Cortical simulations with <span
class="math inline">\(10^9\)</span> neurons, <span
class="math inline">\(10^{13}\)</span> synapses, in: Proceedings of the
Conference on High Performance Computing Networking, Storage and
Analysis - SC ’09. <a
href="https://doi.org/10.1145/1654059.1654124">https://doi.org/10.1145/1654059.1654124</a>
</div>
<div id="ref-Lawrence-atomic24" class="csl-entry" role="listitem">
Lawrence, N.D., 2024. The atomic human: Understanding ourselves in the
age of AI. Allen Lane.
</div>
<div id="ref-Lawrence-drl17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017b. Data readiness levels. ArXiv.
</div>
<div id="ref-Lawrence:embodiment17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017a. <a href="https://arxiv.org/abs/1705.07996">Living
together: Mind and machine intelligence</a>. arXiv.
</div>
<div id="ref-Moillica-humansstore19" class="csl-entry" role="listitem">
Mollica, F., Piantadosi, S.T., 2019. Humans store about 1.5 megabytes of
information during language acquisition. Royal Society Open Science 6,
181393. <a
href="https://doi.org/10.1098/rsos.181393">https://doi.org/10.1098/rsos.181393</a>
</div>
<div id="ref-Reed-information98" class="csl-entry" role="listitem">
Reed, C., Durlach, N.I., 1998. Note on information transfer rates in
human communication. Presence Teleoperators &amp; Virtual Environments
7, 509–518. <a
href="https://doi.org/10.1162/105474698565893">https://doi.org/10.1162/105474698565893</a>
</div>
<div id="ref-Sculley:debt15" class="csl-entry" role="listitem">
Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner,
D., Chaudhary, V., Young, M., Crespo, J.-F., Dennison, D., 2015. <a
href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden
technical debt in machine learning systems</a>, in: Cortes, C.,
Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R. (Eds.), Advances in
Neural Information Processing Systems 28. Curran Associates, Inc., pp.
2503–2511.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>the challenge of understanding what information pertains
to is known as knowledge representation.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Another related factor is our ability to <em>store</em>
information. <span class="citation"
data-cites="Moillica-humansstore19">Mollica and Piantadosi (2019)</span>
suggest that during language acquisition we store 1.5 Megabytes of data
(12 million bits). That would take around 2000 hours, or nearly twelve
weeks, to transmit verbally.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This is related to challenges of machine learning and
technical debt <span class="citation"
data-cites="Sculley:debt15">(Sculley et al., 2015)</span>, although we
are trying to frame the solution here rather than the problem.<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

