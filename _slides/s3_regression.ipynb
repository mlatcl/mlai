{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3: Regression\n",
    "\n",
    "### Neil D. Lawrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: Looked at objective functions for movie recommendation.\n",
    "- Minimized sum of squares objective by steepest descent and stochastic gradients.\n",
    "- This time: explore least squares for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Examples\n",
    "\n",
    "-   Predict a real value, $y_i$ given some inputs\n",
    "    $x_i$.\n",
    "\n",
    "-   Predict quality of meat given spectral measurements (Tecator data).\n",
    "\n",
    "-   Radiocarbon dating, the C14 calibration curve: predict age given\n",
    "    quantity of C14 isotope.\n",
    "\n",
    "-   Predict quality of different Go or Backgammon moves given expert\n",
    "    rated training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "-  Gold medal times for Olympic 100 m runners since 1896.\n",
    "\n",
    "![image](./diagrams/100m_final_start.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/191adDC>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_100m_men()\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'ro', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "![image](./diagrams/Stephen_Kiprotich.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/16kMKHQ>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'ro',markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "$$ \\text{data} + \\text{model} = \\text{prediction}$$\n",
    "\n",
    "-   $\\text{data}$ : observations, could be actively or passively\n",
    "    acquired (meta-data).\n",
    "\n",
    "-   $\\text{model}$ : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of\n",
    "    the universe. Inductive bias.\n",
    "\n",
    "-   $\\text{prediction}$ : an action to be taken or a categorization or a\n",
    "    quality score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression: Linear Releationship\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "-   $y_i$ : winning time/pace.\n",
    "\n",
    "-   $x_i$ : year of Olympics.\n",
    "\n",
    "-   $m$ : rate of improvement over time.\n",
    "\n",
    "-   $c$ : winning time at year 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overdetermined System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.over_determined_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('over_determined_system{samp:0>3}.svg', directory='./diagrams', samp=(1, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/Pierre-Simon_Laplace.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c + \\epsilon$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ \n",
    "$$3 = m + c + \\epsilon_1$$ \n",
    "\n",
    "point 2: $x = 3$, $y=1$ \n",
    "$$1 = 3m + c + \\epsilon_2$$ \n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Density\n",
    "- Perhaps the most common probability density.\n",
    "\\begin{align*}\n",
    "p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "& \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "- The Gaussian density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.gaussian_of_height()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "![](./diagrams/gaussian_of_height.svg)\n",
    "The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n",
    "  0.0225$. Mean shown as red line. It could represent the heights of a population of\n",
    "  students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Sum of Gaussian**\n",
    "\n",
    "-   Sum of Gaussian variables is also Gaussian.\n",
    "    $$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ \n",
    "    And the sum is distributed as\n",
    "    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n",
    "    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n",
    "    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Scaling a Gaussian**\n",
    "\n",
    "-   Scaling a Gaussian leads to a Gaussian.\n",
    "    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "    And the scaled density is distributed as\n",
    "    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace's Idea\n",
    "\n",
    "### A Probabilistic Process\n",
    "\n",
    "-   Set the mean of Gaussian to be a function.\n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   This gives us a ‘noisy function’.\n",
    "\n",
    "-   This is known as a stochastic process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height as a Function of Weight\n",
    "\n",
    "-   In the standard Gaussian, parametized by mean and variance.\n",
    "\n",
    "-   Make the mean a linear function of an *input*.\n",
    "\n",
    "-   This leads to a regression model. \n",
    "    \\begin{align*}\n",
    "       y_i=&f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "         \\epsilon_i \\sim &\\mathcal{N}(0, \\sigma^2).\n",
    "     \\end{align*}\n",
    "        \n",
    "-   Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "-   Likelihood of an individual data point\n",
    "    $$p\\left(y_i|x_i,m,c\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   Parameters are gradient, $m$, offset, $c$ of the function and noise\n",
    "    variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "-   If the noise, $\\epsilon_i$ is sampled independently for each\n",
    "    data point.\n",
    "\n",
    "-   Each data point is independent (given $m$ and $c$).\n",
    "\n",
    "-   For independent variables:\n",
    "    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n p(y_i|x_i, m, c)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For Gaussian \n",
    "\n",
    "- i.i.d. assumption\n",
    "    \n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-\\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood Function\n",
    "\n",
    "-   Normally work with the log likelihood:\n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\sum _{i=1}^{n}\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of Maximum Likelihood\n",
    "\n",
    "\n",
    "-   If data was really generated according to probability we specified.\n",
    "\n",
    "-   Correct parameters will be recovered in limit as\n",
    "    $n \\rightarrow \\infty$.\n",
    "\n",
    "-   This can be proven through sample based approximations (law of\n",
    "    large numbers) of “KL divergences”.\n",
    "\n",
    "-   Mainstay of classical statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Interpretation of the Error Function\n",
    "\n",
    "-   Probabilistic Interpretation for Error Function is Negative\n",
    "    Log Likelihood.\n",
    "\n",
    "-   *Minimizing* error function is equivalent to *maximizing*\n",
    "    log likelihood.\n",
    "\n",
    "-   Maximizing *log likelihood* is equivalent to maximizing the\n",
    "    *likelihood* because $\\log$ is monotonic.\n",
    "\n",
    "-   Probabilistic interpretation: Minimizing error function is\n",
    "    equivalent to maximum likelihood with respect to parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "-   Negative log likelihood is the error function leading to an error\n",
    "    function\n",
    "    $$E(m,c,\\sigma^{2})=\\frac{n}{2}\\log \\sigma^2 +\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\left(y_i-mx_i-c\\right)^{2}.$$\n",
    "\n",
    "-   Learning proceeds by minimizing this error function for the data\n",
    "    set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection: Sum of Squares Error\n",
    "\n",
    "-   Ignoring terms which don’t depend on $m$ and $c$ gives\n",
    "    $$E(m, c) \\propto \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "    where $f(x_i) = mx_i + c$.\n",
    "\n",
    "-   This is known as the *sum of squares* error function.\n",
    "\n",
    "-   Commonly used and is closely associated with the\n",
    "    Gaussian likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "- Two functions involved:\n",
    "  - Prediction function: $f(x_i)$\n",
    "  - Error, or Objective function: $E(m, c)$\n",
    "- Error function depends on parameters through prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Interpretation\n",
    "\n",
    "-   What is the mathematical interpretation?\n",
    "\n",
    "    -   There is a cost function.\n",
    "\n",
    "    -   It expresses mismatch between your prediction and reality.\n",
    "        $$E(m, c)=\\sum_{i=1}^n \\left(y_i - mx_i -c\\right)^2$$\n",
    "\n",
    "    -   This is known as the sum of squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning is minimization of the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   Coordinate ascent, find gradient in each coordinate and set to zero.\n",
    "    $$\\frac{\\text{d}E(m)}{\\text{d}m} =\n",
    "          -2\\sum_{i=1}^n x_i\n",
    "          \\left(y_i- m x_i - c \\right)$$\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n x_i\n",
    "          \\left(y_i-\n",
    "            m x_i - c \\right)$$            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equations\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n x_iy_i\n",
    "          +2\\sum_{i=1}^n\n",
    "            m x_i^2 +2\\sum_{i=1}^n cx_i$$\n",
    "    $$m  =    \\frac{\\sum_{i=1}^n \\left(y_i\n",
    "          -c\\right)x_i}{\\sum_{i=1}^nx_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning is minimization of the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   Coordinate ascent, find gradient in each coordinate and set to zero.\n",
    "     $$\\frac{\\text{d}E(c)}{\\text{d}c} =\n",
    "          -2\\sum_{i=1}^n \n",
    "          \\left(y_i- m x_i - c \\right)$$\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n\\left(y_i-\n",
    "            m x_i - c \\right)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning is Optimization\n",
    "\n",
    "- Fixed point equations\n",
    "    $$0 = -2\\sum_{i=1}^n y_i +2\\sum_{i=1}^n m x_i +2n c$$\n",
    "    $$c  =    \\frac{\\sum_{i=1}^n \\left(y_i\n",
    "          -mx_i\\right)}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point Updates\n",
    "\n",
    "Worked example. $$\\begin{aligned}\n",
    "    c^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
    "    m^{*}=&\\frac{\\sum _{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
    "    \\left.\\sigma^2\\right.^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
    "  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Concepts Not Covered\n",
    "\n",
    "-   Other optimization methods:\n",
    "\n",
    "    -   Second order methods, conjugate gradient, quasi-Newton\n",
    "        and Newton.\n",
    "\n",
    "    -   Effective heuristics such as momentum.\n",
    "\n",
    "-   Local vs global solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.1-1.2 of @Rogers:book11 for fitting linear models. \n",
    "- Section 1.2.5 of @Bishop:book06 up to equation 1.65.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Inputs\n",
    "\n",
    "-   Multivariate functions involve more than one input.\n",
    "\n",
    "-   Height might be a function of weight and gender.\n",
    "\n",
    "-   There could be other contributory factors.\n",
    "\n",
    "-   Place these factors in a feature vector $\\mathbf{x}_i$.\n",
    "\n",
    "-   Linear function is now defined as\n",
    "    $$f(\\mathbf{x}_i) = \\sum_{j=1}^p w_j x_{i, j} + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "-   Write in vector notation,\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c$$\n",
    "\n",
    "-   Can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$\n",
    "    which is always 1.\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood for Multivariate Regression\n",
    "\n",
    "-   The likelihood of a single data point is\n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\n",
    "        \\left(-\\frac{\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   Leading to a log likelihood for the data set of\n",
    "    $$L(\\mathbf{w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2\n",
    "          -\\frac{n}{2}\\log 2\\pi -\\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$\n",
    "\n",
    "-   And a corresponding error function of\n",
    "    $$E(\\mathbf{w},\\sigma^2)= \\frac{n}{2}\\log\n",
    "          \\sigma^2 + \\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expand the Brackets\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum _{i=1}^{n}y_i\\mathbf{w}^{\\top}\\mathbf{x}_i\\\\&+\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\mathbf{w}^{\\top}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\mathbf{w} +\\text{const}.\\\\\n",
    "    = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "  \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\\\\&+\\frac{1}{2\\sigma^2} \\mathbf{w}^{\\top}\\left[\\sum\n",
    "    _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w} +\\text{const}.\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Derivatives\n",
    "\n",
    "-   We will need some multivariate calculus.\n",
    "\n",
    "-   For now some simple multivariate differentiation:\n",
    "    $$\\frac{\\text{d}{\\mathbf{a}^{\\top}}{\\mathbf{w}}}{\\text{d}\\mathbf{w}}=\\mathbf{a}$$\n",
    "    and\n",
    "    $$\\frac{\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{w}$$\n",
    "    or if $\\mathbf{A}$ is symmetric (*i.e.*\n",
    "    $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=2\\mathbf{A}\\mathbf{w}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiate\n",
    "\n",
    "Differentiating with respect to the vector $\\mathbf{w}$ we obtain\n",
    "$$\\frac{\\partial L\\left(\\mathbf{w},\\sigma^2 \\right)}{\\partial \\mathbf{w}}=\\frac{1}{\\sigma^2} \\sum _{i=1}^{n}\\mathbf{x}_iy_i-\\frac{1}{\\sigma^2} \\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w}$$\n",
    "Leading to\n",
    "$$\\mathbf{w}^{*}=\\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]^{-1}\\sum _{i=1}^{n}\\mathbf{x}_iy_i,$$\n",
    "Rewrite in matrix notation:\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{X}$$\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_iy_i = \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "-   Update for $\\mathbf{w}^{*}$.\n",
    "    $$\\mathbf{w}^{*} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "-   The equation for $\\left.\\sigma^2\\right.^{*}$ may also be found\n",
    "    $$\\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum _{i=1}^{n}\\left(y_i-\\left.\\mathbf{w}^{*}\\right.^{\\top}\\mathbf{x}_i\\right)^{2}}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.3 of @Rogers:book11 for Matrix & Vector Review.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
