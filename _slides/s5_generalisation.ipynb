{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 5: Generalisation\n",
    "\n",
    "### Neil D. Lawrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: introduced basis functions.\n",
    "- Showed how to maximize the likelihood of a non-linear model that's linear in parameters.\n",
    "- Explored the different characteristics of different basis function models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial Fits to Olympics Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mlai\n",
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "max_basis = 7\n",
    "basis = mlai.polynomial\n",
    "\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "data_limits = [1892, 2020]\n",
    "num_data = x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rmse_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n",
    "              model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "              xlim=data_limits, objective_ylim=[0, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "- Increase number of basis functions we obtain a better 'fit' to the data.\n",
    "- How will the model perform on previously unseen data?\n",
    "- Let's consider predicting the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', \n",
    "                 param_range=(1, max_basis+1), \n",
    "                 model=mlai.LM, basis=basis, data_limits=data_limits,\n",
    "                 permute=False, objective_ylim=[0, 0.6], xlim=data_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extrapolation\n",
    "\n",
    "- Here we are training beyond where the model has learnt.\n",
    "- This is known as *extrapolation*.\n",
    "- Extrapolation is predicting into the future here, but could be:\n",
    "    - Predicting back to the unseen past (pre 1892)\n",
    "    - Spatial prediction (e.g. Cholera rates outside Manchester given rates inside Manchester)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alan Turing\n",
    "- He was a formidable Marathon runner. \n",
    "- In 1946 he ran a time 2 hours 46 minutes.\n",
    "- What is the probability he would have won an Olympics if one had been held in 1946?  \n",
    "![Alan Turing running in 1946](http://www.turing.org.uk/turing/pi2/run.jpg)\n",
    "<center>*Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: [Alan Turing Internet Scrapbook](http://www.turing.org.uk/scrapbook/run.html).*</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpolation\n",
    "- Predicting the wining time for 1946 Olympics is *interpolation*.\n",
    "- This is because we have times from 1936 and 1948.\n",
    "- If we want a model for *interpolation* how can we test it?\n",
    "- One trick is to sample the validation set from throughout the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n",
    "                 model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "                 xlim=data_limits, prefix='olympic_val_inter', objective_ylim=[0.1, 0.6], permute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_val_inter_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice of Validation Set\n",
    "\n",
    "- The choice of validation set should reflect how you will use the model in practice.\n",
    "- For extrapolation into the future we tried validating with data from the future.\n",
    "- For interpolation we chose validation set from data.\n",
    "- For different validation sets we could get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave One Out Error\n",
    "- Take training set and remove one point.\n",
    "- Train on the remaining data.\n",
    "- Compute the error on the point you removed (which wasn't in the training data).\n",
    "- Do this for each point in the training set in turn.\n",
    "- Average the resulting error. \n",
    "- This is the leave one out error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.loo_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "             model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "             xlim=data_limits, objective_ylim=[0.1, 0.6], prefix='olympic_loo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_loo{part:0>3}_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis), part=(0,x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias Variance Decomposition\n",
    "\n",
    "Expected test error for different variations of the *training data* sampled from, $\\Pr(\\mathbf{x}, y)$\n",
    "\n",
    "$$\\mathbb{E}\\left[ (y - f^*(\\mathbf{x}))^2 \\right]$$\n",
    "\n",
    "Decompose as\n",
    "\n",
    "$$\\mathbb{E}\\left[ (y - f(\\mathbf{x}))^2 \\right] = \\text{bias}\\left[f^*(\\mathbf{x})\\right]^2 + \\text{variance}\\left[f^*(\\mathbf{x})\\right] +\\sigma^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias\n",
    "\n",
    "- Given by\n",
    "    $$\\text{bias}\\left[f^*(\\mathbf{x})\\right] = \\mathbb{E}\\left[f^*(\\mathbf{x})\\right] - f(\\mathbf{x})$$\n",
    "    \n",
    "- Error due to bias comes from a model that's too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance\n",
    "\n",
    "- Given by\n",
    "    $$\\text{variance}\\left[f^*(\\mathbf{x})\\right] = \\mathbb{E}\\left[\\left(f^*(\\mathbf{x}) -  \\mathbb{E}\\left[f^*(\\mathbf{x})\\right]\\right)^2\\right]$$\n",
    "    \n",
    "- Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$ Fold Cross Validation\n",
    "\n",
    "- Leave one out error can be very time consuming.\n",
    "- Need to train your algorithm $n$ times.\n",
    "- An alternative: $k$ fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.cv_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "               model=mlai.LM, basis=basis, data_limits=data_limits,\n",
    "               xlim=data_limits, objective_ylim=[0.2,0.6], num_parts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_{num_parts}'.format(num_parts=5) + 'cv{part:0>2}_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', part=(0,5),num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "- Section 1.5 of @Rogers:book11"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
