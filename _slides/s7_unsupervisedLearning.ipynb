{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MLAI Week 8: Unsupervised Learning\n",
    "\n",
    "### Neil D. Lawrence\n",
    "\n",
    "### 17th November 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "* Last time: Looked at Bayesian Regression.\n",
    "* Introduced priors and marginal likelihoods.\n",
    "* This time: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "* Supervised learning is learning where each data has a label (e.g. regression output)\n",
    "* In unsupervised learning we have no labels for the data.\n",
    "* Often thought of as structure discovery.\n",
    "    * Finding features in the data\n",
    "    * Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "num_centres = 20\n",
    "num_data = 200\n",
    "centres = np.random.normal(size=(num_centres, 2))\n",
    "w = np.random.normal(size=(num_centres, 2))*0.1\n",
    "alloc = np.random.randint(0, num_centres, size=(num_data))\n",
    "sigma = np.random.normal(size=(num_centres, 1))*0.05\n",
    "epsilon = np.random.normal(size=(num_data,2))*sigma[alloc, :]\n",
    "\n",
    "Y = w[alloc, :]*np.random.normal(size=(num_data, 1)) + centres[alloc, :] + epsilon\n",
    "\n",
    "ax.plot(Y[:, 0], Y[:, 1], 'rx')\n",
    "ax.set_xlabel('$y_1$', fontsize=20)\n",
    "ax.set_ylabel('$y_2$', fontsize=20)\n",
    "\n",
    "plt.savefig('./diagrams/cluster_data00.svg')\n",
    "pi_vals = np.linspace(-np.pi, np.pi, 200)[:, None]\n",
    "for i in range(num_centres):\n",
    "    ax.plot(centres[i, 0], centres[i, 1], 'o', markersize=5, color=[0, 0, 0], linewidth=2)\n",
    "    x = np.hstack([np.sin(pi_vals), np.cos(pi_vals)])\n",
    "    L = np.linalg.cholesky(np.outer(w[i, :],w[i, :]) + sigma[i]**2*np.eye(2))\n",
    "    el = np.dot(x, L.T)\n",
    "    ax.plot(centres[i, 0] + el[:, 0], centres[i, 1] + el[:, 1], linewidth=2, color=[0,0,0])\n",
    "plt.savefig('./diagrams/cluster_data01.svg')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('cluster_data{counter:0>2}.svg', directory='./diagrams', counter=(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering\n",
    "\n",
    "* One common approach, not deeply covered in this course. \n",
    "* Associate each data point, $\\mathbf{y}_{i, :}$ with one of $k$ different discrete groups.\n",
    "* For example:\n",
    "    * Clustering animals into discrete groups. Are animals discrete or continuous?\n",
    "    * Clustering into different different *political* affiliations.\n",
    "* Humans do seem to like clusters:\n",
    "    * Very useful when interacting with biologists.\n",
    "* Subtle difference between clustering and *vector quantisation*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trying to Teach About Infinity\n",
    "\n",
    "* Little anecdote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering and Vector Quantisation\n",
    "\n",
    "* To my mind difference is in clustering there should be a reduction in data density between samples.\n",
    "* This definition is not universally applied.\n",
    "* For today's purposes we merge them:\n",
    "    * Determine how to allocate each point to a group and *harder* total number of groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-means Clustering\n",
    "\n",
    "* Simple algorithm for allocating points to groups. \n",
    "* *Require*: Set of $k$ cluster centres & assignment of each points to a cluster.\n",
    "    1. Initialize cluster centres as randomly selected data points.\n",
    "    2. Assign each data point to *nearest* cluster centre.\n",
    "    3. Update each cluster centre by setting it to the mean of assigned data points.\n",
    "    4. Repeat 2 and 3 until cluster allocations do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective Function\n",
    "\n",
    "* This minimizes the objective\n",
    "  $$\n",
    "    E=\\sum_{j=1}^K \\sum_{i\\ \\text{allocated to}\\ j}  \\left(\\mathbf{y}_{i, :} - \\boldsymbol{\\mu}_{j, :}\\right)^\\top\\left(\\mathbf{y}_{i, :} - \\boldsymbol{\\mu}_{j, :}\\right)\n",
    "  $$\n",
    "  *i.e.* it minimizes thesum of Euclidean squared distances betwen points and their associated centres.\n",
    "* The minimum is *not* guaranteed to be *global* or *unique*.\n",
    "* This objective is a non-convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def write_plot(counter, caption):\n",
    "    filebase = './diagrams/kmeans_clustering_{counter:0>3}'.format(counter=counter)\n",
    "    plt.savefig(filebase + '.svg')\n",
    "    f = open(filebase + '.tex', 'w')\n",
    "    f.write(caption)\n",
    "    f.close()\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "fontsize = 20\n",
    "\n",
    "num_clust_points = 30\n",
    "\n",
    "Y = np.vstack([np.random.normal(size=(num_clust_points, 2)) + 2.5,\n",
    "       np.random.normal(size=(num_clust_points, 2)) - 2.5,\n",
    "       np.random.normal(size=(num_clust_points, 2)) + np.array([2.5, -2.5])])\n",
    "\n",
    "centre_inds = np.random.permutation(Y.shape[0])[:3]\n",
    "centres = Y[centre_inds, :]\n",
    "\n",
    "ax.cla()\n",
    "\n",
    "ax.plot(Y[:, 0], Y[:, 1], '.', color=[0, 0, 0], markersize=10)\n",
    "ax.set_xlabel('$y_1$')\n",
    "ax.set_ylabel('$y_2$')\n",
    "ax.set_title('Data')\n",
    "counter=0\n",
    "write_plot(counter, 'Data set to be analyzed. Initialize cluster centres.')\n",
    "ax.plot(centres[:, 0], centres[:, 1], 'o', color=[0,0,0], linewidth=3, markersize=12)    \n",
    "counter+=1\n",
    "write_plot(counter, 'Allocate each point to the cluster with the nearest centre')\n",
    "i = 0\n",
    "\n",
    "for i in range(6):\n",
    "    dist_mat = ((Y[:, :, None] - centres.T[None, :, :])**2).sum(1)\n",
    "    ind = dist_mat.argmin(1)\n",
    "    ax.cla()\n",
    "    ax.plot(Y[ind==0, 0], Y[ind==0, 1], 'x', color= [1, 0, 0], markersize=10)\n",
    "    ax.plot(Y[ind==1, 0], Y[ind==1, 1], 'o', color=[0, 1, 0], markersize=10)\n",
    "    ax.plot(Y[ind==2, 0], Y[ind==2, 1], '+', color=[0, 0, 1], markersize=10)\n",
    "    c = ax.plot(centres[:, 0], centres[:, 1], 'o', color=[0,0, 0], markersize=12, linewidth=3)\n",
    "    ax.set_xlabel('$y_1$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$y_2$',fontsize=fontsize)\n",
    "    ax.set_title('Iteration ' + str(i))\n",
    "    counter+=1\n",
    "    write_plot(counter, 'Update each centre by setting to the mean of the allocated points.')\n",
    "    for j in range(centres.shape[0]):\n",
    "          centres[j, :] = np.mean(Y[ind==j, :], 0)\n",
    "    c[0].set_data(centres[:, 0], centres[:, 1])\n",
    "    counter+=1\n",
    "    plt.savefig('./diagrams/kmeans_clustering_{counter:0>3}.svg'.format(counter=counter))\n",
    "    write_plot(counter, 'Allocate each data point to the nearest cluster centre.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('kmeans_clustering_{counter:0>3}.svg', directory='./diagrams', \n",
    "                            text_top='kmeans_clustering_{counter:0>3}.tex', counter=(0, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Clustering Approaches\n",
    "\n",
    "* Spectral clustering (@Shi:normalized00,@Ng:spectral02)\n",
    "    * Allows clusters which aren't convex hulls.\n",
    "* Dirichlet process\n",
    "    * A probabilistic formulation for a clustering algorithm that is *non-parametric*. \n",
    "    * Loosely speaking it allows infinite clusters\n",
    "    * In practice useful for dealing with previously unknown species (e.g. a \"Black Swan Event\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High Dimensional Data\n",
    "\n",
    "* USPS Data Set Handwritten Digit\n",
    "* 3648 dimensions (64 rows, 57 columns)\n",
    "* Space contains much more than just this digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "six_image = mlai.load_pgm('br1561_6.3.pgm', directory ='./diagrams')\n",
    "rows = six_image.shape[0]\n",
    "col = six_image.shape[1]\n",
    "      \n",
    "ax.imshow(six_image,interpolation='none').set_cmap('gray')\n",
    "plt.savefig('./diagrams/dem_six000.png')\n",
    "for i in range(3):\n",
    "    rand_image = np.random.rand(rows, col)<((six_image>0).sum()/float(rows*col))\n",
    "    ax.imshow(rand_image,interpolation='none').set_cmap('gray')\n",
    "    plt.savefig('./diagrams/dem_six{i:0>3}.png'.format(i=i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_six{counter:0>3}.png', directory='./diagrams', counter=(0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### USPS Samples\n",
    "\n",
    "* Even if we sample every nanonsecond from now until end of universe you won't see original six!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Model of Digit\n",
    "* Rotate a prototype"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "from scipy.misc import imrotate\n",
    "six_image = np.hstack([np.zeros((rows, 3)), six_image, np.zeros((rows, 4))])\n",
    "dim_one = np.asarray(six_image.shape)      \n",
    "angles = range(360)\n",
    "i = 0\n",
    "Y = np.zeros((len(angles), np.prod(dim_one)))\n",
    "for angle in angles:\n",
    "    rot_image = imrotate(six_image, angle, interp='nearest')\n",
    "    dim_two = np.asarray(rot_image.shape)\n",
    "    start = [int(round((dim_two[0] - dim_one[0])/2)), int(round((dim_two[1] - dim_one[1])/2))]\n",
    "    crop_image = rot_image[start[0]+np.array(range(dim_one[0])), start[1]+np.array(range(dim_one[1]))]\n",
    "    Y[i, :] = crop_image.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_six_rotate{counter:0>3}.png', directory='./diagrams', counter=(0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Low Dimensional Manifolds\n",
    "\n",
    "* Pure rotation is too simple\n",
    "    * In practice data may undergo several distortions.\n",
    "* For high dimensional data with *structure*:\n",
    "    * We expect fewer distortions than dimensions;\n",
    "    * Therefore we expect the data to live on a lower dimensional manifold.\n",
    "    * Conclusion: Deal with high dimensional data by looking for a lower dimensional non-linear embedding.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "* PCA (@Hotelling:analysis33) is a linear embedding.\n",
    "* Today its presented as:\n",
    "    * Rotate to find 'directions' in data with maximal variance.\n",
    "    * How do we find these directions?\n",
    "* Algorithmically we do this by diagonalizing the sample covariance matrix \n",
    "    $$\n",
    "    \\mathbf{S}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\mathbf{y}_{i, :}-\\boldsymbol{\\mu}\\right)\\left(\\mathbf{y}_{i, :} - \\boldsymbol{\\mu}\\right)^\\top\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "* Find directions in the data, $\\mathbf{x} = \\mathbf{U}\\mathbf{y}$, for which variance is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lagrangian\n",
    "\n",
    "* Solution is found via constrained optimisation (which uses [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)): \n",
    "  $$\n",
    "    L\\left(\\mathbf{u}_{1},\\lambda_{1}\\right)=\\mathbf{u}_{1}^{\\top}\\mathbf{S}\\mathbf{u}_{1}+\\lambda_{1}\\left(1-\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{1}\\right)\n",
    "  $$\n",
    "\n",
    "* Gradient with respect to $\\mathbf{u}_{1}$ \n",
    "  $$\\frac{\\text{d}L\\left(\\mathbf{u}_{1},\\lambda_{1}\\right)}{\\text{d}\\mathbf{u}_{1}}=2\\mathbf{S}\\mathbf{u}_{1}-2\\lambda_{1}\\mathbf{u}_{1}$$\n",
    "rearrange to form\n",
    "$$\\mathbf{S}\\mathbf{u}_{1}=\\lambda_{1}\\mathbf{u}_{1}.$$\n",
    "  Which is known as an [*eigenvalue problem*](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).\n",
    "  \n",
    "* Further directions that are *orthogonal* to the first can also be shown to be eigenvectors of the covariance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Dimensionality Reduction\n",
    "\n",
    "* Represent data, $\\mathbf{Y}$, with a lower dimensional set of latent\n",
    "    variables $\\mathbf{X}$.\n",
    "* Assume a linear relationship of the form\n",
    "  $$ \\mathbf{y}_{i,:}=\\mathbf{W}\\mathbf{x}_{i,:}+\\boldsymbol{\\epsilon}_{i,:},\n",
    "  $$\n",
    "  where\n",
    "  $$\n",
    "  \\boldsymbol{\\epsilon}_{i,:} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Latent Variable Model\n",
    "**Probabilistic PCA**\n",
    "\n",
    "* Define *linear-Gaussian relationship* between latent variables and data.\n",
    "* **Standard** Latent variable approach:\n",
    "  * Define Gaussian prior over *latent space*, $\\mathbf{X}$.\n",
    "  * Integrate out *latent variables*.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\begin{tikzpicture}\n",
    "          \n",
    "          % Define nodes\n",
    "          \\node[obs]                               (Y) {$\\dataMatrix$};\n",
    "          \\node[const, above=of Y, xshift=-1.2cm] (W) {$\\mappingMatrix$};\n",
    "          \\node[latent, above=of Y, xshift=1.2cm]  (X) {$\\latentMatrix$};\n",
    "          \\node[const, right=1cm of Y]            (sigma) {$\\dataStd^2$};\n",
    "          \n",
    "          % Connect the nodes\n",
    "          \\edge {X,W,sigma} {Y} ; %\n",
    "          \n",
    "        \\end{tikzpicture}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p\\left(\\mathbf{Y}|\\mathbf{X},\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{y}_{i,:}|\\mathbf{W}\\mathbf{x}_{i,:}, \\sigma^2\\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p\\left(\\mathbf{X}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{x}_{i,:}|\\mathbf{0},\\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{y}_{i,:}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computation of the Marginal Likelihood\n",
    "\n",
    "$$\\mathbf{y}_{i,:}=\\mathbf{W}\\mathbf{x}_{i,:}+\\boldsymbol{\\epsilon}_{i,:},\\quad\n",
    "\\mathbf{x}_{i,:} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}), \\quad\n",
    "\\boldsymbol{\\epsilon}_{i,:} \\sim \\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbf{W}\\mathbf{x}_{i,:} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbf{W}\\mathbf{x}_{i, :} + \\boldsymbol{\\epsilon}_{i, :} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Latent Variable Model II\n",
    "  **Probabilistic PCA Max. Likelihood Soln** (@Tipping:probpca99)\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " %\\includegraphics<1>[width=0.25\\textwidth]{../../../gplvm/tex/diagrams/ppcaGraph}\n",
    "    \\begin{tikzpicture}\n",
    "        \n",
    "      % Define nodes\n",
    "      \\node[obs]                               (Y) {$\\dataMatrix$};\n",
    "      \\node[const, above=of Y] (W) {$\\mappingMatrix$};\n",
    "      \\node[const, right=1cm of Y]            (sigma) {$\\dataStd^2$};\n",
    "      \n",
    "      % Connect the nodes\n",
    "      \\edge {W,sigma} {Y} ; %\n",
    "    \\end{tikzpicture}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{y}_{i, :}|\\mathbf{0}, \\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Latent Variable Model II\n",
    "  \n",
    "**Probabilistic PCA Max. Likelihood Soln** (@Tipping:probpca99)\n",
    "  $$  p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathcal{N}\\left(\\mathbf{y}_{i,:}|\\mathbf{0},\\mathbf{C}\\right),\\quad \\mathbf{C}=\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\n",
    "  $$\n",
    "  $$\n",
    "    \\log p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=-\\frac{n}{2}\\log\\left|\\mathbf{C}\\right|-\\frac{1}{2}\\text{tr}\\left(\\mathbf{C}^{-1}\\mathbf{Y}^{\\top}\\mathbf{Y}\\right)+\\text{const.}\n",
    "  $$\n",
    "  If $\\mathbf{U}_{q}$ are first $q$ principal eigenvectors of $n^{-1}\\mathbf{Y}^{\\top}\\mathbf{Y}$\n",
    "  and the corresponding eigenvalues are $\\boldsymbol{\\Lambda}_{q}$,\n",
    "  $$\n",
    " \\mathbf{W}=\\mathbf{U}_{q}\\mathbf{L}\\mathbf{R}^{\\top},\\quad\\mathbf{L}=\\left(\\boldsymbol{\\Lambda}_{q}-\\sigma^{2}\\mathbf{I}\\right)^{\\frac{1}{2}}\n",
    "  $$\n",
    "    where $\\mathbf{R}$ is an arbitrary rotation matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "-   Chapter 7 of @Rogers:book11 up to pg 249."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
