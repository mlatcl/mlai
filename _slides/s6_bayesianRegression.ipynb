{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 6: Bayesian Regression\n",
    "\n",
    "### Neil D. Lawrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Simultaneous Equations\n",
    "\n",
    "A system of two simultaneous equations with two\n",
    "unknowns.\n",
    "\n",
    "How do we deal with three simultaneous\n",
    "equations with only two unknowns?\n",
    "\n",
    "$$\\begin{aligned}\n",
    "        y_1 = & mx_1 + c\\\\\n",
    "        y_2 = & mx_2 + c\n",
    "      \\end{aligned}$$ \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        y_1-y_2 = & m(x_1 - x_2)\n",
    "      \\end{aligned}$$  \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        \\frac{y_1-y_2}{x_1 - x_2} = & m\n",
    "      \\end{aligned}$$ \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        m & =\\frac{y_2-y_1}{x_2 - x_1}\\\\\n",
    "        c & = y_1 - m x_1\n",
    "      \\end{aligned}$$ \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        y_1 = & mx_1 + c\\\\\n",
    "        y_2 = & mx_2 + c\\\\\n",
    "        y_3 = & mx_3 + c\n",
    "      \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.under_determined_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "- What about two unknowns and *one* observation?\n",
    "    $$y_1 =  mx_1 + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('under_determined_system{samp:0>3}.svg', directory='./diagrams', samp=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "- Can compute $m$ given $c$.\n",
    "$$m = \\frac{y_1 -c}{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "\n",
    "- Can compute $m$ given $c$.\n",
    "\n",
    "Assume \n",
    "$$c \\sim \\mathcal{N}(0, 4),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined System\n",
    "\n",
    "-   With two unknowns and two observations: \n",
    "    $$\\begin{aligned}\n",
    "          y_1 = & mx_1 + c\\\\\n",
    "          y_2 = & mx_2 + c\n",
    "        \\end{aligned}$$\n",
    "\n",
    "-   Additional observation leads to *overdetermined* system.\n",
    "    $$y_3 =  mx_3 + c$$\n",
    "\n",
    "-   This problem is solved through a noise model\n",
    "    $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ $$\\begin{aligned}\n",
    "          y_1 = mx_1 + c + \\epsilon_1\\\\\n",
    "          y_2 = mx_2 + c + \\epsilon_2\\\\\n",
    "          y_3 = mx_3 + c + \\epsilon_3\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noise Models\n",
    "\n",
    "-   We aren’t modeling entire system.\n",
    "\n",
    "-   Noise model gives mismatch between model and data.\n",
    "\n",
    "-   Gaussian model justified by appeal to central limit theorem.\n",
    "\n",
    "-   Other models also possible (Student-$t$ for heavy tails).\n",
    "\n",
    "-   Maximum likelihood with Gaussian noise leads to *least squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different Types of Uncertainty\n",
    "\n",
    "-   The first type of uncertainty we are assuming is\n",
    "    *aleatoric* uncertainty.\n",
    "\n",
    "-   The second type of uncertainty we are assuming is\n",
    "    *epistemic* uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aleatoric Uncertainty\n",
    "\n",
    "-   This is uncertainty we couldn’t know even if we wanted to. e.g. the\n",
    "    result of a football match before it’s played.\n",
    "\n",
    "-   Where a sheet of paper might land on the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Epistemic Uncertainty\n",
    "\n",
    "-   This is uncertainty we could in principal know the answer too. We\n",
    "    just haven’t observed enough yet, e.g. the result of a football\n",
    "    match *after* it’s played.\n",
    "\n",
    "-   What colour socks your lecturer is wearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "-   @Bishop:book06 Section 1.2.3 (pg 21–24).\n",
    "\n",
    "-   @Bishop:book06 Section 1.2.6 (start from just past eq 1.64\n",
    "    pg 30-32).\n",
    "\n",
    "-   @Rogers:book11 use an example of a coin toss for introducing\n",
    "    Bayesian inference Chapter 3, Sections 3.1-3.4 (pg 95-117). Although\n",
    "    you also need the beta density which we haven’t yet discussed. This\n",
    "    is also the example that @Laplace:memoire74 used.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Bayesian Inference\n",
    "\n",
    "    -   @Rogers:book11 use an example of a coin toss for introducing\n",
    "        Bayesian inference Chapter 3, Sections 3.1-3.4 (pg 95-117).\n",
    "        Although you also need the beta density which we haven’t\n",
    "        yet discussed. This is also the example that\n",
    "        @Laplace:memoire74 used.\n",
    "\n",
    "    -   @Bishop:book06 Section 1.2.3 (pg 21–24).\n",
    "\n",
    "    -   @Bishop:book06 Section 1.2.6 (start from just past eq 1.64\n",
    "        pg 30-32)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior Distribution\n",
    "\n",
    "-   Bayesian inference requires a prior on the parameters.\n",
    "\n",
    "-   The prior represents your belief *before* you see the data of the\n",
    "    likely value of the parameters.\n",
    "\n",
    "-   For linear regression, consider a Gaussian prior on the intercept:\n",
    "    $$c \\sim \\mathcal{N}(0, \\alpha_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior Distribution\n",
    "\n",
    "-   Posterior distribution is found by combining the prior with\n",
    "    the likelihood.\n",
    "\n",
    "-   Posterior distribution is your belief *after* you see the data of\n",
    "    the likely value of the parameters.\n",
    "\n",
    "-   The posterior is found through **Bayes’ Rule**\n",
    "    $$p(c|y) = \\frac{p(y|c)p(c)}{p(y)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.bayes_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_gaussian{stage:0>2}.svg', './diagrams/', stage=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stages to Derivation of the Posterior\n",
    "\n",
    "-   Multiply likelihood by prior\n",
    "\n",
    "    -   they are \"exponentiated quadratics\", the answer is always also\n",
    "        an exponentiated quadratic because\n",
    "        $$\\exp(a^2)\\exp(b^2) = \\exp(a^2 + b^2)$$\n",
    "\n",
    "-   Complete the square to get the resulting density in the form of\n",
    "    a Gaussian.\n",
    "\n",
    "-   Recognise the mean and (co)variance of the Gaussian. This is the\n",
    "    estimate of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main Trick\n",
    "\n",
    "$$p(c) = \\frac{1}{\\sqrt{2\\pi\\alpha_1}} \\exp\\left(-\\frac{1}{2\\alpha_1}c^2\\right)$$\n",
    "$$p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2) = \\frac{1}{\\left(2\\pi\\sigma^2\\right)^{\\frac{n}{2}}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - mx_i - c)^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) = \\frac{p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)}{p(\\mathbf{y}|\\mathbf{x}, m, \\sigma^2)}$$\n",
    "\n",
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) =  \\frac{p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)}{\\int p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c) \\text{d} c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) \\propto  p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\log p(c | \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) =&-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n(y_i-c - mx_i)^2-\\frac{1}{2\\alpha_1} c^2 + \\text{const}\\\\\n",
    "     = &-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-mx_i)^2 -\\left(\\frac{n}{2\\sigma^2} + \\frac{1}{2\\alpha_1}\\right)c^2\\\\\n",
    "    & + c\\frac{\\sum_{i=1}^n(y_i-mx_i)}{\\sigma^2},\n",
    "  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "complete the square of the quadratic form to obtain\n",
    "$$\\log p(c | \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) = -\\frac{1}{2\\tau^2}(c - \\mu)^2 +\\text{const},$$\n",
    "where $\\tau^2 = \\left(n\\sigma^{-2} +\\alpha_1^{-1}\\right)^{-1}$\n",
    "and\n",
    "$\\mu = \\frac{\\tau^2}{\\sigma^2} \\sum_{i=1}^n(y_i-mx_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Joint Density\n",
    "\n",
    "-   Really want to know the *joint* posterior density over the\n",
    "    parameters $c$ *and* $m$.\n",
    "\n",
    "-   Could now integrate out over $m$, but it’s easier to consider the\n",
    "    multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height and Weight Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.height_weight()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./diagrams/height_weight_gaussian.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Sampling Two Dimensional Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.independent_height_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('independent_height_weight{fig:0>3}.png', './diagrams/', fig=(0, 79))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independence Assumption\n",
    "\n",
    "- This assumes height and weight are independent.\n",
    "    $$\n",
    "    p(h, w) = p(h)p(w)\n",
    "    $$\n",
    "- In reality they are dependent (body mass index) $= \\frac{w}{h^2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampling Two Dimensional Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.correlated_height_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('correlated_height_weight{fig:0>3}.png', './diagrams/', fig=(0, 79))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independent Gaussians\n",
    "\n",
    "$$p(w, h) = p(w)p(h)$$ \n",
    "$$p(w, h) = \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{(w-\\mu_1)^2}{\\sigma_1^2} + \\frac{(h-\\mu_2)^2}{\\sigma_2^2}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independent Gaussians\n",
    "$$p(w, h) = \\frac{1}{\\sqrt{2\\pi\\sigma_1^2 2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{1}{2}\\left(\\begin{bmatrix}w \\\\ h\\end{bmatrix} - \\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\\right)^\\top\\begin{bmatrix}\\sigma_1^2& 0\\\\0&\\sigma_2^2\\end{bmatrix}^{-1}\\left(\\begin{bmatrix}w \\\\ h\\end{bmatrix} - \\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independent Gaussians\n",
    "$$p(\\mathbf{y}) = \\frac{1}{\\left|2\\pi \\mathbf{D}\\right|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\\mathbf{D}^{-1}(\\mathbf{y} - \\boldsymbol{\\mu})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated Gaussian\n",
    "\n",
    "Form correlated from original by rotating the data space using matrix\n",
    "$\\mathbf{R}$.\n",
    "\n",
    "\n",
    "$$p(\\mathbf{y}) = \\frac{1}{\\left|2\\pi\\mathbf{D}\\right|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\\mathbf{D}^{-1}(\\mathbf{y} - \\boldsymbol{\\mu})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated Gaussian\n",
    "\n",
    "$$p(\\mathbf{y}) = \\frac{1}{\\left|2\\pi\\mathbf{D}\\right|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{R}^\\top\\mathbf{y} - \\mathbf{R}^\\top\\boldsymbol{\\mu})^\\top\\mathbf{D}^{-1}(\\mathbf{R}^\\top\\mathbf{y} - \\mathbf{R}^\\top\\boldsymbol{\\mu})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated Gaussian\n",
    "\n",
    "$$p(\\mathbf{y}) = \\frac{1}{\\left|2\\pi\\mathbf{D}\\right|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\\mathbf{R}\\mathbf{D}^{-1}\\mathbf{R}^\\top(\\mathbf{y} - \\boldsymbol{\\mu})\\right)$$\n",
    "this gives a covariance matrix:\n",
    "$$\\mathbf{C}^{-1} = \\mathbf{R} \\mathbf{D}^{-1} \\mathbf{R}^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated Gaussian\n",
    "\n",
    "$$p(\\mathbf{y}) = \\frac{1}{\\left|{2\\pi\\mathbf{C}}\\right|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\\mathbf{C}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu})\\right)$$\n",
    "this gives a covariance matrix:\n",
    "$$\\mathbf{C} = \\mathbf{R} \\mathbf{D} \\mathbf{R}^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "-   Section 2.3 of @Bishop:book06 up to top of pg 85\n",
    "    (multivariate Gaussians).\n",
    "\n",
    "-   Section 3.3 of @Bishop:book06 up to 159 (pg 152–159)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Revisit Olympics Data\n",
    "\n",
    "-   Use Bayesian approach on olympics data with polynomials.\n",
    "\n",
    "-   Choose a prior\n",
    "    $\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0},\\alpha \\mathbf{I})$ with\n",
    "    $\\alpha = 1$.\n",
    "\n",
    "-   Choose noise variance $\\sigma^2 = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampling the Prior\n",
    "\n",
    "-   Always useful to perform a ‘sanity check’ and sample from the prior\n",
    "    before observing the data.\n",
    "\n",
    "-   Since $\\mathbf{y} = \\boldsymbol{\\Phi} \\mathbf{w} + \\boldsymbol{\\epsilon}$\n",
    "    just need to sample $$w \\sim \\mathcal{N}(0,\\alpha)$$\n",
    "    $$\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2)$$ with\n",
    "    $\\alpha=1$ and $\\sigma^2 = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "basis = mlai.polynomial\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "num_data = x.shape[0]\n",
    "\n",
    "data_limits = [1892, 2020]\n",
    "\n",
    "max_basis = y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rmse_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "              model=mlai.BLM, basis=basis, alpha=1, sigma2=0.04, data_limits=data_limits,\n",
    "              xlim=data_limits, objective_ylim=[0.5,1.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Data with Bayesian Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pods.notebook.display_plots('olympic_BLM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "              model=mlai.BLM, basis=basis, alpha=1, sigma2=0.04, data_limits=data_limits,\n",
    "              xlim=data_limits, objective_ylim=[0.1,0.6], permute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hold Out Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_val_BLM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, 27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts=5\n",
    "plot.cv_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "              model=mlai.BLM, basis=basis, alpha=1, sigma2=0.04, data_limits=data_limits,\n",
    "              xlim=data_limits, objective_ylim=[0.2,0.6], num_parts=num_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_5cv{part:0>2}_BLM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', part=(0, 5), num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit\n",
    "\n",
    "-   Marginal likelihood doesn’t always increase as model\n",
    "    order increases.\n",
    "\n",
    "-   Bayesian model always has 2 parameters, regardless of how many basis\n",
    "    functions (and here we didn’t even fit them).\n",
    "\n",
    "-   Maximum likelihood model over fits through increasing number\n",
    "    of parameters.\n",
    "\n",
    "-   Revisit maximum likelihood solution with validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularized Mean\n",
    "\n",
    "-   Validation fit here based on mean solution for\n",
    "    $\\mathbf{w}$ only.\n",
    "\n",
    "-   For Bayesian solution\n",
    "    $$\\boldsymbol{\\mu}_w = \\left[\\sigma^{-2}\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi} + \\alpha^{-1}\\mathbf{I}\\right]^{-1} \\sigma^{-2} \\boldsymbol{\\Phi}^\\top \\mathbf{y}$$\n",
    "    instead of\n",
    "    $$\\mathbf{w}^* = \\left[\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}\\right]^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{y}$$\n",
    "\n",
    "-   Two are equivalent when $\\alpha \\rightarrow \\infty$.\n",
    "\n",
    "-   Equivalent to a prior for $\\mathbf{w}$ with infinite variance.\n",
    "\n",
    "-   In other cases $\\alpha \\mathbf{I}$ *regularizes* the system (keeps\n",
    "    parameters smaller)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampling the Posterior\n",
    "\n",
    "-   Now check samples by extracting $\\mathbf{w}$ from the\n",
    "    *posterior*.\n",
    "\n",
    "-   Now for $\\mathbf{y} = \\boldsymbol{\\Phi} \\mathbf{w} + \\boldsymbol{\\epsilon}$\n",
    "    need\n",
    "    $$w \\sim \\mathcal{N}(\\boldsymbol{\\mu}_w,\\mathbf{C}_w)$$\n",
    "    with\n",
    "    $\\mathbf{C}_w = \\left[\\sigma^{-2}\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} + \\alpha^{-1} \\mathbf{I}\\right]^{-1}$\n",
    "    and\n",
    "    $\\boldsymbol{\\mu}_w =\\mathbf{C}_w \\sigma^{-2} \\boldsymbol{\\Phi}^\\top \\mathbf{y}$\n",
    "    $$\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I})$$ with\n",
    "    $\\alpha=1$ and $\\sigma^2 = 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Marginal Likelihood\n",
    "\n",
    "-   The marginal likelihood can also be computed, it has the form:\n",
    "    $$p(\\mathbf{y}|\\mathbf{X}, \\sigma^2, \\alpha) = \\frac{1}{(2\\pi)^\\frac{n}{2}\\left|\\mathbf{K}\\right|^\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}^{-1} \\mathbf{y}\\right)$$\n",
    "    where\n",
    "    $\\mathbf{K} = \\alpha \\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\top + \\sigma^2 \\mathbf{I}$.\n",
    "\n",
    "-   So it is a zero mean $n$-dimensional Gaussian with covariance\n",
    "    matrix $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing the Expected Output\n",
    "\n",
    "-   Given the posterior for the parameters, how can we compute the\n",
    "    expected output at a given location?\n",
    "\n",
    "-   Output of model at location $\\mathbf{x}_i$ is given by\n",
    "    $$f(\\mathbf{x}_i; \\mathbf{w}) = \\boldsymbol{\\phi}_i^\\top \\mathbf{w}$$\n",
    "\n",
    "-   We want the expected output under the posterior density,\n",
    "    $p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}, \\sigma^2, \\alpha)$.\n",
    "\n",
    "-   Mean of mapping function will be given by $$\\begin{aligned}\n",
    "          \\left\\langle f(\\mathbf{x}_i; \\mathbf{w})\\right\\rangle_{p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}, \\sigma^2, \\alpha)} &= \\boldsymbol{\\phi}_i^\\top \\left\\langle\\mathbf{w}\\right\\rangle_{p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}, \\sigma^2, \\alpha)} \\\\\n",
    "          & = \\boldsymbol{\\phi}_i^\\top \\boldsymbol{\\mu}_w\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance of Expected Output\n",
    "\n",
    "-   Variance of model at location $\\mathbf{x}_i$ is given by\n",
    "    $$\\begin{aligned}\n",
    "          \\text{var}(f(\\mathbf{x}_i; \\mathbf{w})) &= \\left\\langle(f(\\mathbf{x}_i; \\mathbf{w}))^2\\right\\rangle - \\left\\langle f(\\mathbf{x}_i; \\mathbf{w})\\right\\rangle^2 \\\\&= \\boldsymbol{\\phi}_i^\\top \\left\\langle\\mathbf{w}\\mathbf{w}^\\top\\right\\rangle \\boldsymbol{\\phi}_i - \\boldsymbol{\\phi}_i^\\top \\left\\langle\\mathbf{w}\\right\\rangle\\left\\langle\\mathbf{w}\\right\\rangle^\\top \\boldsymbol{\\phi}_i \\\\&= \\boldsymbol{\\phi}_i^\\top \\mathbf{C}_i\\boldsymbol{\\phi}_i\n",
    "        \\end{aligned}$$ where all these expectations are taken under the\n",
    "    posterior density,\n",
    "    $p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}, \\sigma^2, \\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "-   Section 3.7–3.8 of @Rogers:book11 (pg 122–133).\n",
    "\n",
    "-   Section 3.4 of @Bishop:book06 (pg 161–165)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
