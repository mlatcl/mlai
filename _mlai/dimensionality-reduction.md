---
week: 8
title: "Dimensionality Reduction: Latent Variable Modelling"
abstract: "In this lecture we turn to *unsupervised learning*. Specifically, we introduce the idea of a latent variable model. Latent variable models are a probabilistic perspective on unsupervised learning which lead to dimensionality reduction algorithms. "
youtube: 0mtK2_rc0IY
---

talk-macros.gpp}lk-macros.gpp}

talk-macros.gpp}lai/includes/mlai-notebook-setup.md}

\subsection{Review}

\slides{* Last time: Looked at Bayesian Regression.
* Introduced priors and marginal likelihoods.
* This time: Unsupervised Learning}

\notes{So far in our classes we have focussed mainly on regression
problems, which are examples of supervised learning. We have considered the
relationship between the likelihood and the objective function and we have shown
how we can find paramters by maximizing the likelihood (equivalent to minimizing
the objective function) and in the last session we saw how we can *marginalize*
the parameters in a process known as Bayesian inference.}


talk-macros.gpp}l/includes/clustering.md}

talk-macros.gpp}imred/includes/high-dimensional-data.md}
talk-macros.gpp}imred/includes/latent-variables.md}
talk-macros.gpp}imred/includes/principal-component-analysis.md}

talk-macros.gpp}imred/includes/probabilistic-pca.md}

talk-macros.gpp}imred/includes/mocap-ppca.md}
talk-macros.gpp}imred/includes/robot-wireless-ppca.md}
talk-macros.gpp}imred/includes/ppca-interpretations.md}
talk-macros.gpp}imred/includes/pca-in-practice.md}
talk-macros.gpp}imred/includes/ppca-marginal-likelihood.md}
talk-macros.gpp}imred/includes/ppca-reconstruction.md}

\reading

\thanks

\references
